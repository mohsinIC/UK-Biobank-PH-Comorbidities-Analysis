{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this code is to include demographic (BMI,Smoking,Sex,Age Groups, IMD) vairables to get the statistical data for UK biobank and PH Cohort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>1. Importing Libraries</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.reset_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.reset_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>2. Complete Structured Dataset (with ICD10 Codes)</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a. Imported dataset from UK Biobank (only with ICD10 codes).**\n",
    "\n",
    "**This dataset contains the variables such as \"Participant IDs\", \"Year of Birth\",\"Month of Birth\", \"Ethnicity\", \"Sex\"  and having both ICD10 and Main ICD10 Codes (with Diagnosis Dates), Diseases Names, Birth and Death Records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD10-base Clinal Dataset:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Importing the dataset\n",
    "d = []\n",
    "d = pd.read_csv('dataset.csv')\n",
    "ICD10_dataset = d\n",
    "## initialize df equal to dataset\n",
    "ICD10_dataset = ICD10_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_ICD10_dataset = {}\n",
    "row_counts_ICD10_dataset = {}\n",
    "nan_counts_ICD10_dataset = {}\n",
    "empty_counts_ICD10_dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_ICD10_dataset in ICD10_dataset.columns:\n",
    "    unique_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].nunique()\n",
    "    row_count_ICD10_dataset = len(ICD10_dataset[column_ICD10_dataset])\n",
    "    nan_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_ICD10_dataset[column_ICD10_dataset] = [unique_count_ICD10_dataset]\n",
    "    row_counts_ICD10_dataset[column_ICD10_dataset] = [row_count_ICD10_dataset]\n",
    "    nan_counts_ICD10_dataset[column_ICD10_dataset] = [nan_count_ICD10_dataset]\n",
    "    empty_counts_ICD10_dataset[column_ICD10_dataset] = [empty_count_ICD10_dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_ICD10_databank = []\n",
    "row_counts_ICD10_databank = []\n",
    "nan_counts_ICD10_databank = []\n",
    "empty_counts_ICD10_databank = []\n",
    "\n",
    "unique_counts_ICD10_databank = pd.DataFrame(unique_counts_ICD10_dataset, index=['Unique Count'])\n",
    "row_counts_ICD10_databank = pd.DataFrame(row_counts_ICD10_dataset, index=['Row Count'])\n",
    "nan_counts_ICD10_databank = pd.DataFrame(nan_counts_ICD10_dataset, index=['NaN Count'])\n",
    "empty_counts_ICD10_databank = pd.DataFrame(empty_counts_ICD10_dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_ICD10_dataset = []\n",
    "result_ICD10_dataset = pd.concat([unique_counts_ICD10_databank, row_counts_ICD10_databank, nan_counts_ICD10_databank, empty_counts_ICD10_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"ICD10-base Clinal Dataset:\")\n",
    "print()\n",
    "#display(result_ICD10_dataset)\n",
    "print()\n",
    "#ICD10_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b. Extract the Variables related to \"ICD10 Codes\" and its related diseases names and Diagnosis Dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICD10 Codes with Dates Table:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### initialize df1 as ICD10 codes dataset\n",
    "ICD10_Codes_Dates_Table = []\n",
    "ICD10_Codes_Dates_Table = ICD10_dataset.iloc[:, :249]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_ICD10_Codes_Dates_Table in ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_ICD10_Codes_Dates_Table = len(ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table])\n",
    "    nan_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [unique_count_ICD10_Codes_Dates_Table]\n",
    "    row_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [row_count_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [nan_count_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [empty_count_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_ICD10_Codes_Dates_Table = []\n",
    "result_ICD10_Codes_Dates_Table = pd.concat([unique_counts_ICD10_Codes_Dates_Table_databank, row_counts_ICD10_Codes_Dates_Table_databank, nan_counts_ICD10_Codes_Dates_Table_databank, empty_counts_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "#display(result_ICD10_Codes_Dates_Table)\n",
    "print()\n",
    "#ICD10_Codes_Dates_Table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c. Structure the \"ICD10 Codes\" with related Diseases and Diagnosis Dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organised ICD10 Codes with Dates Table:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant ID</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Month of Birth</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ICD10 codes with Diseases Names</th>\n",
       "      <th>ICD10 Diagnosis Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unique Count</th>\n",
       "      <td>440014</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>12226</td>\n",
       "      <td>9642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row Count</th>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "      <td>6301993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN Count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Empty Count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Participant ID  Year of Birth  Month of Birth  Ethnicity  \\\n",
       "Unique Count          440014             37              12         22   \n",
       "Row Count            6301993        6301993         6301993    6301993   \n",
       "NaN Count                  0              0               0      14578   \n",
       "Empty Count                0              0               0          0   \n",
       "\n",
       "                  Sex  ICD10 codes with Diseases Names  ICD10 Diagnosis Date  \n",
       "Unique Count        2                            12226                  9642  \n",
       "Row Count     6301993                          6301993               6301993  \n",
       "NaN Count           0                                0                     0  \n",
       "Empty Count         0                                0                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Create a new DataFrame to store the modified data\n",
    "Storing_ICD10_Codes_Dates_Table = []\n",
    "\n",
    "#### Iterate through each row\n",
    "for idx, row in ICD10_Codes_Dates_Table.iterrows():\n",
    "    participant_id = row['Participant ID']\n",
    "    year_of_birth = row['Year of Birth']\n",
    "    month_of_birth = row['Month of Birth']\n",
    "    ethnicity = row['Ethnicity']\n",
    "    sex = row['Sex']\n",
    "    icd_names = row['ICD10 - Diagnosis'].split('|')\n",
    "    dates = [row['ICD10 date-0'], row['ICD10 date-1'], row['ICD10 date-2'],\n",
    "             row['ICD10 date-3'], row['ICD10 date-4'], row['ICD10 date-5'],\n",
    "             row['ICD10 date-6'], row['ICD10 date-7'], row['ICD10 date-8'],\n",
    "             row['ICD10 date-9'], row['ICD10 date-10'], row['ICD10 date-11'],\n",
    "             row['ICD10 date-12'], row['ICD10 date-13'], row['ICD10 date-14'],\n",
    "             row['ICD10 date-15'], row['ICD10 date-16'], row['ICD10 date-17'],\n",
    "             row['ICD10 date-18'], row['ICD10 date-19'], row['ICD10 date-20'],\n",
    "             row['ICD10 date-21'], row['ICD10 date-22'], row['ICD10 date-23'],\n",
    "             row['ICD10 date-24'], row['ICD10 date-25'], row['ICD10 date-26'],\n",
    "             row['ICD10 date-27'], row['ICD10 date-28'], row['ICD10 date-29'],\n",
    "             row['ICD10 date-30'], row['ICD10 date-31'], row['ICD10 date-32'],\n",
    "             row['ICD10 date-33'], row['ICD10 date-34'], row['ICD10 date-35'],\n",
    "             row['ICD10 date-36'], row['ICD10 date-37'], row['ICD10 date-38'],\n",
    "             row['ICD10 date-39'], row['ICD10 date-40'], row['ICD10 date-41'],\n",
    "             row['ICD10 date-42'], row['ICD10 date-43'], row['ICD10 date-44'],\n",
    "             row['ICD10 date-45'], row['ICD10 date-46'], row['ICD10 date-47'],\n",
    "             row['ICD10 date-48'], row['ICD10 date-49'], row['ICD10 date-50'],\n",
    "             row['ICD10 date-51'], row['ICD10 date-52'], row['ICD10 date-53'],\n",
    "             row['ICD10 date-54'], row['ICD10 date-55'], row['ICD10 date-56'],\n",
    "             row['ICD10 date-57'], row['ICD10 date-58'], row['ICD10 date-59'],\n",
    "             row['ICD10 date-60'], row['ICD10 date-61'], row['ICD10 date-62'],\n",
    "             row['ICD10 date-63'], row['ICD10 date-64'], row['ICD10 date-65'],\n",
    "             row['ICD10 date-66'], row['ICD10 date-67'], row['ICD10 date-68'],\n",
    "             row['ICD10 date-69'], row['ICD10 date-70'], row['ICD10 date-71'],\n",
    "             row['ICD10 date-72'], row['ICD10 date-73'], row['ICD10 date-74'],\n",
    "             row['ICD10 date-75'], row['ICD10 date-76'], row['ICD10 date-77'],\n",
    "             row['ICD10 date-78'], row['ICD10 date-79'], row['ICD10 date-80'],\n",
    "             row['ICD10 date-81'], row['ICD10 date-82'],row['ICD10 date-83'],\n",
    "             row['ICD10 date-84'], row['ICD10 date-85'],row['ICD10 date-86'],\n",
    "             row['ICD10 date-87'], row['ICD10 date-88'],row['ICD10 date-89'], row['ICD10 date-90'],\n",
    "             row['ICD10 date-91'], row['ICD10 date-92'],row['ICD10 date-93'],\n",
    "             row['ICD10 date-94'], row['ICD10 date-95'],row['ICD10 date-96'],\n",
    "             row['ICD10 date-97'], row['ICD10 date-98'],row['ICD10 date-99'], row['ICD10 date-100'],\n",
    "             row['ICD10 date-101'], row['ICD10 date-102'],\n",
    "             row['ICD10 date-103'], row['ICD10 date-104'], row['ICD10 date-105'],\n",
    "             row['ICD10 date-106'], row['ICD10 date-107'], row['ICD10 date-108'],\n",
    "             row['ICD10 date-109'], row['ICD10 date-110'], row['ICD10 date-111'],\n",
    "             row['ICD10 date-112'], row['ICD10 date-113'], row['ICD10 date-114'],\n",
    "             row['ICD10 date-115'], row['ICD10 date-116'], row['ICD10 date-117'],\n",
    "             row['ICD10 date-118'], row['ICD10 date-119'], row['ICD10 date-120'],\n",
    "             row['ICD10 date-121'], row['ICD10 date-122'], row['ICD10 date-123'],\n",
    "             row['ICD10 date-124'], row['ICD10 date-125'], row['ICD10 date-126'],\n",
    "             row['ICD10 date-127'], row['ICD10 date-128'], row['ICD10 date-129'],\n",
    "             row['ICD10 date-130'], row['ICD10 date-131'], row['ICD10 date-132'],\n",
    "             row['ICD10 date-133'], row['ICD10 date-134'], row['ICD10 date-135'],\n",
    "             row['ICD10 date-136'], row['ICD10 date-137'], row['ICD10 date-138'],\n",
    "             row['ICD10 date-139'], row['ICD10 date-140'], row['ICD10 date-141'],\n",
    "             row['ICD10 date-142'], row['ICD10 date-143'], row['ICD10 date-144'],\n",
    "             row['ICD10 date-145'], row['ICD10 date-146'], row['ICD10 date-147'],\n",
    "             row['ICD10 date-148'], row['ICD10 date-149'], row['ICD10 date-150'],\n",
    "             row['ICD10 date-151'], row['ICD10 date-152'], row['ICD10 date-153'],\n",
    "             row['ICD10 date-154'], row['ICD10 date-155'], row['ICD10 date-156'],\n",
    "             row['ICD10 date-157'], row['ICD10 date-158'], row['ICD10 date-159'],\n",
    "             row['ICD10 date-160'], row['ICD10 date-161'], row['ICD10 date-162'],\n",
    "             row['ICD10 date-163'], row['ICD10 date-164'], row['ICD10 date-165'],\n",
    "             row['ICD10 date-166'], row['ICD10 date-167'], row['ICD10 date-168'],\n",
    "             row['ICD10 date-169'], row['ICD10 date-170'], row['ICD10 date-171'],\n",
    "             row['ICD10 date-172'], row['ICD10 date-173'], row['ICD10 date-174'],\n",
    "             row['ICD10 date-175'], row['ICD10 date-176'], row['ICD10 date-177'],\n",
    "             row['ICD10 date-178'], row['ICD10 date-179'], row['ICD10 date-180'],\n",
    "             row['ICD10 date-181'], row['ICD10 date-182'],row['ICD10 date-183'],\n",
    "             row['ICD10 date-184'], row['ICD10 date-185'],row['ICD10 date-186'],\n",
    "             row['ICD10 date-187'], row['ICD10 date-188'],row['ICD10 date-189'], row['ICD10 date-190'],\n",
    "             row['ICD10 date-191'], row['ICD10 date-192'],row['ICD10 date-193'],\n",
    "             row['ICD10 date-194'], row['ICD10 date-195'],row['ICD10 date-196'],\n",
    "             row['ICD10 date-197'], row['ICD10 date-198'],row['ICD10 date-199'], row['ICD10 date-200'],\n",
    "             row['ICD10 date-201'], row['ICD10 date-202'],\n",
    "             row['ICD10 date-203'], row['ICD10 date-204'], row['ICD10 date-205'],\n",
    "             row['ICD10 date-206'], row['ICD10 date-207'], row['ICD10 date-208'],\n",
    "             row['ICD10 date-209'], row['ICD10 date-210'], row['ICD10 date-211'],\n",
    "             row['ICD10 date-212'], row['ICD10 date-213'], row['ICD10 date-214'],\n",
    "             row['ICD10 date-215'], row['ICD10 date-216'], row['ICD10 date-217'],\n",
    "             row['ICD10 date-218'], row['ICD10 date-219'], row['ICD10 date-220'],\n",
    "             row['ICD10 date-221'], row['ICD10 date-222'], row['ICD10 date-223'],\n",
    "             row['ICD10 date-224'], row['ICD10 date-225'], row['ICD10 date-226'],\n",
    "             row['ICD10 date-227'], row['ICD10 date-228'], row['ICD10 date-229'],\n",
    "             row['ICD10 date-230'], row['ICD10 date-231'], row['ICD10 date-232'],\n",
    "             row['ICD10 date-233'], row['ICD10 date-234'], row['ICD10 date-235'],\n",
    "             row['ICD10 date-236'], row['ICD10 date-237'], row['ICD10 date-238'],\n",
    "             row['ICD10 date-239'], row['ICD10 date-240'], row['ICD10 date-241'],\n",
    "             row['ICD10 date-242']]\n",
    "\n",
    "\n",
    "    for i in range(243):\n",
    "        new_row = [participant_id,year_of_birth,month_of_birth,ethnicity,sex, icd_names[i] if len(icd_names) > i else None, dates[i]]\n",
    "        Storing_ICD10_Codes_Dates_Table.append(new_row)\n",
    "\n",
    "#### Create a new DataFrame with the modified data\n",
    "ICD10_Codes_Dates_dataset = []\n",
    "ICD10_Codes_Dates_dataset = pd.DataFrame(Storing_ICD10_Codes_Dates_Table, columns=['Participant ID','Year of Birth','Month of Birth','Ethnicity','Sex','ICD10 codes with Diseases Names', 'ICD10 Diagnosis Date'])\n",
    "#### Remove rows with NaN values in both columns\n",
    "ICD10_Codes_Dates_dataset = ICD10_Codes_Dates_dataset.dropna(subset=['ICD10 codes with Diseases Names', 'ICD10 Diagnosis Date'])\n",
    "ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "#### Reset the index after dropping rows\n",
    "ICD10_Codes_Dates_dataset.reset_index(drop=True, inplace=True)\n",
    "Organised_ICD10_Codes_Dates_Table = []\n",
    "Organised_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Organised_ICD10_Codes_Dates_Table in Organised_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Organised_ICD10_Codes_Dates_Table = len(Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [unique_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [row_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [nan_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [empty_count_Organised_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Organised_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Organised_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Organised_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Organised_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Organised_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Organised_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Organised_ICD10_Codes_Dates_Table = []\n",
    "result_Organised_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Organised_ICD10_Codes_Dates_Table_databank, row_counts_Organised_ICD10_Codes_Dates_Table_databank, nan_counts_Organised_ICD10_Codes_Dates_Table_databank, empty_counts_Organised_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Organised ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_Organised_ICD10_Codes_Dates_Table)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Seperate the ICD10 Codes and ICD10 Diseases columns\n",
    "Organised_ICD10_Codes_Dates_dataset = []\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_Table\n",
    "Organised_ICD10_Codes_Dates_dataset['ICD10 codes'] = Organised_ICD10_Codes_Dates_dataset['ICD10 codes with Diseases Names'].str.extract(r'([A-Z]\\d+\\.\\d+|[A-Z]\\d+)')  # Handles both cases\n",
    "Organised_ICD10_Codes_Dates_dataset['ICD10 Diseases'] = Organised_ICD10_Codes_Dates_dataset['ICD10 codes with Diseases Names'].str.replace(r'[A-Z]\\d+\\.\\d+|[A-Z]\\d+', '')\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_dataset.drop(columns=['ICD10 codes with Diseases Names'])\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "#ICD10_Codes_Dates_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d. Extract the \"Main ICD10 Codes\" Variables from the main dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main ICD10 Codes with Dates Table:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### initialize df1 as ICD10 codes dataset\n",
    "Main_ICD10_Codes_Dates_Table = []\n",
    "Main_ICD10_Codes_Dates_Table = ICD10_dataset.iloc[:, :5].join(ICD10_dataset.iloc[:, 249:329])\n",
    "Main_ICD10_Codes_Dates_Table  = Main_ICD10_Codes_Dates_Table .dropna(subset=['Main ICD10 - Diagnosis'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Main_ICD10_Codes_Dates_Table in Main_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Main_ICD10_Codes_Dates_Table = len(Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [unique_count_Main_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [row_count_Main_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [nan_count_Main_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [empty_count_Main_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Main_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Main_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Main_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Main_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Main_ICD10_Codes_Dates_Table = []\n",
    "result_Main_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Main_ICD10_Codes_Dates_Table_databank, row_counts_Main_ICD10_Codes_Dates_Table_databank, nan_counts_Main_ICD10_Codes_Dates_Table_databank, empty_counts_Main_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Main ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "#display(result_Main_ICD10_Codes_Dates_Table)\n",
    "print()\n",
    "#Main_ICD10_Codes_Dates_Table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e. Structure the \"Main ICD10 Codes\" with related Diseases and Diagnosis Dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organised Main ICD10 Codes with Dates Table:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant ID</th>\n",
       "      <th>Year of Birth</th>\n",
       "      <th>Month of Birth</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Main ICD10 codes with Diseases Names</th>\n",
       "      <th>Main ICD10 Diagnosis Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unique Count</th>\n",
       "      <td>440010</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>8681</td>\n",
       "      <td>9641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row Count</th>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "      <td>2296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN Count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Empty Count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Participant ID  Year of Birth  Month of Birth  Ethnicity  \\\n",
       "Unique Count          440010             37              12         22   \n",
       "Row Count            2296600        2296600         2296600    2296600   \n",
       "NaN Count                  0              0               0       5025   \n",
       "Empty Count                0              0               0          0   \n",
       "\n",
       "                  Sex  Main ICD10 codes with Diseases Names  \\\n",
       "Unique Count        2                                  8681   \n",
       "Row Count     2296600                               2296600   \n",
       "NaN Count           0                                     0   \n",
       "Empty Count         0                                     0   \n",
       "\n",
       "              Main ICD10 Diagnosis Date  \n",
       "Unique Count                       9641  \n",
       "Row Count                       2296600  \n",
       "NaN Count                             0  \n",
       "Empty Count                           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Create a new DataFrame to store the modified data\n",
    "Storing_Main_ICD10_Codes_Dates_Table = []\n",
    "\n",
    "#### Iterate through each row\n",
    "for idx_Main, row in Main_ICD10_Codes_Dates_Table.iterrows():\n",
    "    participant_id_Main = row['Participant ID']\n",
    "    year_of_birth_Main = row['Year of Birth']\n",
    "    month_of_birth_Main = row['Month of Birth']\n",
    "    ethnicity_Main = row['Ethnicity']\n",
    "    sex_Main = row['Sex']\n",
    "    icd_names_Main = row['Main ICD10 - Diagnosis'].split('|')\n",
    "    dates_Main = [row['Main ICD10 date-0'], row['Main ICD10 date-1'], row['Main ICD10 date-2'],\n",
    "             row['Main ICD10 date-3'], row['Main ICD10 date-4'], row['Main ICD10 date-5'],\n",
    "             row['Main ICD10 date-6'], row['Main ICD10 date-7'], row['Main ICD10 date-8'],\n",
    "             row['Main ICD10 date-9'], row['Main ICD10 date-10'], row['Main ICD10 date-11'],\n",
    "             row['Main ICD10 date-12'], row['Main ICD10 date-13'], row['Main ICD10 date-14'],\n",
    "             row['Main ICD10 date-15'], row['Main ICD10 date-16'], row['Main ICD10 date-17'],\n",
    "             row['Main ICD10 date-18'], row['Main ICD10 date-19'], row['Main ICD10 date-20'],\n",
    "             row['Main ICD10 date-21'], row['Main ICD10 date-22'], row['Main ICD10 date-23'],\n",
    "             row['Main ICD10 date-24'], row['Main ICD10 date-25'], row['Main ICD10 date-26'],\n",
    "             row['Main ICD10 date-27'], row['Main ICD10 date-28'], row['Main ICD10 date-29'],\n",
    "             row['Main ICD10 date-30'], row['Main ICD10 date-31'], row['Main ICD10 date-32'],\n",
    "             row['Main ICD10 date-33'], row['Main ICD10 date-34'], row['Main ICD10 date-35'],\n",
    "             row['Main ICD10 date-36'], row['Main ICD10 date-37'], row['Main ICD10 date-38'],\n",
    "             row['Main ICD10 date-39'], row['Main ICD10 date-40'], row['Main ICD10 date-41'],\n",
    "             row['Main ICD10 date-42'], row['Main ICD10 date-43'], row['Main ICD10 date-44'],\n",
    "             row['Main ICD10 date-45'], row['Main ICD10 date-46'], row['Main ICD10 date-47'],\n",
    "             row['Main ICD10 date-48'], row['Main ICD10 date-49'], row['Main ICD10 date-50'],\n",
    "             row['Main ICD10 date-51'], row['Main ICD10 date-52'], row['Main ICD10 date-53'],\n",
    "             row['Main ICD10 date-54'], row['Main ICD10 date-55'], row['Main ICD10 date-56'],\n",
    "             row['Main ICD10 date-57'], row['Main ICD10 date-58'], row['Main ICD10 date-59'],\n",
    "             row['Main ICD10 date-60'], row['Main ICD10 date-61'], row['Main ICD10 date-62'],\n",
    "             row['Main ICD10 date-63'], row['Main ICD10 date-64'], row['Main ICD10 date-65'],\n",
    "             row['Main ICD10 date-66'], row['Main ICD10 date-67'], row['Main ICD10 date-68'],\n",
    "             row['Main ICD10 date-69'], row['Main ICD10 date-70'], row['Main ICD10 date-71'],\n",
    "             row['Main ICD10 date-72'], row['Main ICD10 date-73'], row['Main ICD10 date-74'],\n",
    "             row['Main ICD10 date-75'], row['Main ICD10 date-76'], row['Main ICD10 date-77'],\n",
    "             row['Main ICD10 date-78']]\n",
    "\n",
    "\n",
    "    for i in range(79):\n",
    "        new_row_Main = [participant_id_Main,year_of_birth_Main,month_of_birth_Main,ethnicity_Main,sex_Main, icd_names_Main[i] if len(icd_names_Main) > i else None, dates_Main[i]]\n",
    "        Storing_Main_ICD10_Codes_Dates_Table.append(new_row_Main)\n",
    "\n",
    "#### Create a new DataFrame with the modified data\n",
    "Main_ICD10_Codes_Dates_dataset = []\n",
    "Main_ICD10_Codes_Dates_dataset = pd.DataFrame(Storing_Main_ICD10_Codes_Dates_Table, columns=['Participant ID','Year of Birth','Month of Birth','Ethnicity','Sex','Main ICD10 codes with Diseases Names', 'Main ICD10 Diagnosis Date'])\n",
    "### Remove rows with NaN values in both columns\n",
    "Main_ICD10_Codes_Dates_dataset = Main_ICD10_Codes_Dates_dataset.dropna(subset=['Main ICD10 codes with Diseases Names', 'Main ICD10 Diagnosis Date'])\n",
    "Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "#### Reset the index after dropping rows\n",
    "Main_ICD10_Codes_Dates_dataset.reset_index(drop=True, inplace=True)\n",
    "Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Organised_Main_ICD10_Codes_Dates_Table = []\n",
    "Organised_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Organised_Main_ICD10_Codes_Dates_Table in Organised_Main_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Organised_Main_ICD10_Codes_Dates_Table = len(Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [unique_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [row_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [nan_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [empty_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Organised_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Organised_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Organised_Main_ICD10_Codes_Dates_Table = []\n",
    "result_Organised_Main_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, nan_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, empty_counts_Organised_Main_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Organised Main ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_Organised_Main_ICD10_Codes_Dates_Table)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Seperate the Main ICD10 Codes and Main ICD10 Diseases columns\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = []\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_Table\n",
    "\n",
    "Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes'] = Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes with Diseases Names'].str.extract(r'([A-Z]\\d+\\.\\d+|[A-Z]\\d+)')  # Handles both cases\n",
    "Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 Diseases'] = Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes with Diseases Names'].str.replace(r'[A-Z]\\d+\\.\\d+|[A-Z]\\d+', '')\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_dataset.drop(columns=['Main ICD10 codes with Diseases Names'])\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "#Main_ICD10_Codes_Dates_dataset.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f. Concentenate \"ICD10_Code_dataset\" and \"Main_ICD10_Code_dataset\" datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/apps/jupyterhub/2019-04-29/miniconda/lib/python3.6/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Table Record:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Concentenate (combine) ICD10 codes dataset  and Main ICD10 codes dataset\n",
    "data_table = []\n",
    "data_table = pd.concat([Organised_ICD10_Codes_Dates_dataset ,Organised_Main_ICD10_Codes_Dates_dataset])\n",
    "data_table = data_table.sort_values(by='Participant ID')\n",
    "data_table.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_data_table = {}\n",
    "row_counts_data_table= {}\n",
    "nan_counts_data_table = {}\n",
    "empty_counts_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_data_table in data_table.columns:\n",
    "    unique_count_data_table = data_table[column_data_table].nunique()\n",
    "    row_count_data_table = len(data_table[column_data_table])\n",
    "    nan_count_data_table = data_table[column_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_data_table = data_table[column_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_data_table[column_data_table] = [unique_count_data_table]\n",
    "    row_counts_data_table[column_data_table] = [row_count_data_table]\n",
    "    nan_counts_data_table[column_data_table] = [nan_count_data_table]\n",
    "    empty_counts_data_table[column_data_table] = [empty_count_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_data_table_records = []\n",
    "row_counts_data_table_records = []\n",
    "nan_counts_data_table_records = []\n",
    "empty_counts_data_table_records = []\n",
    "\n",
    "unique_counts_data_table_records = pd.DataFrame(unique_counts_data_table, index=['Unique Count'])\n",
    "row_counts_data_table_records = pd.DataFrame(row_counts_data_table, index=['Row Count'])\n",
    "nan_counts_data_table_records = pd.DataFrame(nan_counts_data_table, index=['NaN Count'])\n",
    "empty_counts_data_table_records = pd.DataFrame(empty_counts_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_data_table = []\n",
    "result_data_table = pd.concat([unique_counts_data_table_records, row_counts_data_table_records, nan_counts_data_table_records, empty_counts_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Data Table Record:\")\n",
    "print()\n",
    "#display(result_data_table)\n",
    "print()\n",
    "#data_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g. Create Two new Variables: \"Combined ICD10 Codes\" and \"Combined ICD10 Diagnosis Date\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new column 'Combined ICD10 Codes' by concatenating 'ICD10 codes' and 'Main ICD10 codes'\n",
    "data_table['Combined ICD10 Codes'] = data_table['ICD10 codes'] + data_table['Main ICD10 codes']\n",
    "\n",
    "\n",
    "#### Create a new column 'Combined ICD10 Diagnosis Date' by selecting the non-empty value between 'ICD10 Diagnosis Date' and 'Main ICD10 Diagnosis Date'\n",
    "data_table['Combined ICD10 Diagnosis Date'] = np.where(data_table['ICD10 Diagnosis Date'] != '', data_table['ICD10 Diagnosis Date'], data_table['Main ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "#### Create a new column 'Combined ICD10 Diseases' by concatenating 'ICD10 Diseases' and 'Main ICD10 Diseases'\n",
    "data_table['Combined ICD10 Diseases'] = data_table['ICD10 Diseases']  + data_table['Main ICD10 Diseases']\n",
    "\n",
    "\n",
    "### Drop the original columns that are no longer needed\n",
    "data_table.drop(['ICD10 codes', 'ICD10 Diagnosis Date', 'Main ICD10 codes', 'Main ICD10 Diagnosis Date', 'ICD10 Diseases', 'Main ICD10 Diseases'], axis=1, inplace=True)\n",
    "\n",
    "data_table = data_table.sort_values(by=['Participant ID','Combined ICD10 Codes','Combined ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "\n",
    "Combined_ICD10_data_table = []\n",
    "Combined_ICD10_data_table = data_table\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_data_table = {}\n",
    "row_counts_Combined_ICD10_data_table= {}\n",
    "nan_counts_Combined_ICD10_data_table = {}\n",
    "empty_counts_Combined_ICD10_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_data_table in Combined_ICD10_data_table.columns:\n",
    "    unique_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].nunique()\n",
    "    row_count_Combined_ICD10_data_table = len(Combined_ICD10_data_table[column_Combined_ICD10_data_table])\n",
    "    nan_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [unique_count_Combined_ICD10_data_table]\n",
    "    row_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [row_count_Combined_ICD10_data_table]\n",
    "    nan_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [nan_count_Combined_ICD10_data_table]\n",
    "    empty_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [empty_count_Combined_ICD10_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_data_table_records = []\n",
    "row_counts_Combined_ICD10_data_table_records = []\n",
    "nan_counts_Combined_ICD10_data_table_records = []\n",
    "empty_counts_Combined_ICD10_data_table_records = []\n",
    "\n",
    "unique_counts_Combined_ICD10_data_table_records = pd.DataFrame(unique_counts_Combined_ICD10_data_table, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_data_table_records = pd.DataFrame(row_counts_Combined_ICD10_data_table, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_data_table_records = pd.DataFrame(nan_counts_Combined_ICD10_data_table, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_data_table_records = pd.DataFrame(empty_counts_Combined_ICD10_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_data_table = []\n",
    "result_Combined_ICD10_data_table = pd.concat([unique_counts_Combined_ICD10_data_table_records, row_counts_Combined_ICD10_data_table_records, nan_counts_Combined_ICD10_data_table_records, empty_counts_Combined_ICD10_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Combined ICD10 Data Table Record:\")\n",
    "#print()\n",
    "#display(result_Combined_ICD10_data_table)\n",
    "#print()\n",
    "#data_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2h. Drop Duplicates and Rearrange Variables and Save the output as Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = data_table.drop_duplicates(['Participant ID','Combined ICD10 Codes','Combined ICD10 Diagnosis Date','Combined ICD10 Diseases'])\n",
    "data_table\n",
    "\n",
    "\n",
    "#### Rearrange the columns\n",
    "data_table = data_table[['Participant ID', 'Sex', 'Year of Birth', 'Month of Birth', 'Ethnicity', 'Combined ICD10 Diseases','Combined ICD10 Diagnosis Date','Combined ICD10 Codes']]\n",
    "data_table\n",
    "\n",
    "\n",
    "dropped_duplicates_data_table = []\n",
    "dropped_duplicates_data_table = data_table\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_dropped_duplicates_data_table = {}\n",
    "row_counts_dropped_duplicates_data_table= {}\n",
    "nan_counts_dropped_duplicates_data_table = {}\n",
    "empty_counts_dropped_duplicates_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_dropped_duplicates_data_table in dropped_duplicates_data_table.columns:\n",
    "    unique_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].nunique()\n",
    "    row_count_dropped_duplicates_data_table = len(dropped_duplicates_data_table[column_dropped_duplicates_data_table])\n",
    "    nan_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [unique_count_dropped_duplicates_data_table]\n",
    "    row_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [row_count_dropped_duplicates_data_table]\n",
    "    nan_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [nan_count_dropped_duplicates_data_table]\n",
    "    empty_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [empty_count_dropped_duplicates_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_dropped_duplicates_data_table_records = []\n",
    "row_counts_dropped_duplicates_data_table_records = []\n",
    "nan_counts_dropped_duplicates_data_table_records = []\n",
    "empty_counts_dropped_duplicates_data_table_records = []\n",
    "\n",
    "unique_counts_dropped_duplicates_data_table_records = pd.DataFrame(unique_counts_dropped_duplicates_data_table, index=['Unique Count'])\n",
    "row_counts_dropped_duplicates_data_table_records = pd.DataFrame(row_counts_dropped_duplicates_data_table, index=['Row Count'])\n",
    "nan_counts_dropped_duplicates_data_table_records = pd.DataFrame(nan_counts_dropped_duplicates_data_table, index=['NaN Count'])\n",
    "empty_counts_dropped_duplicates_data_table_records = pd.DataFrame(empty_counts_dropped_duplicates_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_dropped_duplicates_data_table = []\n",
    "result_dropped_duplicates_data_table = pd.concat([unique_counts_dropped_duplicates_data_table_records, row_counts_dropped_duplicates_data_table_records, nan_counts_dropped_duplicates_data_table_records, empty_counts_dropped_duplicates_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Dropped Duplicates from Data Table Record:\")\n",
    "print()\n",
    "display(result_dropped_duplicates_data_table)\n",
    "print()\n",
    "data_table.head(3)\n",
    "\n",
    "\n",
    "\n",
    "#### Specify the file path where you want to save the CSV file\n",
    "#file_path = 'All ICD10 Codes with Diseases Names and Dates Data.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "#data_table.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('All ICD10 Codes with Diseases Names and Dates Data.csv')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Dropped Duplicates from Data Table Record:\")\n",
    "print()\n",
    "#display(result_dropped_duplicates_data_table)\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_All_ICD10_with_Diseases_Dates_Data in All_ICD10_with_Diseases_Dates_Data.columns:\n",
    "    unique_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].nunique()\n",
    "    row_count_All_ICD10_with_Diseases_Dates_Data = len(All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data])\n",
    "    nan_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].isna().sum()  # Count NaN values\n",
    "    empty_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [unique_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    row_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [row_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    nan_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [nan_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    empty_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [empty_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(unique_counts_All_ICD10_with_Diseases_Dates_Data, index=['Unique Count'])\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(row_counts_All_ICD10_with_Diseases_Dates_Data, index=['Row Count'])\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(nan_counts_All_ICD10_with_Diseases_Dates_Data, index=['NaN Count'])\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(empty_counts_All_ICD10_with_Diseases_Dates_Data, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_All_ICD10_with_Diseases_Dates_Data = []\n",
    "result_All_ICD10_with_Diseases_Dates_Data = pd.concat([unique_counts_All_ICD10_with_Diseases_Dates_Datas, row_counts_All_ICD10_with_Diseases_Dates_Datas, nan_counts_All_ICD10_with_Diseases_Dates_Datas, empty_counts_All_ICD10_with_Diseases_Dates_Datas])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"All_ICD10_with_Diseases_Dates_Data Record:\")\n",
    "#print()\n",
    "#display(result_All_ICD10_with_Diseases_Dates_Data)\n",
    "#print()\n",
    "\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2i. Import Death Dates dataset from UK biobank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Death_Date_Record = []\n",
    "Death_Date_Record = pd.read_csv('Death Date Record.csv')\n",
    "Death_Date_Record = Death_Date_Record.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Date_Record = {}\n",
    "row_counts_Death_Date_Record = {}\n",
    "nan_counts_Death_Date_Record = {}\n",
    "empty_counts_Death_Date_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Date_Record in Death_Date_Record.columns:\n",
    "    unique_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].nunique()\n",
    "    row_count_Death_Date_Record = len(Death_Date_Record[column_Death_Date_Record])\n",
    "    nan_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Date_Record[column_Death_Date_Record] = [unique_count_Death_Date_Record]\n",
    "    row_counts_Death_Date_Record[column_Death_Date_Record] = [row_count_Death_Date_Record]\n",
    "    nan_counts_Death_Date_Record[column_Death_Date_Record] = [nan_count_Death_Date_Record]\n",
    "    empty_counts_Death_Date_Record[column_Death_Date_Record] = [empty_count_Death_Date_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Date = []\n",
    "row_counts_Death_Date = []\n",
    "nan_counts_Death_Date = []\n",
    "empty_counts_Death_Date = []\n",
    "\n",
    "unique_counts_Death_Date = pd.DataFrame(unique_counts_Death_Date_Record, index=['Unique Count'])\n",
    "row_counts_Death_Date = pd.DataFrame(row_counts_Death_Date_Record, index=['Row Count'])\n",
    "nan_counts_Death_Date = pd.DataFrame(nan_counts_Death_Date_Record, index=['NaN Count'])\n",
    "empty_counts_Death_Date = pd.DataFrame(empty_counts_Death_Date_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Date_Record = []\n",
    "result_Death_Date_Record = pd.concat([unique_counts_Death_Date, row_counts_Death_Date, nan_counts_Death_Date, empty_counts_Death_Date])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Death Date Record:\")\n",
    "#print()\n",
    "#display(result_Death_Date_Record)\n",
    "\n",
    "\n",
    "\n",
    "#Death_Date_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2j. Import Death Cause dataset from UK biobank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Death_Cause_Record = []\n",
    "Death_Cause_Record = pd.read_csv('Death Cause Record.csv')\n",
    "Death_Cause_Record = Death_Cause_Record.drop_duplicates()\n",
    "Death_Cause_Record = Death_Cause_Record.rename(columns={'ICD10 codes':'Death Cause Disease ICD10 Codes','ICD10 Diseases':'Death Cause Diseases'})\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Cause_Record = {}\n",
    "row_counts_Death_Cause_Record = {}\n",
    "nan_counts_Death_Cause_Record = {}\n",
    "empty_counts_Death_Cause_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Cause_Record in Death_Cause_Record.columns:\n",
    "    unique_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].nunique()\n",
    "    row_count_Death_Cause_Record = len(Death_Cause_Record[column_Death_Cause_Record])\n",
    "    nan_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Cause_Record[column_Death_Cause_Record] = [unique_count_Death_Cause_Record]\n",
    "    row_counts_Death_Cause_Record[column_Death_Cause_Record] = [row_count_Death_Cause_Record]\n",
    "    nan_counts_Death_Cause_Record[column_Death_Cause_Record] = [nan_count_Death_Cause_Record]\n",
    "    empty_counts_Death_Cause_Record[column_Death_Cause_Record] = [empty_count_Death_Cause_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Cause = []\n",
    "row_counts_Death_Cause = []\n",
    "nan_counts_Death_Cause = []\n",
    "empty_counts_Death_Cause = []\n",
    "\n",
    "unique_counts_Death_Cause = pd.DataFrame(unique_counts_Death_Cause_Record, index=['Unique Count'])\n",
    "row_counts_Death_Cause = pd.DataFrame(row_counts_Death_Cause_Record, index=['Row Count'])\n",
    "nan_counts_Death_Cause = pd.DataFrame(nan_counts_Death_Cause_Record, index=['NaN Count'])\n",
    "empty_counts_Death_Cause = pd.DataFrame(empty_counts_Death_Cause_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Cause_Record = []\n",
    "result_Death_Cause_Record = pd.concat([unique_counts_Death_Cause, row_counts_Death_Cause, nan_counts_Death_Cause, empty_counts_Death_Cause])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Death Cause Record:\")\n",
    "#print()\n",
    "#display(result_Death_Cause_Record)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Death_Cause_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2k. Import GP dataset of drugs (medicine) from UK biobank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_Prescription_Record = []\n",
    "GP_Prescription_Record = pd.read_csv('GP Prescription Record.csv')\n",
    "GP_Prescription_Record = GP_Prescription_Record.drop_duplicates()\n",
    "GP_Prescription_Record = GP_Prescription_Record.rename(columns={'Issue Date':'Drug Issue Date','Quantity':'Drug Quantity'})\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_GP_Prescription_Record = {}\n",
    "row_counts_GP_Prescription_Record = {}\n",
    "nan_counts_GP_Prescription_Record = {}\n",
    "empty_counts_GP_Prescription_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_GP_Prescription_Record in GP_Prescription_Record.columns:\n",
    "    unique_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].nunique()\n",
    "    row_count_GP_Prescription_Record = len(GP_Prescription_Record[column_GP_Prescription_Record])\n",
    "    nan_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [unique_count_GP_Prescription_Record]\n",
    "    row_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [row_count_GP_Prescription_Record]\n",
    "    nan_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [nan_count_GP_Prescription_Record]\n",
    "    empty_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [empty_count_GP_Prescription_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_GP_Prescription = []\n",
    "row_counts_GP_Prescription = []\n",
    "nan_counts_GP_Prescription = []\n",
    "empty_counts_GP_Prescription_Record = []\n",
    "\n",
    "unique_counts_GP_Prescription = pd.DataFrame(unique_counts_GP_Prescription_Record, index=['Unique Count'])\n",
    "row_counts_GP_Prescription = pd.DataFrame(row_counts_GP_Prescription_Record, index=['Row Count'])\n",
    "nan_counts_GP_Prescription = pd.DataFrame(nan_counts_GP_Prescription_Record, index=['NaN Count'])\n",
    "empty_counts_GP_Prescription = pd.DataFrame(empty_counts_GP_Prescription_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_GP_Prescription_Record = []\n",
    "result_GP_Prescription_Record = pd.concat([unique_counts_GP_Prescription, row_counts_GP_Prescription, nan_counts_GP_Prescription, empty_counts_GP_Prescription])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"GP Prescription Dataset Record:\")\n",
    "#print()\n",
    "#display(result_GP_Prescription_Record)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GP_Prescription_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2l. Merge the datasets \"Death_Date_Record\" and \"Death_Cause_Record\" based on the \"Participant ID\" variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge the datasets based on the \"Participant ID\" column\n",
    "Death_Pair_datasets = []\n",
    "Death_Pair_datasets = pd.merge(Death_Date_Record, Death_Cause_Record,   on=\"Participant ID\", how=\"outer\")\n",
    "Death_Pair_datasets = Death_Pair_datasets.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Pair_datasets = {}\n",
    "row_counts_Death_Pair_datasets = {}\n",
    "nan_counts_Death_Pair_datasets = {}\n",
    "empty_counts_Death_Pair_datasets = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Pair_datasets in Death_Pair_datasets.columns:\n",
    "    unique_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].nunique()\n",
    "    row_count_Death_Pair_datasets = len(Death_Pair_datasets[column_Death_Pair_datasets])\n",
    "    nan_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [unique_count_Death_Pair_datasets]\n",
    "    row_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [row_count_Death_Pair_datasets]\n",
    "    nan_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [nan_count_Death_Pair_datasets]\n",
    "    empty_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [empty_count_Death_Pair_datasets]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Pair = []\n",
    "row_counts_Death_Pair = []\n",
    "nan_counts_Death_Pair = []\n",
    "empty_counts_Death_Pair = []\n",
    "\n",
    "unique_counts_Death_Pair = pd.DataFrame(unique_counts_Death_Pair_datasets, index=['Unique Count'])\n",
    "row_counts_Death_Pair = pd.DataFrame(row_counts_Death_Pair_datasets, index=['Row Count'])\n",
    "nan_counts_Death_Pair = pd.DataFrame(nan_counts_Death_Pair_datasets, index=['NaN Count'])\n",
    "empty_counts_Death_Pair = pd.DataFrame(empty_counts_Death_Pair_datasets, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Pair_datasets = []\n",
    "result_Death_Pair_datasets = pd.concat([unique_counts_Death_Pair, row_counts_Death_Pair, nan_counts_Death_Pair, empty_counts_Death_Pair])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Death Date & Cause Paired Datasets:\")\n",
    "#print()\n",
    "#display(result_Death_Pair_datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Death_Pair_datasets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count NaN values in the DataFrame\n",
    "nan_counts_in_Death_Pair_datasets = []\n",
    "nan_counts_in_Death_Pair_datasets = Death_Pair_datasets.isna().sum()\n",
    "#nan_counts_in_Death_Pair_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Boolean mask to identify rows with NaN values\n",
    "nan_mask = []\n",
    "nan_mask = Death_Pair_datasets.isna().any(axis=1)\n",
    "\n",
    "#### Use the mask to extract and display rows with NaN values\n",
    "rows_with_nan_in_Death_Pair_datasets = Death_Pair_datasets[nan_mask]\n",
    "\n",
    "#### Display the rows with NaN values\n",
    "#rows_with_nan_in_Death_Pair_datasets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2m. Merge the datsets: \"Death_Pair_datasets\" and \"All_ICD10_with_Diseases_Dates_Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('All ICD10 Codes with Diseases Names and Dates Data.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset = []\n",
    "Final_Dataset = pd.merge(Death_Pair_datasets,All_ICD10_with_Diseases_Dates_Data,   on=\"Participant ID\", how=\"outer\")\n",
    "new_order = ['Participant ID', 'Year of Birth', 'Month of Birth' , 'Sex' , 'Ethnicity' ,  'Combined ICD10 Diagnosis Date', 'Combined ICD10 Diseases' , 'Date of Death' , 'Death Cause Diseases' , 'Combined ICD10 Codes','Death Cause Disease ICD10 Codes']  # Specify the order of columns\n",
    "Final_Dataset = Final_Dataset[new_order]\n",
    "Final_Dataset = Final_Dataset.sort_values(by=['Participant ID', 'Combined ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Final_Dataset = {}\n",
    "row_counts_Final_Dataset = {}\n",
    "nan_counts_Final_Dataset = {}\n",
    "empty_counts_Final_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Final_Dataset in Final_Dataset.columns:\n",
    "    unique_count_Final_Dataset = Final_Dataset[column_Final_Dataset].nunique()\n",
    "    row_count_Final_Dataset = len(Final_Dataset[column_Final_Dataset])\n",
    "    nan_count_Final_Dataset = Final_Dataset[column_Final_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_Final_Dataset = Final_Dataset[column_Final_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Final_Dataset[column_Final_Dataset] = [unique_count_Final_Dataset]\n",
    "    row_counts_Final_Dataset[column_Final_Dataset] = [row_count_Final_Dataset]\n",
    "    nan_counts_Final_Dataset[column_Final_Dataset] = [nan_count_Final_Dataset]\n",
    "    empty_counts_Final_Dataset[column_Final_Dataset] = [empty_count_Final_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Final_Dataset_Record = []\n",
    "row_counts_Final_Dataset_Record = []\n",
    "nan_counts_Final_Dataset_Record = []\n",
    "empty_counts_Final_Dataset_Record = []\n",
    "\n",
    "unique_counts_Final_Dataset_Record = pd.DataFrame(unique_counts_Final_Dataset, index=['Unique Count'])\n",
    "row_counts_Final_Dataset_Record = pd.DataFrame(row_counts_Final_Dataset, index=['Row Count'])\n",
    "nan_counts_Final_Dataset_Record = pd.DataFrame(nan_counts_Final_Dataset, index=['NaN Count'])\n",
    "empty_counts_Final_Dataset_Record = pd.DataFrame(empty_counts_Final_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Final_Dataset = []\n",
    "result_Final_Dataset = pd.concat([unique_counts_Final_Dataset_Record, row_counts_Final_Dataset_Record, nan_counts_Final_Dataset_Record, empty_counts_Final_Dataset_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "#print()\n",
    "#display(result_Final_Dataset)\n",
    "#print()\n",
    "#Final_Dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2n. Drop Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataset = Final_Dataset.drop_duplicates()\n",
    "Dropped_Duplicates_Final_Dataset = []\n",
    "Dropped_Duplicates_Final_Dataset = Final_Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "row_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Dropped_Duplicates_Final_Dataset in Dropped_Duplicates_Final_Dataset.columns:\n",
    "    unique_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].nunique()\n",
    "    row_count_Dropped_Duplicates_Final_Dataset = len(Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset])\n",
    "    nan_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [unique_count_Dropped_Duplicates_Final_Dataset]\n",
    "    row_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [row_count_Dropped_Duplicates_Final_Dataset]\n",
    "    nan_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [nan_count_Dropped_Duplicates_Final_Dataset]\n",
    "    empty_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [empty_count_Dropped_Duplicates_Final_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "row_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(unique_counts_Dropped_Duplicates_Final_Dataset, index=['Unique Count'])\n",
    "row_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(row_counts_Dropped_Duplicates_Final_Dataset, index=['Row Count'])\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(nan_counts_Dropped_Duplicates_Final_Dataset, index=['NaN Count'])\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(empty_counts_Dropped_Duplicates_Final_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Dropped_Duplicates_Final_Dataset = []\n",
    "result_Dropped_Duplicates_Final_Dataset = pd.concat([unique_counts_Dropped_Duplicates_Final_Dataset_Record, row_counts_Dropped_Duplicates_Final_Dataset_Record, nan_counts_Dropped_Duplicates_Final_Dataset_Record, empty_counts_Dropped_Duplicates_Final_Dataset_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Dropped Duplicates Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "#print()\n",
    "#display(result_Dropped_Duplicates_Final_Dataset)\n",
    "#print()\n",
    "#Final_Dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count NaN values in the DataFrame\n",
    "nan_counts_Final_Dataset = []\n",
    "nan_counts_Final_Dataset = Final_Dataset.isna().sum()\n",
    "#nan_counts_Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Boolean mask to identify rows with NaN values\n",
    "nan_mask_Final_Dataset = []\n",
    "nan_mask_Final_Dataset = Final_Dataset[['Year of Birth', 'Month of Birth' , 'Sex' , 'Combined ICD10 Diagnosis Date' , 'Combined ICD10 Codes' , 'Combined ICD10 Codes']].isna().any(axis=1)\n",
    "\n",
    "### Use the mask to extract and display rows with NaN values\n",
    "rows_with_nan_Final_Dataset = []\n",
    "rows_with_nan_Final_Dataset = Final_Dataset[nan_mask_Final_Dataset]\n",
    "\n",
    "#### Display the rows with NaN values\n",
    "#rows_with_nan_Final_Dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### List of columns with NaN values to consider\n",
    "columns_with_nan = []\n",
    "columns_with_nan = [\"Year of Birth\", \"Month of Birth\", \"Sex\", \"Combined ICD10 Diagnosis Date\", \"Combined ICD10 Diseases\", \"Combined ICD10 Codes\"]\n",
    "columns_with_nan\n",
    "\n",
    "\n",
    "\n",
    "#### Remove rows with NaN values in the specified columns\n",
    "Final_Dataset = Final_Dataset.dropna(subset=columns_with_nan)\n",
    "#Final_Dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count NaN values in the DataFrame\n",
    "nan_counts_Final_Dataset = []\n",
    "nan_counts_Final_Dataset = Final_Dataset.isna().sum()\n",
    "#nan_counts_Final_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2o. Create a new variable in the dataset: \"Alive / Dead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a new column \"Alive / Dead\" based on the condition\n",
    "import numpy as np\n",
    "Final_Dataset['Alive / Dead'] = np.where(Final_Dataset['Date of Death'].isna(), 'Alive', 'Dead')\n",
    "\n",
    "Final_Dataset2 = []\n",
    "Final_Dataset2 = Final_Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Final_Dataset2 = {}\n",
    "row_counts_Final_Dataset2 = {}\n",
    "nan_counts_Final_Dataset2 = {}\n",
    "empty_counts_Final_Dataset2 = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Final_Dataset2 in Final_Dataset2.columns:\n",
    "    unique_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].nunique()\n",
    "    row_count_Final_Dataset2 = len(Final_Dataset2[column_Final_Dataset2])\n",
    "    nan_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].isna().sum()  # Count NaN values\n",
    "    empty_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Final_Dataset2[column_Final_Dataset2] = [unique_count_Final_Dataset2]\n",
    "    row_counts_Final_Dataset2[column_Final_Dataset2] = [row_count_Final_Dataset2]\n",
    "    nan_counts_Final_Dataset2[column_Final_Dataset2] = [nan_count_Final_Dataset2]\n",
    "    empty_counts_Final_Dataset2[column_Final_Dataset2] = [empty_count_Final_Dataset2]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Final_Dataset_Record2 = []\n",
    "row_counts_Final_Dataset_Record2 = []\n",
    "nan_counts_Final_Dataset_Record2 = []\n",
    "empty_counts_Final_Dataset_Record2 = []\n",
    "\n",
    "unique_counts_Final_Dataset_Record2 = pd.DataFrame(unique_counts_Final_Dataset2, index=['Unique Count'])\n",
    "row_counts_Final_Dataset_Record2 = pd.DataFrame(row_counts_Final_Dataset2, index=['Row Count'])\n",
    "nan_counts_Final_Dataset_Record2 = pd.DataFrame(nan_counts_Final_Dataset2, index=['NaN Count'])\n",
    "empty_counts_Final_Dataset_Record2 = pd.DataFrame(empty_counts_Final_Dataset2, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Final_Dataset2 = []\n",
    "result_Final_Dataset2 = pd.concat([unique_counts_Final_Dataset_Record2, row_counts_Final_Dataset_Record2, nan_counts_Final_Dataset_Record2, empty_counts_Final_Dataset_Record2])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Updated Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "#print()\n",
    "#display(result_Final_Dataset2)\n",
    "#print()\n",
    "\n",
    "\n",
    "#Final_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2p. Create the \"Age\" column in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = []\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = pd.DataFrame(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records)\n",
    "\n",
    "# Convert 'Date of Death' to datetime, handling errors by coercing to NaT\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Date of Death'] = pd.to_datetime(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Date of Death'], errors='coerce')\n",
    "\n",
    "# Calculate the age, only where 'Date of Death' is not NaT\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Individual Age'] = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.apply(\n",
    "    lambda row: row['Date of Death'].year - row['Year of Birth'] if pd.notnull(row['Date of Death']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Convert 'Combined ICD10 Diagnosis Date' to datetime\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Diagnosis Date'] = pd.to_datetime(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Diagnosis Date'], errors='coerce')\n",
    "\n",
    "# Find the latest diagnosis date for each participant\n",
    "latest_dates = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.groupby('Participant ID')['Combined ICD10 Diagnosis Date'].max().reset_index()\n",
    "\n",
    "# Rename the column to 'Latest Diagnosis Date'\n",
    "latest_dates.rename(columns={'Combined ICD10 Diagnosis Date': 'Latest Diagnosis Date'}, inplace=True)\n",
    "\n",
    "# Merge the latest dates back into the original DataFrame\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.merge(latest_dates, on='Participant ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Convert 'Date of Death' to datetime, handling errors by coercing to NaT\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Latest Diagnosis Date'] = pd.to_datetime(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Latest Diagnosis Date'], errors='coerce')\n",
    "\n",
    "\n",
    "# Replace NaN values in 'Individual Age' with the difference between 'Latest Diagnosis Date' and 'Year of Birth'\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Individual Age'] = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.apply(\n",
    "    lambda row: row['Latest Diagnosis Date'].year - row['Year of Birth'] if pd.isnull(row['Individual Age']) else row['Individual Age'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Drop the 'Latest Diagnosis Date' column\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.drop(columns=['Latest Diagnosis Date'], inplace=True)\n",
    "\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the age ranges\n",
    "bins = [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = [\"30 - 40\", \"41 - 50\", \"51 - 60\", \"61 - 70\", \"71 - 80\", \"81 - 90\", \"91 - 100\"]\n",
    "\n",
    "# Create a new column 'Age Range' with the defined bins\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Age Range'] = pd.cut(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Individual Age'], bins=bins, labels=labels, right=False)\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'Dataset with ICD10 and Diseases Names and Death Records.csv'\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center> Age - Range Count -for UKB</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = []\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "# Count occurrences of each age range\n",
    "age_range_counts = []\n",
    "age_range_counts = unique_participants['Age Range'].value_counts().sort_index()\n",
    "\n",
    "# Calculate percentages\n",
    "age_range_percentages = (age_range_counts / total_participants) * 100\n",
    "\n",
    "# Display the results\n",
    "for age_range, count in age_range_counts.items():\n",
    "    percentage = age_range_percentages[age_range]\n",
    "    print(f\"{age_range} : {count} ({percentage:.2f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>3. Sex and Ethnicity Count -for UKB</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a. Import the \"Final Dataset\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the combined DataFrame\n",
    "#print(\"Dropped Duplicates from Data Table Record:\")\n",
    "#print()\n",
    "#display(result_dropped_duplicates_data_table)\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Data = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_All_ICD10_with_Diseases_Dates_Data in All_ICD10_with_Diseases_Dates_Data.columns:\n",
    "    unique_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].nunique()\n",
    "    row_count_All_ICD10_with_Diseases_Dates_Data = len(All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data])\n",
    "    nan_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].isna().sum()  # Count NaN values\n",
    "    empty_count_All_ICD10_with_Diseases_Dates_Data = All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [unique_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    row_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [row_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    nan_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [nan_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "    empty_counts_All_ICD10_with_Diseases_Dates_Data[column_All_ICD10_with_Diseases_Dates_Data] = [empty_count_All_ICD10_with_Diseases_Dates_Data]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Datas = []\n",
    "\n",
    "unique_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(unique_counts_All_ICD10_with_Diseases_Dates_Data, index=['Unique Count'])\n",
    "row_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(row_counts_All_ICD10_with_Diseases_Dates_Data, index=['Row Count'])\n",
    "nan_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(nan_counts_All_ICD10_with_Diseases_Dates_Data, index=['NaN Count'])\n",
    "empty_counts_All_ICD10_with_Diseases_Dates_Datas = pd.DataFrame(empty_counts_All_ICD10_with_Diseases_Dates_Data, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_All_ICD10_with_Diseases_Dates_Data = []\n",
    "result_All_ICD10_with_Diseases_Dates_Data = pd.concat([unique_counts_All_ICD10_with_Diseases_Dates_Datas, row_counts_All_ICD10_with_Diseases_Dates_Datas, nan_counts_All_ICD10_with_Diseases_Dates_Datas, empty_counts_All_ICD10_with_Diseases_Dates_Datas])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"All_ICD10_with_Diseases_Dates_Data Record:\")\n",
    "#print()\n",
    "#display(result_All_ICD10_with_Diseases_Dates_Data)\n",
    "#print()\n",
    "\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a. Count the number of Male and Female and Ethnicity items counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = All_ICD10_with_Diseases_Dates_Data.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "# Count the number of each sex\n",
    "sex_counts = []\n",
    "sex_counts = unique_participants['Sex'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "sex_percentages = []\n",
    "sex_percentages = (sex_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "sex_counts_with_percentages = []\n",
    "sex_counts_with_percentages = sex_counts.astype(str) + \" (\" + sex_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of missing (NaN) values in each column\n",
    "missing_counts = []\n",
    "missing_counts = unique_participants.isna().sum()\n",
    "\n",
    "# Get unique items in the \"Ethnicity\" column\n",
    "unique_ethnicities = []\n",
    "unique_ethnicities = unique_participants['Ethnicity'].unique()\n",
    "\n",
    "# Count the number of each unique ethnicity\n",
    "ethnicity_counts = []\n",
    "ethnicity_counts = unique_participants['Ethnicity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "ethnicity_percentages = []\n",
    "ethnicity_percentages = (ethnicity_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "ethnicity_counts_with_percentages = []\n",
    "ethnicity_counts_with_percentages = ethnicity_counts.astype(str) + \" (\" + ethnicity_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "# Print the counts\n",
    "#print(\"Sex Counts with Percentages:\")\n",
    "#print(sex_counts_with_percentages)\n",
    "#print()\n",
    "#print(\"\\nEthnicity Counts with Percentages:\")\n",
    "#print(ethnicity_counts_with_percentages)\n",
    "#print()\n",
    "#print(\"\\nMissing Values Counts:\")\n",
    "#print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>4. Importing UK Biobank Dataset (Smoking Records)-for UKB</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a. Import Smoking Dataset from UK Biobank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKB_Smoking_Records = []\n",
    "UKB_Smoking_Records = pd.read_csv('UK_biobank_Smoking_data.csv')\n",
    "\n",
    "# Rename multiple variables\n",
    "UKB_Smoking_Records = UKB_Smoking_Records.rename(columns={\"eid\": \"Participant ID\", \n",
    "                        \"20116-0.0\": \"Smoking status_Instance 0(2006-2010)\",\n",
    "                        \"20116-1.0\": \"Smoking status_Instance 1(2012-2013)\",\n",
    "                        \"20116-2.0\": \"Smoking status_Instance 2(2014+)\",\n",
    "                        \"20116-3.0\": \"Smoking status_Instance 3(2019+)\",\n",
    "                        \"2644-0.0\": \"Light smokers_Instance 0(2006-2010)\",\n",
    "                        \"2644-1.0\": \"Light smokers_Instance 0(2012-2013)\",\n",
    "                        \"2644-2.0\": \"Light smokers_Instance 0(2014+)\",\n",
    "                        \"2644-3.0\": \"Light smokers_Instance 0(2019+)\"})\n",
    "UKB_Smoking_Records = UKB_Smoking_Records.drop_duplicates()\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_Smoking_Records = {}\n",
    "row_counts_UKB_Smoking_Records = {}\n",
    "nan_counts_UKB_Smoking_Records = {}\n",
    "empty_counts_UKB_Smoking_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_Smoking_Records in UKB_Smoking_Records.columns:\n",
    "    unique_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].nunique()\n",
    "    row_count_UKB_Smoking_Records = len(UKB_Smoking_Records[column_UKB_Smoking_Records])\n",
    "    nan_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [unique_count_UKB_Smoking_Records]\n",
    "    row_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [row_count_UKB_Smoking_Records]\n",
    "    nan_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [nan_count_UKB_Smoking_Records]\n",
    "    empty_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [empty_count_UKB_Smoking_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_Smoking_Record = []\n",
    "row_counts_UKB_Smoking_Record = []\n",
    "nan_counts_UKB_Smoking_Record = []\n",
    "empty_counts_UKB_Smoking_Record = []\n",
    "\n",
    "unique_counts_UKB_Smoking_Record = pd.DataFrame(unique_counts_UKB_Smoking_Records, index=['Unique Count'])\n",
    "row_counts_UKB_Smoking_Record = pd.DataFrame(row_counts_UKB_Smoking_Records, index=['Row Count'])\n",
    "nan_counts_UKB_Smoking_Record = pd.DataFrame(nan_counts_UKB_Smoking_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_Smoking_Record = pd.DataFrame(empty_counts_UKB_Smoking_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_Smoking_Records = []\n",
    "result_UKB_Smoking_Records = pd.concat([unique_counts_UKB_Smoking_Record, row_counts_UKB_Smoking_Record, nan_counts_UKB_Smoking_Record, empty_counts_UKB_Smoking_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_Smoking_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_Smoking_Records)\n",
    "\n",
    "#UKB_Smoking_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b. I removed the all the columns named \"Light smokers_Instance\" as they are not required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = []\n",
    "columns_to_check = ['Smoking status_Instance 0(2006-2010)',\n",
    "                    'Smoking status_Instance 1(2012-2013)',\n",
    "                    'Smoking status_Instance 2(2014+)',\n",
    "                    'Smoking status_Instance 3(2019+)']\n",
    "\n",
    "# Create a new column 'Smoking Status' and fill it with non-NaN values from the specified columns\n",
    "UKB_Smoking_Records['Smoking Status'] = UKB_Smoking_Records[columns_to_check].apply(lambda row: next((value for value in row if not pd.isna(value)), np.nan), axis=1)\n",
    "\n",
    "\n",
    "columns_to_drop = []\n",
    "columns_to_drop = ['Smoking status_Instance 0(2006-2010)',\n",
    "                   'Smoking status_Instance 1(2012-2013)',\n",
    "                   'Smoking status_Instance 2(2014+)',\n",
    "                   'Smoking status_Instance 3(2019+)']\n",
    "\n",
    "# Drop the specified columns\n",
    "UKB_Smoking_Records.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "columns_to_check = []\n",
    "columns_to_check = ['Light smokers_Instance 0(2006-2010)',\n",
    "                    'Light smokers_Instance 0(2012-2013)',\n",
    "                    'Light smokers_Instance 0(2014+)',\n",
    "                    'Light smokers_Instance 0(2019+)']\n",
    "\n",
    "# Create a new column 'Smoking Status' and fill it with non-NaN values from the specified columns\n",
    "#UKB_Smoking_Records['Occassional Smoking'] = UKB_Smoking_Records[columns_to_check].apply(lambda row: next((value for value in row if not pd.isna(value)), np.nan), axis=1)\n",
    "\n",
    "\n",
    "columns_to_drop = []\n",
    "columns_to_drop = ['Light smokers_Instance 0(2006-2010)',\n",
    "                    'Light smokers_Instance 0(2012-2013)',\n",
    "                    'Light smokers_Instance 0(2014+)',\n",
    "                    'Light smokers_Instance 0(2019+)']\n",
    "\n",
    "# Drop the specified columns\n",
    "UKB_Smoking_Records.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_Smoking_Records = {}\n",
    "row_counts_UKB_Smoking_Records = {}\n",
    "nan_counts_UKB_Smoking_Records = {}\n",
    "empty_counts_UKB_Smoking_Records = {}\n",
    "prefer_not_to_say_counts_UKB_Smoking_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_Smoking_Records in UKB_Smoking_Records.columns:\n",
    "    unique_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].nunique()\n",
    "    row_count_UKB_Smoking_Records = len(UKB_Smoking_Records[column_UKB_Smoking_Records])\n",
    "    nan_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [unique_count_UKB_Smoking_Records]\n",
    "    row_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [row_count_UKB_Smoking_Records]\n",
    "    nan_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [nan_count_UKB_Smoking_Records]\n",
    "    empty_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [empty_count_UKB_Smoking_Records]\n",
    "    prefer_not_to_say_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [prefer_not_to_say_count_UKB_Smoking_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_Smoking_Record = []\n",
    "row_counts_UKB_Smoking_Record = []\n",
    "nan_counts_UKB_Smoking_Record = []\n",
    "empty_counts_UKB_Smoking_Record = []\n",
    "prefer_not_to_say_counts_UKB_Smoking_Record =[]\n",
    "\n",
    "unique_counts_UKB_Smoking_Record = pd.DataFrame(unique_counts_UKB_Smoking_Records, index=['Unique Count'])\n",
    "row_counts_UKB_Smoking_Record = pd.DataFrame(row_counts_UKB_Smoking_Records, index=['Row Count'])\n",
    "nan_counts_UKB_Smoking_Record = pd.DataFrame(nan_counts_UKB_Smoking_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_Smoking_Record = pd.DataFrame(empty_counts_UKB_Smoking_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_UKB_Smoking_Record = pd.DataFrame(prefer_not_to_say_counts_UKB_Smoking_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_Smoking_Records = []\n",
    "result_UKB_Smoking_Records = pd.concat([unique_counts_UKB_Smoking_Record, row_counts_UKB_Smoking_Record, nan_counts_UKB_Smoking_Record, empty_counts_UKB_Smoking_Record,prefer_not_to_say_counts_UKB_Smoking_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"UKB_Smoking_Records:\")\n",
    "print()\n",
    "display(result_UKB_Smoking_Records)\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"Smoking Status\"\n",
    "unique_smoking_status = []\n",
    "unique_smoking_status = UKB_Smoking_Records['Smoking Status'].unique()\n",
    "# Print the unique items\n",
    "display(unique_smoking_status)\n",
    "\n",
    "\n",
    "# Display unique items under the column \"Smoking Status\"\n",
    "#unique_smoking_status = []\n",
    "#unique_smoking_status = UKB_Smoking_Records['Occassional Smoking'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_smoking_status)\n",
    "\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#UKB_Smoking_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'Smoking Status Record Dataset.csv'\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#UKB_Smoking_Records.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c. Remove rows with NaN and \"Prefer not to answer\" values from the DataFrame named \"UKB_Smoking_Records\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the \"Smoking Status\" column\n",
    "UKB_Smoking_Records = UKB_Smoking_Records.dropna(subset=[\"Smoking Status\"])\n",
    "\n",
    "# Filter out rows with \"Prefer not to answer\" values in the \"Smoking Status\" column\n",
    "UKB_Smoking_Records = UKB_Smoking_Records[UKB_Smoking_Records[\"Smoking Status\"] != \"Prefer not to answer\"]\n",
    "\n",
    "# Reset index\n",
    "UKB_Smoking_Records.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_Smoking_Records = {}\n",
    "row_counts_UKB_Smoking_Records = {}\n",
    "nan_counts_UKB_Smoking_Records = {}\n",
    "empty_counts_UKB_Smoking_Records = {}\n",
    "prefer_not_to_say_counts_UKB_Smoking_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_Smoking_Records in UKB_Smoking_Records.columns:\n",
    "    unique_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].nunique()\n",
    "    row_count_UKB_Smoking_Records = len(UKB_Smoking_Records[column_UKB_Smoking_Records])\n",
    "    nan_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_UKB_Smoking_Records = UKB_Smoking_Records[column_UKB_Smoking_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [unique_count_UKB_Smoking_Records]\n",
    "    row_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [row_count_UKB_Smoking_Records]\n",
    "    nan_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [nan_count_UKB_Smoking_Records]\n",
    "    empty_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [empty_count_UKB_Smoking_Records]\n",
    "    prefer_not_to_say_counts_UKB_Smoking_Records[column_UKB_Smoking_Records] = [prefer_not_to_say_count_UKB_Smoking_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_Smoking_Record = []\n",
    "row_counts_UKB_Smoking_Record = []\n",
    "nan_counts_UKB_Smoking_Record = []\n",
    "empty_counts_UKB_Smoking_Record = []\n",
    "prefer_not_to_say_counts_UKB_Smoking_Record =[]\n",
    "\n",
    "unique_counts_UKB_Smoking_Record = pd.DataFrame(unique_counts_UKB_Smoking_Records, index=['Unique Count'])\n",
    "row_counts_UKB_Smoking_Record = pd.DataFrame(row_counts_UKB_Smoking_Records, index=['Row Count'])\n",
    "nan_counts_UKB_Smoking_Record = pd.DataFrame(nan_counts_UKB_Smoking_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_Smoking_Record = pd.DataFrame(empty_counts_UKB_Smoking_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_UKB_Smoking_Record = pd.DataFrame(prefer_not_to_say_counts_UKB_Smoking_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_Smoking_Records = []\n",
    "result_UKB_Smoking_Records = pd.concat([unique_counts_UKB_Smoking_Record, row_counts_UKB_Smoking_Record, nan_counts_UKB_Smoking_Record, empty_counts_UKB_Smoking_Record,prefer_not_to_say_counts_UKB_Smoking_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_Smoking_Records:\")\n",
    "print()\n",
    "#display(result_UKB_Smoking_Records)\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"Smoking Status\"\n",
    "unique_smoking_status = []\n",
    "unique_smoking_status = UKB_Smoking_Records['Smoking Status'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_smoking_status)\n",
    "print()\n",
    "\n",
    "# Count occurrences of unique values in the 'Smoking Status' column\n",
    "smoking_status_counts = UKB_Smoking_Records['Smoking Status'].value_counts()\n",
    "print()\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'Smoking Status' column:\")\n",
    "#display(smoking_status_counts)\n",
    "\n",
    "#UKB_Smoking_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d. Combine ICD10 codes dataset with UKB Smoking Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ICD10_codes_and_UKB_Smoking_Records = []\n",
    "Combined_ICD10_codes_and_UKB_Smoking_Records = pd.merge(All_ICD10_with_Diseases_Dates_Data,UKB_Smoking_Records,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Records = {}\n",
    "row_counts_Combined_ICD10_codes_and_UKB_Smoking_Records = {}\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Records = {}\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Records = {}\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_codes_and_UKB_Smoking_Records in Combined_ICD10_codes_and_UKB_Smoking_Records.columns:\n",
    "    unique_count_Combined_ICD10_codes_and_UKB_Smoking_Records = Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records].nunique()\n",
    "    row_count_Combined_ICD10_codes_and_UKB_Smoking_Records = len(Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records])\n",
    "    nan_count_Combined_ICD10_codes_and_UKB_Smoking_Records = Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_codes_and_UKB_Smoking_Records = Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_Smoking_Records = Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records] = [unique_count_Combined_ICD10_codes_and_UKB_Smoking_Records]\n",
    "    row_counts_Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records] = [row_count_Combined_ICD10_codes_and_UKB_Smoking_Records]\n",
    "    nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records] = [nan_count_Combined_ICD10_codes_and_UKB_Smoking_Records]\n",
    "    empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records] = [empty_count_Combined_ICD10_codes_and_UKB_Smoking_Records]\n",
    "    prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Records[column_Combined_ICD10_codes_and_UKB_Smoking_Records] = [prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_Smoking_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = []\n",
    "row_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = []\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = []\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = []\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Record =[]\n",
    "\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = pd.DataFrame(unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Records, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = pd.DataFrame(row_counts_Combined_ICD10_codes_and_UKB_Smoking_Records, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = pd.DataFrame(nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Records, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = pd.DataFrame(empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Record = pd.DataFrame(prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_codes_and_UKB_Smoking_Records = []\n",
    "result_Combined_ICD10_codes_and_UKB_Smoking_Records = pd.concat([unique_counts_Combined_ICD10_codes_and_UKB_Smoking_Record, row_counts_Combined_ICD10_codes_and_UKB_Smoking_Record, nan_counts_Combined_ICD10_codes_and_UKB_Smoking_Record, empty_counts_Combined_ICD10_codes_and_UKB_Smoking_Record,prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_Smoking_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined_ICD10_codes_and_UKB_Smoking_Records:\")\n",
    "display(result_Combined_ICD10_codes_and_UKB_Smoking_Records)\n",
    "print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = Combined_ICD10_codes_and_UKB_Smoking_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"Smoking Status\"\n",
    "unique_smoking_status = []\n",
    "unique_smoking_status = unique_participants['Smoking Status'].unique()\n",
    "# Print the unique items\n",
    "display(unique_smoking_status)\n",
    "print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'Smoking Status' column\n",
    "smoking_status_counts = []\n",
    "smoking_status_counts = unique_participants['Smoking Status'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "smoking_status_percentages = []\n",
    "smoking_status_percentages = (smoking_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "smoking_status_counts_with_percentages = []\n",
    "smoking_status_counts_with_percentages = smoking_status_counts.astype(str) + \" (\" + smoking_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'Smoking Status' column:\")\n",
    "#display(smoking_status_counts_with_percentages)\n",
    "#print()\n",
    "\n",
    "\n",
    "#Combined_ICD10_codes_and_UKB_Smoking_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>5. Importing UK Biobank Dataset (BMI Records)-for UKB</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKB_BMI_Records = []\n",
    "UKB_BMI_Records = pd.read_csv('UK_biobank_BMI_data.csv')\n",
    "\n",
    "# Rename multiple variables\n",
    "UKB_BMI_Records = UKB_BMI_Records.rename(columns={\"eid\": \"Participant ID\", \n",
    "                        \"21001-0.0\": \"Body mass index (BMI)_Instance 0(2006-2010)\",\n",
    "                        \"21001-1.0\": \"Body mass index (BMI)_Instance 1(2012-2013)\",\n",
    "                        \"21001-2.0\": \"Body mass index (BMI)_Instance 2(2014+)\",\n",
    "                        \"21001-3.0\": \"Body mass index (BMI)_Instance 3(2019+)\"})\n",
    "\n",
    "UKB_BMI_Records = UKB_BMI_Records.drop_duplicates()\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_BMI_Records = {}\n",
    "row_counts_UKB_BMI_Records = {}\n",
    "nan_counts_UKB_BMI_Records = {}\n",
    "empty_counts_UKB_BMI_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_BMI_Records in UKB_BMI_Records.columns:\n",
    "    unique_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].nunique()\n",
    "    row_count_UKB_BMI_Records = len(UKB_BMI_Records[column_UKB_BMI_Records])\n",
    "    nan_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [unique_count_UKB_BMI_Records]\n",
    "    row_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [row_count_UKB_BMI_Records]\n",
    "    nan_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [nan_count_UKB_BMI_Records]\n",
    "    empty_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [empty_count_UKB_BMI_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_BMI_Record = []\n",
    "row_counts_UKB_BMI_Record = []\n",
    "nan_counts_UKB_BMI_Record = []\n",
    "empty_counts_UKB_BMI_Record = []\n",
    "\n",
    "unique_counts_UKB_BMI_Record = pd.DataFrame(unique_counts_UKB_BMI_Records, index=['Unique Count'])\n",
    "row_counts_UKB_BMI_Record = pd.DataFrame(row_counts_UKB_BMI_Records, index=['Row Count'])\n",
    "nan_counts_UKB_BMI_Record = pd.DataFrame(nan_counts_UKB_BMI_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_BMI_Record = pd.DataFrame(empty_counts_UKB_BMI_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_BMI_Records = []\n",
    "result_UKB_BMI_Records = pd.concat([unique_counts_UKB_BMI_Record, row_counts_UKB_BMI_Record, nan_counts_UKB_BMI_Record, empty_counts_UKB_BMI_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_BMI_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_BMI_Records)\n",
    "\n",
    "\n",
    "#UKB_BMI_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a. I combined the all the columns named \"Body mass index (BMI)_Instance\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = []\n",
    "columns_to_check = ['Body mass index (BMI)_Instance 0(2006-2010)',\n",
    "                    'Body mass index (BMI)_Instance 1(2012-2013)',\n",
    "                    'Body mass index (BMI)_Instance 2(2014+)',\n",
    "                    'Body mass index (BMI)_Instance 3(2019+)']\n",
    "\n",
    "# Create a new column 'Body mass index (BMI)' and fill it with non-NaN values from the specified columns\n",
    "UKB_BMI_Records['Body mass index (BMI)'] = UKB_BMI_Records[columns_to_check].apply(lambda row: next((value for value in row if not pd.isna(value)), np.nan), axis=1)\n",
    "\n",
    "\n",
    "columns_to_drop = []\n",
    "columns_to_drop = ['Body mass index (BMI)_Instance 0(2006-2010)',\n",
    "                    'Body mass index (BMI)_Instance 1(2012-2013)',\n",
    "                    'Body mass index (BMI)_Instance 2(2014+)',\n",
    "                    'Body mass index (BMI)_Instance 3(2019+)']\n",
    "\n",
    "# Drop the specified columns\n",
    "UKB_BMI_Records.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_BMI_Records = {}\n",
    "row_counts_UKB_BMI_Records = {}\n",
    "nan_counts_UKB_BMI_Records = {}\n",
    "empty_counts_UKB_BMI_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_BMI_Records in UKB_BMI_Records.columns:\n",
    "    unique_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].nunique()\n",
    "    row_count_UKB_BMI_Records = len(UKB_BMI_Records[column_UKB_BMI_Records])\n",
    "    nan_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [unique_count_UKB_BMI_Records]\n",
    "    row_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [row_count_UKB_BMI_Records]\n",
    "    nan_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [nan_count_UKB_BMI_Records]\n",
    "    empty_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [empty_count_UKB_BMI_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_BMI_Record = []\n",
    "row_counts_UKB_BMI_Record = []\n",
    "nan_counts_UKB_BMI_Record = []\n",
    "empty_counts_UKB_BMI_Record = []\n",
    "\n",
    "unique_counts_UKB_BMI_Record = pd.DataFrame(unique_counts_UKB_BMI_Records, index=['Unique Count'])\n",
    "row_counts_UKB_BMI_Record = pd.DataFrame(row_counts_UKB_BMI_Records, index=['Row Count'])\n",
    "nan_counts_UKB_BMI_Record = pd.DataFrame(nan_counts_UKB_BMI_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_BMI_Record = pd.DataFrame(empty_counts_UKB_BMI_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_BMI_Records = []\n",
    "result_UKB_BMI_Records = pd.concat([unique_counts_UKB_BMI_Record, row_counts_UKB_BMI_Record, nan_counts_UKB_BMI_Record, empty_counts_UKB_BMI_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_BMI_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_BMI_Records)\n",
    "\n",
    "\n",
    "#UKB_BMI_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b. Remove the \"NaN\" or missing values from BMI column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the \"Body mass index (BMI)\" column\n",
    "UKB_BMI_Records = UKB_BMI_Records.dropna(subset=[\"Body mass index (BMI)\"])\n",
    "\n",
    "# Reset index\n",
    "UKB_BMI_Records.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_BMI_Records = {}\n",
    "row_counts_UKB_BMI_Records = {}\n",
    "nan_counts_UKB_BMI_Records = {}\n",
    "empty_counts_UKB_BMI_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_BMI_Records in UKB_BMI_Records.columns:\n",
    "    unique_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].nunique()\n",
    "    row_count_UKB_BMI_Records = len(UKB_BMI_Records[column_UKB_BMI_Records])\n",
    "    nan_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_BMI_Records = UKB_BMI_Records[column_UKB_BMI_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [unique_count_UKB_BMI_Records]\n",
    "    row_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [row_count_UKB_BMI_Records]\n",
    "    nan_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [nan_count_UKB_BMI_Records]\n",
    "    empty_counts_UKB_BMI_Records[column_UKB_BMI_Records] = [empty_count_UKB_BMI_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_BMI_Record = []\n",
    "row_counts_UKB_BMI_Record = []\n",
    "nan_counts_UKB_BMI_Record = []\n",
    "empty_counts_UKB_BMI_Record = []\n",
    "\n",
    "unique_counts_UKB_BMI_Record = pd.DataFrame(unique_counts_UKB_BMI_Records, index=['Unique Count'])\n",
    "row_counts_UKB_BMI_Record = pd.DataFrame(row_counts_UKB_BMI_Records, index=['Row Count'])\n",
    "nan_counts_UKB_BMI_Record = pd.DataFrame(nan_counts_UKB_BMI_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_BMI_Record = pd.DataFrame(empty_counts_UKB_BMI_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_BMI_Records = []\n",
    "result_UKB_BMI_Records = pd.concat([unique_counts_UKB_BMI_Record, row_counts_UKB_BMI_Record, nan_counts_UKB_BMI_Record, empty_counts_UKB_BMI_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_BMI_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_BMI_Records)\n",
    "\n",
    "\n",
    "#UKB_BMI_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c. Define the BMI range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map BMI values to BMI ranges\n",
    "def map_bmi_range(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return \"Underweight\"\n",
    "    elif 18.5 <= bmi <= 24.9:\n",
    "        return \"Healthy Weight\"\n",
    "    elif 25.0 <= bmi <= 29.9:\n",
    "        return \"Overweight\"\n",
    "    else:\n",
    "        return \"Obesity\"\n",
    "\n",
    "# Apply the function to create the 'BMI Range' column\n",
    "UKB_BMI_Records['BMI Range'] = UKB_BMI_Records['Body mass index (BMI)'].apply(map_bmi_range)\n",
    "\n",
    "# Count occurrences of each BMI range\n",
    "bmi_range_counts = UKB_BMI_Records['BMI Range'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts for each BMI range:\")\n",
    "#display(bmi_range_counts)\n",
    "\n",
    "# Display the DataFrame with the new 'BMI Range' column\n",
    "#UKB_BMI_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'BMI Range Record Dataset.csv'\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#UKB_BMI_Records.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e. Combine BMI Records with ICD10 Codes Structured dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ICD10_codes_and_UKB_BMI_Records = []\n",
    "Combined_ICD10_codes_and_UKB_BMI_Records = pd.merge(All_ICD10_with_Diseases_Dates_Data,UKB_BMI_Records,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_BMI_Records = {}\n",
    "row_counts_Combined_ICD10_codes_and_UKB_BMI_Records = {}\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_BMI_Records = {}\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_BMI_Records = {}\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_codes_and_UKB_BMI_Records in Combined_ICD10_codes_and_UKB_BMI_Records.columns:\n",
    "    unique_count_Combined_ICD10_codes_and_UKB_BMI_Records = Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records].nunique()\n",
    "    row_count_Combined_ICD10_codes_and_UKB_BMI_Records = len(Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records])\n",
    "    nan_count_Combined_ICD10_codes_and_UKB_BMI_Records = Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_codes_and_UKB_BMI_Records = Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_BMI_Records = Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records] = [unique_count_Combined_ICD10_codes_and_UKB_BMI_Records]\n",
    "    row_counts_Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records] = [row_count_Combined_ICD10_codes_and_UKB_BMI_Records]\n",
    "    nan_counts_Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records] = [nan_count_Combined_ICD10_codes_and_UKB_BMI_Records]\n",
    "    empty_counts_Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records] = [empty_count_Combined_ICD10_codes_and_UKB_BMI_Records]\n",
    "    prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Records[column_Combined_ICD10_codes_and_UKB_BMI_Records] = [prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_BMI_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_BMI_Record = []\n",
    "row_counts_Combined_ICD10_codes_and_UKB_BMI_Record = []\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_BMI_Record = []\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_BMI_Record = []\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Record =[]\n",
    "\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_BMI_Record = pd.DataFrame(unique_counts_Combined_ICD10_codes_and_UKB_BMI_Records, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_codes_and_UKB_BMI_Record = pd.DataFrame(row_counts_Combined_ICD10_codes_and_UKB_BMI_Records, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_BMI_Record = pd.DataFrame(nan_counts_Combined_ICD10_codes_and_UKB_BMI_Records, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_BMI_Record = pd.DataFrame(empty_counts_Combined_ICD10_codes_and_UKB_BMI_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Record = pd.DataFrame(prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_codes_and_UKB_BMI_Records = []\n",
    "result_Combined_ICD10_codes_and_UKB_BMI_Records = pd.concat([unique_counts_Combined_ICD10_codes_and_UKB_BMI_Record, row_counts_Combined_ICD10_codes_and_UKB_BMI_Record, nan_counts_Combined_ICD10_codes_and_UKB_BMI_Record, empty_counts_Combined_ICD10_codes_and_UKB_BMI_Record,prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_BMI_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined_ICD10_codes_and_UKB_BMI_Records:\")\n",
    "display(result_Combined_ICD10_codes_and_UKB_BMI_Records)\n",
    "print()\n",
    "display(Combined_ICD10_codes_and_UKB_BMI_Records.head(3))\n",
    "print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = Combined_ICD10_codes_and_UKB_BMI_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"BMI Range\"\n",
    "unique_BMI_status = []\n",
    "unique_BMI_status = unique_participants['BMI Range'].unique()\n",
    "# Print the unique items\n",
    "display(unique_BMI_status)\n",
    "print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'BMI Range' column\n",
    "BMI_status_counts = []\n",
    "BMI_status_counts = unique_participants['BMI Range'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "BMI_status_percentages = []\n",
    "BMI_status_percentages = (BMI_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "BMI_status_counts_with_percentages = []\n",
    "BMI_status_counts_with_percentages = BMI_status_counts.astype(str) + \" (\" + BMI_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'BMI Range' column:\")\n",
    "#display(BMI_status_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>5. Importing UK Biobank Dataset (IMD Records) - for UKB</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKB_IMD_Records = []\n",
    "UKB_IMD_Records = pd.read_csv('UK_biobank_IMD_data.csv')\n",
    "\n",
    "# Rename multiple variables\n",
    "UKB_IMD_Records = UKB_IMD_Records.rename(columns={\"eid\": \"Participant ID\", \n",
    "                        \"26410-0.0\": \"Index of Multiple Deprivation (England)\"})\n",
    "\n",
    "UKB_IMD_Records = UKB_IMD_Records.drop_duplicates()\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_IMD_Records = {}\n",
    "row_counts_UKB_IMD_Records = {}\n",
    "nan_counts_UKB_IMD_Records = {}\n",
    "empty_counts_UKB_IMD_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_IMD_Records in UKB_IMD_Records.columns:\n",
    "    unique_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].nunique()\n",
    "    row_count_UKB_IMD_Records = len(UKB_IMD_Records[column_UKB_IMD_Records])\n",
    "    nan_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [unique_count_UKB_IMD_Records]\n",
    "    row_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [row_count_UKB_IMD_Records]\n",
    "    nan_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [nan_count_UKB_IMD_Records]\n",
    "    empty_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [empty_count_UKB_IMD_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_IMD_Record = []\n",
    "row_counts_UKB_IMD_Record = []\n",
    "nan_counts_UKB_IMD_Record = []\n",
    "empty_counts_UKB_IMD_Record = []\n",
    "\n",
    "unique_counts_UKB_IMD_Record = pd.DataFrame(unique_counts_UKB_IMD_Records, index=['Unique Count'])\n",
    "row_counts_UKB_IMD_Record = pd.DataFrame(row_counts_UKB_IMD_Records, index=['Row Count'])\n",
    "nan_counts_UKB_IMD_Record = pd.DataFrame(nan_counts_UKB_IMD_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_IMD_Record = pd.DataFrame(empty_counts_UKB_IMD_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_IMD_Records = []\n",
    "result_UKB_IMD_Records = pd.concat([unique_counts_UKB_IMD_Record, row_counts_UKB_IMD_Record, nan_counts_UKB_IMD_Record, empty_counts_UKB_IMD_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_IMD_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_IMD_Records)\n",
    "\n",
    "\n",
    "\n",
    "#UKB_IMD_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a. Remove the \"NaN\" or missing values from Index of Multiple Deprivation (England) column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the \"Index of Multiple Deprivation (England)\" column\n",
    "UKB_IMD_Records = UKB_IMD_Records.dropna(subset=[\"Index of Multiple Deprivation (England)\"])\n",
    "\n",
    "# Reset index\n",
    "UKB_IMD_Records.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_IMD_Records = {}\n",
    "row_counts_UKB_IMD_Records = {}\n",
    "nan_counts_UKB_IMD_Records = {}\n",
    "empty_counts_UKB_IMD_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_IMD_Records in UKB_IMD_Records.columns:\n",
    "    unique_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].nunique()\n",
    "    row_count_UKB_IMD_Records = len(UKB_IMD_Records[column_UKB_IMD_Records])\n",
    "    nan_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [unique_count_UKB_IMD_Records]\n",
    "    row_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [row_count_UKB_IMD_Records]\n",
    "    nan_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [nan_count_UKB_IMD_Records]\n",
    "    empty_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [empty_count_UKB_IMD_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_IMD_Record = []\n",
    "row_counts_UKB_IMD_Record = []\n",
    "nan_counts_UKB_IMD_Record = []\n",
    "empty_counts_UKB_IMD_Record = []\n",
    "\n",
    "unique_counts_UKB_IMD_Record = pd.DataFrame(unique_counts_UKB_IMD_Records, index=['Unique Count'])\n",
    "row_counts_UKB_IMD_Record = pd.DataFrame(row_counts_UKB_IMD_Records, index=['Row Count'])\n",
    "nan_counts_UKB_IMD_Record = pd.DataFrame(nan_counts_UKB_IMD_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_IMD_Record = pd.DataFrame(empty_counts_UKB_IMD_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_IMD_Records = []\n",
    "result_UKB_IMD_Records = pd.concat([unique_counts_UKB_IMD_Record, row_counts_UKB_IMD_Record, nan_counts_UKB_IMD_Record, empty_counts_UKB_IMD_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_IMD_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_IMD_Records)\n",
    "#print()\n",
    "\n",
    "\n",
    "#UKB_IMD_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b. Maximum and minum range of the Index of Multiple Deprivation(IMD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in the \"Index of Multiple Deprivation (England)\" column\n",
    "UKB_IMD_Records = UKB_IMD_Records.dropna(subset=[\"Index of Multiple Deprivation (England)\"])\n",
    "\n",
    "# Reset index\n",
    "UKB_IMD_Records.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_UKB_IMD_Records = {}\n",
    "row_counts_UKB_IMD_Records = {}\n",
    "nan_counts_UKB_IMD_Records = {}\n",
    "empty_counts_UKB_IMD_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_UKB_IMD_Records in UKB_IMD_Records.columns:\n",
    "    unique_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].nunique()\n",
    "    row_count_UKB_IMD_Records = len(UKB_IMD_Records[column_UKB_IMD_Records])\n",
    "    nan_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_UKB_IMD_Records = UKB_IMD_Records[column_UKB_IMD_Records].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [unique_count_UKB_IMD_Records]\n",
    "    row_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [row_count_UKB_IMD_Records]\n",
    "    nan_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [nan_count_UKB_IMD_Records]\n",
    "    empty_counts_UKB_IMD_Records[column_UKB_IMD_Records] = [empty_count_UKB_IMD_Records]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_UKB_IMD_Record = []\n",
    "row_counts_UKB_IMD_Record = []\n",
    "nan_counts_UKB_IMD_Record = []\n",
    "empty_counts_UKB_IMD_Record = []\n",
    "\n",
    "unique_counts_UKB_IMD_Record = pd.DataFrame(unique_counts_UKB_IMD_Records, index=['Unique Count'])\n",
    "row_counts_UKB_IMD_Record = pd.DataFrame(row_counts_UKB_IMD_Records, index=['Row Count'])\n",
    "nan_counts_UKB_IMD_Record = pd.DataFrame(nan_counts_UKB_IMD_Records, index=['NaN Count'])\n",
    "empty_counts_UKB_IMD_Record = pd.DataFrame(empty_counts_UKB_IMD_Records, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_UKB_IMD_Records = []\n",
    "result_UKB_IMD_Records = pd.concat([unique_counts_UKB_IMD_Record, row_counts_UKB_IMD_Record, nan_counts_UKB_IMD_Record, empty_counts_UKB_IMD_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"UKB_IMD_Records:\")\n",
    "#print()\n",
    "#display(result_UKB_IMD_Records)\n",
    "#print()\n",
    "\n",
    "\n",
    "#UKB_IMD_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c. Maximum and minum range of the Index of Multiple Deprivation(IMD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'UKB_IMD_Records' is the DataFrame containing the data\n",
    "maximum_value = UKB_IMD_Records['Index of Multiple Deprivation (England)'].max()\n",
    "minimum_value = UKB_IMD_Records['Index of Multiple Deprivation (England)'].min()\n",
    "\n",
    "#print(\"Maximum value:\", maximum_value)\n",
    "#print(\"Minimum value:\", minimum_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d. Define the 5 Quartiles for IMD**\n",
    "\n",
    "lower the IMD Score, Higher the depriviation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_imd_quintile(imd_value):\n",
    "    if imd_value <= 20:\n",
    "        return 1\n",
    "    elif imd_value <= 40:\n",
    "        return 2\n",
    "    elif imd_value <= 60:\n",
    "        return 3\n",
    "    elif imd_value <= 80:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# Apply the function to create the 'IMD_quintile' column\n",
    "UKB_IMD_Records['IMD_quintile'] = UKB_IMD_Records['Index of Multiple Deprivation (England)'].apply(assign_imd_quintile)\n",
    "\n",
    "# Count occurrences of each quintile\n",
    "quintile_counts = []\n",
    "quintile_counts = UKB_IMD_Records['IMD_quintile'].value_counts().sort_index()\n",
    "\n",
    "# Convert the counts to a DataFrame\n",
    "quintile_counts_df = []\n",
    "quintile_counts_df = pd.DataFrame({'IMD_quintile': quintile_counts.index, 'Count': quintile_counts.values})\n",
    "\n",
    "#display(quintile_counts_df)\n",
    "\n",
    "\n",
    "# Display the DataFrame with IMD quintiles assigned\n",
    "#UKB_IMD_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'IMD Records with IMD_Quintile.csv'\n",
    "\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#UKB_IMD_Records.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5e. Combine IMD Records with ICD10 Codes Structured dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ICD10_codes_and_UKB_IMD_Records = []\n",
    "Combined_ICD10_codes_and_UKB_IMD_Records = pd.merge(All_ICD10_with_Diseases_Dates_Data,UKB_IMD_Records,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_codes_and_UKB_IMD_Records in Combined_ICD10_codes_and_UKB_IMD_Records.columns:\n",
    "    unique_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].nunique()\n",
    "    row_count_Combined_ICD10_codes_and_UKB_IMD_Records = len(Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records])\n",
    "    nan_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [unique_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    row_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [row_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [nan_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [empty_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record =[]\n",
    "\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(row_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_codes_and_UKB_IMD_Records = []\n",
    "result_Combined_ICD10_codes_and_UKB_IMD_Records = pd.concat([unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record, row_counts_Combined_ICD10_codes_and_UKB_IMD_Record, nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record, empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record,prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Combined_ICD10_codes_and_UKB_IMD_Records:\")\n",
    "#display(result_Combined_ICD10_codes_and_UKB_IMD_Records)\n",
    "print()\n",
    "#display(Combined_ICD10_codes_and_UKB_IMD_Records.head(3))\n",
    "print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = Combined_ICD10_codes_and_UKB_IMD_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"IMD_quintile\"\n",
    "unique_IMD_status = []\n",
    "unique_IMD_status = unique_participants['IMD_quintile'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_IMD_status)\n",
    "#print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'IMD_quintile' column\n",
    "IMD_status_counts = []\n",
    "IMD_status_counts = unique_participants['IMD_quintile'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "IMD_status_percentages = []\n",
    "IMD_status_percentages = (IMD_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "IMD_status_counts_with_percentages = []\n",
    "IMD_status_counts_with_percentages = IMD_status_counts.astype(str) + \" (\" + IMD_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'IMD_quintile' column:\")\n",
    "#display(IMD_status_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5d3; padding: 10px;\">\n",
    "    <h2><center>7. Death/ ALive Records according to Age Groups (for UKB)</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = All_ICD10_with_Diseases_Dates_Data.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "# Count occurrences of unique values in the 'Alive / Dead' column\n",
    "total_alive_dead_counts = []\n",
    "total_alive_dead_counts = unique_participants['Alive / Dead'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "total_alive_dead_percentages = []\n",
    "total_alive_dead_percentages = (total_alive_dead_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "total_alive_dead_counts_with_percentages = []\n",
    "total_alive_dead_counts_with_percentages = total_alive_dead_counts.astype(str) + \" (\" + total_alive_dead_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'Alive / Dead' column:\")\n",
    "#display(total_alive_dead_counts_with_percentages)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "# Assume the DataFrame is already loaded as All_ICD10_with_Diseases_Dates_Data\n",
    "\n",
    "# Define the age ranges\n",
    "bins = [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = [\"30 - 40\", \"41 - 50\", \"51 - 60\", \"61 - 70\", \"71 - 80\", \"81 - 90\", \"91 - 100\"]\n",
    "\n",
    "# Create a new column 'Age Range' with the defined bins\n",
    "All_ICD10_with_Diseases_Dates_Data['Age Range'] = pd.cut(All_ICD10_with_Diseases_Dates_Data['Individual Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 440014\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = All_ICD10_with_Diseases_Dates_Data.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "# Separate the counts for 'Alive' and 'Dead' within each age range\n",
    "alive_counts = []\n",
    "dead_counts = []\n",
    "alive_counts = unique_participants[unique_participants['Alive / Dead'] == 'Alive']['Age Range'].value_counts().sort_index()\n",
    "dead_counts = unique_participants[unique_participants['Alive / Dead'] == 'Dead']['Age Range'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentages\n",
    "alive_percentages = []\n",
    "dead_percentages = []\n",
    "alive_percentages = (alive_counts / total_participants) * 100\n",
    "dead_percentages = (dead_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages for 'Alive'\n",
    "alive_counts_with_percentages = alive_counts.astype(str) + \" (\" + alive_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "# Combine counts and percentages for 'Dead'\n",
    "dead_counts_with_percentages = []\n",
    "dead_counts_with_percentages = dead_counts.astype(str) + \" (\" + dead_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "# Display the counts and percentages for 'Alive'\n",
    "#print(\"Counts and percentages for 'Alive' in each age range:\")\n",
    "#display(alive_counts_with_percentages)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "# Display the counts and percentages for 'Dead'\n",
    "#print(\"Counts and percentages for 'Dead' in each age range:\")\n",
    "#display(dead_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Extract PH Patients Data with ICD10 Codes (I27.0, I27.2, I27.9) (for UKB)</center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = []\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract rows where the \"Combined ICD10 Codes\" column has one of the specified values ('I27.0','I27.2','I27.9')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to filter\n",
    "Identify_PH_Patients_using_ICD10_codes = []\n",
    "Identify_PH_Patients_using_ICD10_codes = ['I27.0','I27.2','I27.9']\n",
    "\n",
    "# Extract rows where the \"Item\" column has one of the specified values\n",
    "PH_Patients_using_ICD10_codes_Dataset = []\n",
    "\n",
    "PH_Patients_using_ICD10_codes_Dataset = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Codes'].isin(Identify_PH_Patients_using_ICD10_codes))]# | (Complete_dataset['Drug Name'].notna())]\n",
    "#PH_Patients_using_ICD10_codes_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace 'Combined ICD10 Codes' with the actual column name (PH Group)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = [] \n",
    "column_name = 'Combined ICD10 Codes'\n",
    "\n",
    "icd10_to_ph_group = []\n",
    "icd10_to_ph_group = {'I27.0': 'Primary PH','I27.2': 'Other Secondary PH','I27.9': 'Secondary PH'}\n",
    "# Create the 'PH Group' column based on the mapping\n",
    "PH_Patients_using_ICD10_codes_Dataset['PH Group'] = PH_Patients_using_ICD10_codes_Dataset[column_name].map(icd10_to_ph_group)\n",
    "#PH_Patients_using_ICD10_codes_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create the columns for of PH Patients (I27.0 = Primary PH, I27.2 = Other Secondary PH, I27.9 = Secondary PH) in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "PH_Patients_using_ICD10_codes_Dataset\n",
    "PH_Patients_using_ICD10_codes_Dataset['Primary PH'] = np.nan\n",
    "PH_Patients_using_ICD10_codes_Dataset['Other Secondary PH'] = np.nan\n",
    "PH_Patients_using_ICD10_codes_Dataset['Secondary PH'] = np.nan\n",
    "\n",
    "# Update the new columns based on the values in the 'Combined ICD10 Codes' column\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.0', 'Primary PH'] = 'I27.0'\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.2', 'Other Secondary PH'] = 'I27.2'\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.9', 'Secondary PH'] = 'I27.9'\n",
    "#PH_Patients_using_ICD10_codes_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stored the cleaned and structured records of PH pateints and also display the records summary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the file path where you want to save the CSV file\n",
    "#file_path = []\n",
    "#file_path = 'PH Pateints Data May 2024.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "#PH_Patients_using_ICD10_codes_Dataset.to_csv(file_path, index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "\n",
    "PH_Patients_using_ICD10_codes_Dataset = []\n",
    "PH_Patients_using_ICD10_codes_Dataset = pd.read_csv('PH Pateints Data May 2024.csv')\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "row_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_PH_Patients_using_ICD10_codes_Dataset in PH_Patients_using_ICD10_codes_Dataset.columns:\n",
    "    unique_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].nunique()\n",
    "    row_count_PH_Patients_using_ICD10_codes_Dataset = len(PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset])\n",
    "    nan_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [unique_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    row_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [row_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    nan_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [nan_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    empty_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [empty_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "row_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(unique_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Unique Count'])\n",
    "row_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(row_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Row Count'])\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(nan_counts_PH_Patients_using_ICD10_codes_Dataset, index=['NaN Count'])\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(empty_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_PH_Patients_using_ICD10_codes_Dataset = []\n",
    "result_PH_Patients_using_ICD10_codes_Dataset = pd.concat([unique_counts_PH_Patients_using_ICD10_codes_Datasets, row_counts_PH_Patients_using_ICD10_codes_Datasets, nan_counts_PH_Patients_using_ICD10_codes_Datasets, empty_counts_PH_Patients_using_ICD10_codes_Datasets])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"PH Patients using ICD10 codes & Drugs Names:\")\n",
    "#print()\n",
    "#display(result_PH_Patients_using_ICD10_codes_Dataset)\n",
    "#print()\n",
    "#print()\n",
    "#PH_Patients_using_ICD10_codes_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Sex and Ethnicity Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PH_Patients_Records = []\n",
    "PH_Patients_Records = pd.read_csv('PH Pateints Data May 2024.csv')\n",
    "\n",
    "# Drop the 'Individual Age' and 'Age Range' columns\n",
    "PH_Patients_Records.drop(columns=['Individual Age', 'Age Range'], inplace=True)\n",
    "\n",
    "# Convert 'Combined ICD10 Diagnosis Date' to datetime\n",
    "PH_Patients_Records['Combined ICD10 Diagnosis Date'] = pd.to_datetime(PH_Patients_Records['Combined ICD10 Diagnosis Date'], errors='coerce')\n",
    "\n",
    "# Compute the new 'Individual Age' based on the 'Year of Birth' and 'Combined ICD10 Diagnosis Date'\n",
    "PH_Patients_Records['PH Individual Age'] = PH_Patients_Records.apply(\n",
    "    lambda row: row['Combined ICD10 Diagnosis Date'].year - int(row['Year of Birth']) if pd.notnull(row['Combined ICD10 Diagnosis Date']) else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Function to calculate age range\n",
    "def calculate_age_range(age):\n",
    "    if pd.isnull(age):\n",
    "        return \"\"\n",
    "    if age < 20:\n",
    "        return \"0 - 19\"\n",
    "    elif age < 30:\n",
    "        return \"20 - 29\"\n",
    "    elif age < 40:\n",
    "        return \"30 - 40\"\n",
    "    elif age < 50:\n",
    "        return \"41 - 50\"\n",
    "    elif age < 60:\n",
    "        return \"51 - 60\"\n",
    "    elif age < 70:\n",
    "        return \"61 - 70\"\n",
    "    elif age < 80:\n",
    "        return \"71 - 80\"\n",
    "    elif age < 90:\n",
    "        return \"81 - 90\"\n",
    "    else:\n",
    "        return \"91+\"\n",
    "\n",
    "# Calculate the 'PH Age Range' based on the new 'PH Individual Age'\n",
    "PH_Patients_Records['PH Age Range'] = PH_Patients_Records['PH Individual Age'].apply(calculate_age_range)\n",
    "\n",
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of each sex\n",
    "sex_counts = []\n",
    "sex_counts = unique_participants['Sex'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "sex_percentages = []\n",
    "sex_percentages = (sex_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "sex_counts_with_percentages = []\n",
    "sex_counts_with_percentages = sex_counts.astype(str) + \" (\" + sex_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of missing (NaN) values in each column\n",
    "missing_counts = []\n",
    "missing_counts = unique_participants.isna().sum()\n",
    "\n",
    "# Get unique items in the \"Ethnicity\" column\n",
    "unique_ethnicities = []\n",
    "unique_ethnicities = unique_participants['Ethnicity'].unique()\n",
    "\n",
    "# Count the number of each unique ethnicity\n",
    "ethnicity_counts = []\n",
    "ethnicity_counts = unique_participants['Ethnicity'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "ethnicity_percentages = []\n",
    "ethnicity_percentages = (ethnicity_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "ethnicity_counts_with_percentages = []\n",
    "ethnicity_counts_with_percentages = ethnicity_counts.astype(str) + \" (\" + ethnicity_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "# Print the counts\n",
    "#print(\"Sex Counts with Percentages:\")\n",
    "#print(sex_counts_with_percentages)\n",
    "#print()\n",
    "#print(\"\\nEthnicity Counts with Percentages:\")\n",
    "#print(ethnicity_counts_with_percentages)\n",
    "#print()\n",
    "#print(\"\\nMissing Values Counts:\")\n",
    "#print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Age Groups Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 - 40 : 8 (0.29 %)\n",
      "41 - 50 : 60 (2.20 %)\n",
      "51 - 60 : 274 (10.05 %)\n",
      "61 - 70 : 835 (30.62 %)\n",
      "71 - 80 : 1401 (51.38 %)\n",
      "81 - 90 : 149 (5.46 %)\n"
     ]
    }
   ],
   "source": [
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "# Count occurrences of each age range\n",
    "age_range_counts = []\n",
    "age_range_counts = unique_participants['PH Age Range'].value_counts().sort_index()\n",
    "\n",
    "# Calculate percentages\n",
    "age_range_percentages = (age_range_counts / total_participants) * 100\n",
    "\n",
    "# Display the results\n",
    "for age_range, count in age_range_counts.items():\n",
    "    percentage = age_range_percentages[age_range]\n",
    "    print(f\"{age_range} : {count} ({percentage:.2f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>IMD Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMD_Records_with_IMD_Quintile = []\n",
    "IMD_Records_with_IMD_Quintile = pd.read_csv('IMD Records with IMD_Quintile.csv')\n",
    "#IMD_Records_with_IMD_Quintile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine The \"IMD Records with Quintiles\" with \"PH patients Records dataset\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = []\n",
    "PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = pd.merge(PH_Patients_Records,IMD_Records_with_IMD_Quintile,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = {}\n",
    "row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = {}\n",
    "nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = {}\n",
    "empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = {}\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile in PH_Patients_Records_and_IMD_Records_with_IMD_Quintile.columns:\n",
    "    unique_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile].nunique()\n",
    "    row_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = len(PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile])\n",
    "    nan_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile].isna().sum()  # Count NaN values\n",
    "    empty_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile] = [unique_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile]\n",
    "    row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile] = [row_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile]\n",
    "    nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile] = [nan_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile]\n",
    "    empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile] = [empty_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile]\n",
    "    prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile[column_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile] = [prefer_not_to_say_count_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = []\n",
    "row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = []\n",
    "nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = []\n",
    "empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = []\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles =[]\n",
    "\n",
    "unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = pd.DataFrame(unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile, index=['Unique Count'])\n",
    "row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = pd.DataFrame(row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile, index=['Row Count'])\n",
    "nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = pd.DataFrame(nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile, index=['NaN Count'])\n",
    "empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = pd.DataFrame(empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles = pd.DataFrame(prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = []\n",
    "result_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile = pd.concat([unique_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles, row_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles, nan_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles, empty_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles,prefer_not_to_say_counts_PH_Patients_Records_and_IMD_Records_with_IMD_Quintiles])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"PH_Patients_Records_and_IMD_Records_with_IMD_Quintile:\")\n",
    "#display(result_PH_Patients_Records_and_IMD_Records_with_IMD_Quintile)\n",
    "print()\n",
    "#print()\n",
    "#display(PH_Patients_Records_and_IMD_Records_with_IMD_Quintile.head(3))\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records_and_IMD_Records_with_IMD_Quintile.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"IMD_quintile\"\n",
    "unique_IMD_status = []\n",
    "unique_IMD_status = unique_participants['IMD_quintile'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_IMD_status)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'IMD_quintile' column\n",
    "IMD_status_counts = []\n",
    "IMD_status_counts = unique_participants['IMD_quintile'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "IMD_status_percentages = []\n",
    "IMD_status_percentages = (IMD_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "IMD_status_counts_with_percentages = []\n",
    "IMD_status_counts_with_percentages = IMD_status_counts.astype(str) + \" (\" + IMD_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'IMD_quintile' column:\")\n",
    "#display(IMD_status_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Smoking Status Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smoking_Status_Records = []\n",
    "Smoking_Status_Records = pd.read_csv('Smoking Status Record Dataset.csv')\n",
    "#Smoking_Status_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine \"Smoking Status Records\" with PH Patients records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_Patients_Records_and_Smoking_Status_Records = []\n",
    "PH_Patients_Records_and_Smoking_Status_Records = pd.merge(PH_Patients_Records,Smoking_Status_Records,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_PH_Patients_Records_and_Smoking_Status_Records = {}\n",
    "row_counts_PH_Patients_Records_and_Smoking_Status_Records = {}\n",
    "nan_counts_PH_Patients_Records_and_Smoking_Status_Records = {}\n",
    "empty_counts_PH_Patients_Records_and_Smoking_Status_Records = {}\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_Smoking_Status_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_PH_Patients_Records_and_Smoking_Status_Records in PH_Patients_Records_and_Smoking_Status_Records.columns:\n",
    "    unique_count_PH_Patients_Records_and_Smoking_Status_Records = PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records].nunique()\n",
    "    row_count_PH_Patients_Records_and_Smoking_Status_Records = len(PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records])\n",
    "    nan_count_PH_Patients_Records_and_Smoking_Status_Records = PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_PH_Patients_Records_and_Smoking_Status_Records = PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_PH_Patients_Records_and_Smoking_Status_Records = PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records] = [unique_count_PH_Patients_Records_and_Smoking_Status_Records]\n",
    "    row_counts_PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records] = [row_count_PH_Patients_Records_and_Smoking_Status_Records]\n",
    "    nan_counts_PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records] = [nan_count_PH_Patients_Records_and_Smoking_Status_Records]\n",
    "    empty_counts_PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records] = [empty_count_PH_Patients_Records_and_Smoking_Status_Records]\n",
    "    prefer_not_to_say_counts_PH_Patients_Records_and_Smoking_Status_Records[column_PH_Patients_Records_and_Smoking_Status_Records] = [prefer_not_to_say_count_PH_Patients_Records_and_Smoking_Status_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_PH_Patients_Records_and_Smoking_Status_Record = []\n",
    "row_counts_PH_Patients_Records_and_Smoking_Status_Record = []\n",
    "nan_counts_PH_Patients_Records_and_Smoking_Status_Record = []\n",
    "empty_counts_PH_Patients_Records_and_Smoking_Status_Record = []\n",
    "PH_Patients_Records_and_Smoking_Status_Record =[]\n",
    "\n",
    "unique_counts_PH_Patients_Records_and_Smoking_Status_Record = pd.DataFrame(unique_counts_PH_Patients_Records_and_Smoking_Status_Records, index=['Unique Count'])\n",
    "row_counts_PH_Patients_Records_and_Smoking_Status_Record = pd.DataFrame(row_counts_PH_Patients_Records_and_Smoking_Status_Records, index=['Row Count'])\n",
    "nan_counts_PH_Patients_Records_and_Smoking_Status_Record = pd.DataFrame(nan_counts_PH_Patients_Records_and_Smoking_Status_Records, index=['NaN Count'])\n",
    "empty_counts_PH_Patients_Records_and_Smoking_Status_Record = pd.DataFrame(empty_counts_PH_Patients_Records_and_Smoking_Status_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_Smoking_Status_Record = pd.DataFrame(prefer_not_to_say_counts_PH_Patients_Records_and_Smoking_Status_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_PH_Patients_Records_and_Smoking_Status_Records = []\n",
    "result_PH_Patients_Records_and_Smoking_Status_Records = pd.concat([unique_counts_PH_Patients_Records_and_Smoking_Status_Record, row_counts_PH_Patients_Records_and_Smoking_Status_Record, nan_counts_PH_Patients_Records_and_Smoking_Status_Record, empty_counts_PH_Patients_Records_and_Smoking_Status_Record,prefer_not_to_say_counts_PH_Patients_Records_and_Smoking_Status_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"PH_Patients_Records_and_Smoking_Status_Records:\")\n",
    "#display(result_PH_Patients_Records_and_Smoking_Status_Records)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records_and_Smoking_Status_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"Smoking Status\"\n",
    "unique_smoking_status = []\n",
    "unique_smoking_status = unique_participants['Smoking Status'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_smoking_status)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'Smoking Status' column\n",
    "smoking_status_counts = []\n",
    "smoking_status_counts = unique_participants['Smoking Status'].value_counts()\n",
    "\n",
    "# Calculate the percentage for each ethnicity\n",
    "smoking_status_percentages = []\n",
    "smoking_status_percentages = (smoking_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "smoking_status_counts_with_percentages = []\n",
    "smoking_status_counts_with_percentages = smoking_status_counts.astype(str) + \" (\" + smoking_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'Smoking Status' column:\")\n",
    "#display(smoking_status_counts_with_percentages)\n",
    "#print()\n",
    "\n",
    "\n",
    "#PH_Patients_Records_and_Smoking_Status_Records.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Body Mass Index (BMI) Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_Range_Record_Dataset = []\n",
    "BMI_Range_Record_Dataset = pd.read_csv('BMI Range Record Dataset.csv')\n",
    "#BMI_Range_Record_Dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_Patients_Records_and_BMI_Range_Record_Dataset = []\n",
    "PH_Patients_Records_and_BMI_Range_Record_Dataset = pd.merge(PH_Patients_Records,BMI_Range_Record_Dataset,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset = {}\n",
    "row_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset = {}\n",
    "nan_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset = {}\n",
    "empty_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset = {}\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_PH_Patients_Records_and_BMI_Range_Record_Dataset in PH_Patients_Records_and_BMI_Range_Record_Dataset.columns:\n",
    "    unique_count_PH_Patients_Records_and_BMI_Range_Record_Dataset = PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset].nunique()\n",
    "    row_count_PH_Patients_Records_and_BMI_Range_Record_Dataset = len(PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset])\n",
    "    nan_count_PH_Patients_Records_and_BMI_Range_Record_Dataset = PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_PH_Patients_Records_and_BMI_Range_Record_Dataset = PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_PH_Patients_Records_and_BMI_Range_Record_Dataset = PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset] = [unique_count_PH_Patients_Records_and_BMI_Range_Record_Dataset]\n",
    "    row_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset] = [row_count_PH_Patients_Records_and_BMI_Range_Record_Dataset]\n",
    "    nan_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset] = [nan_count_PH_Patients_Records_and_BMI_Range_Record_Dataset]\n",
    "    empty_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset] = [empty_count_PH_Patients_Records_and_BMI_Range_Record_Dataset]\n",
    "    prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset[column_PH_Patients_Records_and_BMI_Range_Record_Dataset] = [prefer_not_to_say_count_PH_Patients_Records_and_BMI_Range_Record_Dataset]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = []\n",
    "row_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = []\n",
    "nan_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = []\n",
    "empty_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = []\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets =[]\n",
    "\n",
    "unique_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = pd.DataFrame(unique_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset, index=['Unique Count'])\n",
    "row_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = pd.DataFrame(row_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset, index=['Row Count'])\n",
    "nan_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = pd.DataFrame(nan_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset, index=['NaN Count'])\n",
    "empty_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = pd.DataFrame(empty_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets = pd.DataFrame(prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Dataset, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_PH_Patients_Records_and_BMI_Range_Record_Dataset = []\n",
    "result_PH_Patients_Records_and_BMI_Range_Record_Dataset = pd.concat([unique_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets, row_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets, nan_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets, empty_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets,prefer_not_to_say_counts_PH_Patients_Records_and_BMI_Range_Record_Datasets])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"PH_Patients_Records_and_BMI_Range_Record_Dataset:\")\n",
    "#display(result_PH_Patients_Records_and_BMI_Range_Record_Dataset)\n",
    "print()\n",
    "#print()\n",
    "#display(PH_Patients_Records_and_BMI_Range_Record_Dataset.head(3))\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records_and_BMI_Range_Record_Dataset.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "# Display unique items under the column \"BMI Range\"\n",
    "unique_BMI_status = []\n",
    "unique_BMI_status = unique_participants['BMI Range'].unique()\n",
    "# Print the unique items\n",
    "#display(unique_BMI_status)\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "# Count occurrences of unique values in the 'BMI Range' column\n",
    "BMI_status_counts = []\n",
    "BMI_status_counts = unique_participants['BMI Range'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "BMI_status_percentages = []\n",
    "BMI_status_percentages = (BMI_status_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "BMI_status_counts_with_percentages = []\n",
    "BMI_status_counts_with_percentages = BMI_status_counts.astype(str) + \" (\" + BMI_status_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "#print(\"Counts of unique values in the 'BMI Range' column:\")\n",
    "#display(BMI_status_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Age Specific Mortality Counts for PH Patients </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PH_Patients_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "# Count occurrences of unique values in the 'Alive / Dead' column\n",
    "total_alive_dead_counts = []\n",
    "total_alive_dead_counts = unique_participants['Alive / Dead'].value_counts()\n",
    "\n",
    "# Calculate the percentage \n",
    "total_alive_dead_percentages = []\n",
    "total_alive_dead_percentages = (total_alive_dead_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "total_alive_dead_counts_with_percentages = []\n",
    "total_alive_dead_counts_with_percentages = total_alive_dead_counts.astype(str) + \" (\" + total_alive_dead_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "\n",
    "\n",
    "# Display the counts\n",
    "print(\"Counts of unique values in the 'Alive / Dead' column:\")\n",
    "display(total_alive_dead_counts_with_percentages)\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Assume the DataFrame is already loaded as All_ICD10_with_Diseases_Dates_Data\n",
    "\n",
    "# Define the age ranges\n",
    "bins = [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = [\"30 - 40\", \"41 - 50\", \"51 - 60\", \"61 - 70\", \"71 - 80\", \"81 - 90\", \"91 - 100\"]\n",
    "\n",
    "# Create a new column 'Age Range' with the defined bins\n",
    "PH_Patients_Records['PH Age Range'] = pd.cut(PH_Patients_Records['PH Individual Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Total number of participants\n",
    "total_participants = []\n",
    "total_participants = 2727\n",
    "\n",
    "\n",
    "# Drop duplicate Participant ID rows to get unique participants\n",
    "unique_participants = []\n",
    "unique_participants = PH_Patients_Records.drop_duplicates(subset=['Participant ID'])\n",
    "\n",
    "# Separate the counts for 'Alive' and 'Dead' within each age range\n",
    "alive_counts = []\n",
    "dead_counts = []\n",
    "alive_counts = unique_participants[unique_participants['Alive / Dead'] == 'Alive']['PH Age Range'].value_counts().sort_index()\n",
    "dead_counts = unique_participants[unique_participants['Alive / Dead'] == 'Dead']['PH Age Range'].value_counts().sort_index()\n",
    "\n",
    "# Calculate the percentages\n",
    "alive_percentages = []\n",
    "dead_percentages = []\n",
    "alive_percentages = (alive_counts / total_participants) * 100\n",
    "dead_percentages = (dead_counts / total_participants) * 100\n",
    "\n",
    "# Combine counts and percentages for 'Alive'\n",
    "alive_counts_with_percentages = alive_counts.astype(str) + \" (\" + alive_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "# Combine counts and percentages for 'Dead'\n",
    "dead_counts_with_percentages = []\n",
    "dead_counts_with_percentages = dead_counts.astype(str) + \" (\" + dead_percentages.round(2).astype(str) + \"%)\"\n",
    "\n",
    "# Display the counts and percentages for 'Alive'\n",
    "#print(\"Counts and percentages for 'Alive' in each age range:\")\n",
    "#display(alive_counts_with_percentages)\n",
    "#print()\n",
    "#print()\n",
    "\n",
    "# Display the counts and percentages for 'Dead'\n",
    "#print(\"Counts and percentages for 'Dead' in each age range:\")\n",
    "#display(dead_counts_with_percentages)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d3f5f5; padding: 10px;\">\n",
    "    <h2><center>Matched Control Cohort </center></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "#All_ICD10_with_Diseases_Dates_Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smoking = []\n",
    "Smoking = pd.read_csv('Smoking Status Record Dataset.csv')\n",
    "#Smoking.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI = []\n",
    "BMI = pd.read_csv('BMI Range Record Dataset.csv')\n",
    "#BMI.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMD = []\n",
    "IMD = pd.read_csv('IMD Records with IMD_Quintile.csv')\n",
    "#IMD.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_ICD10_codes_and_UKB_IMD_Records = []\n",
    "Combined_ICD10_codes_and_UKB_IMD_Records = pd.merge(All_ICD10_with_Diseases_Dates_Data,Smoking,   on=\"Participant ID\", how=\"left\")\n",
    "Combined_ICD10_codes_and_UKB_IMD_Records = pd.merge(Combined_ICD10_codes_and_UKB_IMD_Records,BMI,   on=\"Participant ID\", how=\"left\")\n",
    "Combined_ICD10_codes_and_UKB_IMD_Records = pd.merge(Combined_ICD10_codes_and_UKB_IMD_Records,IMD,   on=\"Participant ID\", how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_codes_and_UKB_IMD_Records in Combined_ICD10_codes_and_UKB_IMD_Records.columns:\n",
    "    unique_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].nunique()\n",
    "    row_count_Combined_ICD10_codes_and_UKB_IMD_Records = len(Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records])\n",
    "    nan_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].eq('').sum()  # Count empty string values\n",
    "    prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_IMD_Records = Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records].eq('Prefer not to answer').sum()  \n",
    "\n",
    "    unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [unique_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    row_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [row_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [nan_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [empty_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "    prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records[column_Combined_ICD10_codes_and_UKB_IMD_Records] = [prefer_not_to_say_count_Combined_ICD10_codes_and_UKB_IMD_Records]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record = []\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record =[]\n",
    "\n",
    "unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(unique_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(row_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(nan_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(empty_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Empty Count'])\n",
    "prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record = pd.DataFrame(prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Records, index=['Prefer not to answer'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_codes_and_UKB_IMD_Records = []\n",
    "result_Combined_ICD10_codes_and_UKB_IMD_Records = pd.concat([unique_counts_Combined_ICD10_codes_and_UKB_IMD_Record, row_counts_Combined_ICD10_codes_and_UKB_IMD_Record, nan_counts_Combined_ICD10_codes_and_UKB_IMD_Record, empty_counts_Combined_ICD10_codes_and_UKB_IMD_Record,prefer_not_to_say_counts_Combined_ICD10_codes_and_UKB_IMD_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Combined_ICD10_codes_and_UKB_IMD_Records:\")\n",
    "#display(result_Combined_ICD10_codes_and_UKB_IMD_Records)\n",
    "#print()\n",
    "#print()\n",
    "#display(Combined_ICD10_codes_and_UKB_IMD_Records.head(3))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'Dataset for Matched Control Cohort.csv'\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#Combined_ICD10_codes_and_UKB_IMD_Records.to_csv(file_path, index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
