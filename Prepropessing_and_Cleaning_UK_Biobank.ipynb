{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uaag_RmJQcVj"
   },
   "source": [
    "### <center> 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-YjD6P6QcVr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.reset_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.reset_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXbhEuf9QcVu"
   },
   "source": [
    "### <center> 2. Importing Dataset (Contains Death Date Records of the UK Bio Bank Participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SV3kiJkQcVx",
    "outputId": "12118e50-88eb-4685-b5e9-095fed609f40"
   },
   "outputs": [],
   "source": [
    "Death_Date_Record = []\n",
    "Death_Date_Record = pd.read_csv('Death Date Record.csv')\n",
    "Death_Date_Record = Death_Date_Record.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Date_Record = {}\n",
    "row_counts_Death_Date_Record = {}\n",
    "nan_counts_Death_Date_Record = {}\n",
    "empty_counts_Death_Date_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Date_Record in Death_Date_Record.columns:\n",
    "    unique_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].nunique()\n",
    "    row_count_Death_Date_Record = len(Death_Date_Record[column_Death_Date_Record])\n",
    "    nan_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Date_Record = Death_Date_Record[column_Death_Date_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Date_Record[column_Death_Date_Record] = [unique_count_Death_Date_Record]\n",
    "    row_counts_Death_Date_Record[column_Death_Date_Record] = [row_count_Death_Date_Record]\n",
    "    nan_counts_Death_Date_Record[column_Death_Date_Record] = [nan_count_Death_Date_Record]\n",
    "    empty_counts_Death_Date_Record[column_Death_Date_Record] = [empty_count_Death_Date_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Date = []\n",
    "row_counts_Death_Date = []\n",
    "nan_counts_Death_Date = []\n",
    "empty_counts_Death_Date = []\n",
    "\n",
    "unique_counts_Death_Date = pd.DataFrame(unique_counts_Death_Date_Record, index=['Unique Count'])\n",
    "row_counts_Death_Date = pd.DataFrame(row_counts_Death_Date_Record, index=['Row Count'])\n",
    "nan_counts_Death_Date = pd.DataFrame(nan_counts_Death_Date_Record, index=['NaN Count'])\n",
    "empty_counts_Death_Date = pd.DataFrame(empty_counts_Death_Date_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Date_Record = []\n",
    "result_Death_Date_Record = pd.concat([unique_counts_Death_Date, row_counts_Death_Date, nan_counts_Death_Date, empty_counts_Death_Date])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Death Date Record:\")\n",
    "print()\n",
    "display(result_Death_Date_Record)\n",
    "\n",
    "\n",
    "\n",
    "Death_Date_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT2OVAjYQcV1"
   },
   "source": [
    "### <center> 3. Importing Dataset (Contains Death Cause Records of the UK Bio Bank Participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZFYAXIIQcV4",
    "outputId": "9a6b50f4-80b8-4b0a-d40c-f29855dfebbe"
   },
   "outputs": [],
   "source": [
    "Death_Cause_Record = []\n",
    "Death_Cause_Record = pd.read_csv('Death Cause Record.csv')\n",
    "Death_Cause_Record = Death_Cause_Record.drop_duplicates()\n",
    "Death_Cause_Record = Death_Cause_Record.rename(columns={'ICD10 codes':'Death Cause Disease ICD10 Codes','ICD10 Diseases':'Death Cause Diseases'})\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Cause_Record = {}\n",
    "row_counts_Death_Cause_Record = {}\n",
    "nan_counts_Death_Cause_Record = {}\n",
    "empty_counts_Death_Cause_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Cause_Record in Death_Cause_Record.columns:\n",
    "    unique_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].nunique()\n",
    "    row_count_Death_Cause_Record = len(Death_Cause_Record[column_Death_Cause_Record])\n",
    "    nan_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Cause_Record = Death_Cause_Record[column_Death_Cause_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Cause_Record[column_Death_Cause_Record] = [unique_count_Death_Cause_Record]\n",
    "    row_counts_Death_Cause_Record[column_Death_Cause_Record] = [row_count_Death_Cause_Record]\n",
    "    nan_counts_Death_Cause_Record[column_Death_Cause_Record] = [nan_count_Death_Cause_Record]\n",
    "    empty_counts_Death_Cause_Record[column_Death_Cause_Record] = [empty_count_Death_Cause_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Cause = []\n",
    "row_counts_Death_Cause = []\n",
    "nan_counts_Death_Cause = []\n",
    "empty_counts_Death_Cause = []\n",
    "\n",
    "unique_counts_Death_Cause = pd.DataFrame(unique_counts_Death_Cause_Record, index=['Unique Count'])\n",
    "row_counts_Death_Cause = pd.DataFrame(row_counts_Death_Cause_Record, index=['Row Count'])\n",
    "nan_counts_Death_Cause = pd.DataFrame(nan_counts_Death_Cause_Record, index=['NaN Count'])\n",
    "empty_counts_Death_Cause = pd.DataFrame(empty_counts_Death_Cause_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Cause_Record = []\n",
    "result_Death_Cause_Record = pd.concat([unique_counts_Death_Cause, row_counts_Death_Cause, nan_counts_Death_Cause, empty_counts_Death_Cause])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Death Cause Record:\")\n",
    "print()\n",
    "display(result_Death_Cause_Record)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Death_Cause_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqjOulWKQcV6"
   },
   "source": [
    "### <center> 4. Importing Dataset (Contains GP Prescription (Drugs Name and Quantity) of the UK Bio Bank Participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_Prescription_Record = []\n",
    "GP_Prescription_Record = pd.read_csv('GP Prescription Record.csv')\n",
    "unique_count = GP_Prescription_Record['Participant ID'].nunique()\n",
    "\n",
    "print(\"Unique item count in 'Participant ID' column:\", unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s__FbAoxQcV7",
    "outputId": "7f827658-2010-4bf9-ad61-ca6304af5305"
   },
   "outputs": [],
   "source": [
    "GP_Prescription_Record = []\n",
    "GP_Prescription_Record = pd.read_csv('GP Prescription Record.csv')\n",
    "GP_Prescription_Record = GP_Prescription_Record.drop_duplicates()\n",
    "GP_Prescription_Record = GP_Prescription_Record.rename(columns={'Issue Date':'Drug Issue Date','Quantity':'Drug Quantity'})\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_GP_Prescription_Record = {}\n",
    "row_counts_GP_Prescription_Record = {}\n",
    "nan_counts_GP_Prescription_Record = {}\n",
    "empty_counts_GP_Prescription_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_GP_Prescription_Record in GP_Prescription_Record.columns:\n",
    "    unique_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].nunique()\n",
    "    row_count_GP_Prescription_Record = len(GP_Prescription_Record[column_GP_Prescription_Record])\n",
    "    nan_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_GP_Prescription_Record = GP_Prescription_Record[column_GP_Prescription_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [unique_count_GP_Prescription_Record]\n",
    "    row_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [row_count_GP_Prescription_Record]\n",
    "    nan_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [nan_count_GP_Prescription_Record]\n",
    "    empty_counts_GP_Prescription_Record[column_GP_Prescription_Record] = [empty_count_GP_Prescription_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_GP_Prescription = []\n",
    "row_counts_GP_Prescription = []\n",
    "nan_counts_GP_Prescription = []\n",
    "empty_counts_GP_Prescription_Record = []\n",
    "\n",
    "unique_counts_GP_Prescription = pd.DataFrame(unique_counts_GP_Prescription_Record, index=['Unique Count'])\n",
    "row_counts_GP_Prescription = pd.DataFrame(row_counts_GP_Prescription_Record, index=['Row Count'])\n",
    "nan_counts_GP_Prescription = pd.DataFrame(nan_counts_GP_Prescription_Record, index=['NaN Count'])\n",
    "empty_counts_GP_Prescription = pd.DataFrame(empty_counts_GP_Prescription_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_GP_Prescription_Record = []\n",
    "result_GP_Prescription_Record = pd.concat([unique_counts_GP_Prescription, row_counts_GP_Prescription, nan_counts_GP_Prescription, empty_counts_GP_Prescription])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"GP Prescription Dataset Record:\")\n",
    "print()\n",
    "display(result_GP_Prescription_Record)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GP_Prescription_Record.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmnPOvoMQcV_"
   },
   "source": [
    "### <center> 5. Steps for creating Full Complete Structured Dataset\n",
    "This dataset contains the variables such as \"Participant IDs\", \"Year of Birth\",\"Month of Birth\", \"Ethnicity\", \"Sex\"  and having both ICD10 and Main ICD10 Codes (with Diagnosis Dates), Diseases Names, Birth and Death Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ofpETVcQcWB",
    "outputId": "50e991cb-96a1-40d7-ca96-8bf3e8b79fe2"
   },
   "outputs": [],
   "source": [
    "### Importing the dataset\n",
    "d = []\n",
    "d = pd.read_csv('dataset.csv')\n",
    "ICD10_dataset = d\n",
    "## initialize df equal to dataset\n",
    "ICD10_dataset = ICD10_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_ICD10_dataset = {}\n",
    "row_counts_ICD10_dataset = {}\n",
    "nan_counts_ICD10_dataset = {}\n",
    "empty_counts_ICD10_dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_ICD10_dataset in ICD10_dataset.columns:\n",
    "    unique_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].nunique()\n",
    "    row_count_ICD10_dataset = len(ICD10_dataset[column_ICD10_dataset])\n",
    "    nan_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_ICD10_dataset = ICD10_dataset[column_ICD10_dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_ICD10_dataset[column_ICD10_dataset] = [unique_count_ICD10_dataset]\n",
    "    row_counts_ICD10_dataset[column_ICD10_dataset] = [row_count_ICD10_dataset]\n",
    "    nan_counts_ICD10_dataset[column_ICD10_dataset] = [nan_count_ICD10_dataset]\n",
    "    empty_counts_ICD10_dataset[column_ICD10_dataset] = [empty_count_ICD10_dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_ICD10_databank = []\n",
    "row_counts_ICD10_databank = []\n",
    "nan_counts_ICD10_databank = []\n",
    "empty_counts_ICD10_databank = []\n",
    "\n",
    "unique_counts_ICD10_databank = pd.DataFrame(unique_counts_ICD10_dataset, index=['Unique Count'])\n",
    "row_counts_ICD10_databank = pd.DataFrame(row_counts_ICD10_dataset, index=['Row Count'])\n",
    "nan_counts_ICD10_databank = pd.DataFrame(nan_counts_ICD10_dataset, index=['NaN Count'])\n",
    "empty_counts_ICD10_databank = pd.DataFrame(empty_counts_ICD10_dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_ICD10_dataset = []\n",
    "result_ICD10_dataset = pd.concat([unique_counts_ICD10_databank, row_counts_ICD10_databank, nan_counts_ICD10_databank, empty_counts_ICD10_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"ICD10-base Clinal Dataset:\")\n",
    "print()\n",
    "display(result_ICD10_dataset)\n",
    "\n",
    "\n",
    "\n",
    "ICD10_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2NGW9FYQcWC"
   },
   "source": [
    "#### <center> 5 (a). Extract the Variables related to \"ICD10 Codes\" and it`s related diseases names and Diagnosis Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KhtwvjiQcWE",
    "outputId": "17ac9002-7379-4481-c72a-44b3da016d32"
   },
   "outputs": [],
   "source": [
    "### initialize df1 as ICD10 codes dataset\n",
    "ICD10_Codes_Dates_Table = []\n",
    "ICD10_Codes_Dates_Table = ICD10_dataset.iloc[:, :249]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_ICD10_Codes_Dates_Table in ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_ICD10_Codes_Dates_Table = len(ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table])\n",
    "    nan_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [unique_count_ICD10_Codes_Dates_Table]\n",
    "    row_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [row_count_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [nan_count_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_ICD10_Codes_Dates_Table[column_ICD10_Codes_Dates_Table] = [empty_count_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_ICD10_Codes_Dates_Table = []\n",
    "result_ICD10_Codes_Dates_Table = pd.concat([unique_counts_ICD10_Codes_Dates_Table_databank, row_counts_ICD10_Codes_Dates_Table_databank, nan_counts_ICD10_Codes_Dates_Table_databank, empty_counts_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_ICD10_Codes_Dates_Table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ICD10_Codes_Dates_Table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c1tGoFHQcWG"
   },
   "source": [
    "#### <center> 5(b). Structure the \"ICD10 Codes\" with related Diseases and Diagnosis Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPar74LtQcWH",
    "outputId": "86905cd0-7943-45bc-e277-958aa7cdf50b"
   },
   "outputs": [],
   "source": [
    "#### Create a new DataFrame to store the modified data\n",
    "Storing_ICD10_Codes_Dates_Table = []\n",
    "\n",
    "#### Iterate through each row\n",
    "for idx, row in ICD10_Codes_Dates_Table.iterrows():\n",
    "    participant_id = row['Participant ID']\n",
    "    year_of_birth = row['Year of Birth']\n",
    "    month_of_birth = row['Month of Birth']\n",
    "    ethnicity = row['Ethnicity']\n",
    "    sex = row['Sex']\n",
    "    icd_names = row['ICD10 - Diagnosis'].split('|')\n",
    "    dates = [row['ICD10 date-0'], row['ICD10 date-1'], row['ICD10 date-2'],\n",
    "             row['ICD10 date-3'], row['ICD10 date-4'], row['ICD10 date-5'],\n",
    "             row['ICD10 date-6'], row['ICD10 date-7'], row['ICD10 date-8'],\n",
    "             row['ICD10 date-9'], row['ICD10 date-10'], row['ICD10 date-11'],\n",
    "             row['ICD10 date-12'], row['ICD10 date-13'], row['ICD10 date-14'],\n",
    "             row['ICD10 date-15'], row['ICD10 date-16'], row['ICD10 date-17'],\n",
    "             row['ICD10 date-18'], row['ICD10 date-19'], row['ICD10 date-20'],\n",
    "             row['ICD10 date-21'], row['ICD10 date-22'], row['ICD10 date-23'],\n",
    "             row['ICD10 date-24'], row['ICD10 date-25'], row['ICD10 date-26'],\n",
    "             row['ICD10 date-27'], row['ICD10 date-28'], row['ICD10 date-29'],\n",
    "             row['ICD10 date-30'], row['ICD10 date-31'], row['ICD10 date-32'],\n",
    "             row['ICD10 date-33'], row['ICD10 date-34'], row['ICD10 date-35'],\n",
    "             row['ICD10 date-36'], row['ICD10 date-37'], row['ICD10 date-38'],\n",
    "             row['ICD10 date-39'], row['ICD10 date-40'], row['ICD10 date-41'],\n",
    "             row['ICD10 date-42'], row['ICD10 date-43'], row['ICD10 date-44'],\n",
    "             row['ICD10 date-45'], row['ICD10 date-46'], row['ICD10 date-47'],\n",
    "             row['ICD10 date-48'], row['ICD10 date-49'], row['ICD10 date-50'],\n",
    "             row['ICD10 date-51'], row['ICD10 date-52'], row['ICD10 date-53'],\n",
    "             row['ICD10 date-54'], row['ICD10 date-55'], row['ICD10 date-56'],\n",
    "             row['ICD10 date-57'], row['ICD10 date-58'], row['ICD10 date-59'],\n",
    "             row['ICD10 date-60'], row['ICD10 date-61'], row['ICD10 date-62'],\n",
    "             row['ICD10 date-63'], row['ICD10 date-64'], row['ICD10 date-65'],\n",
    "             row['ICD10 date-66'], row['ICD10 date-67'], row['ICD10 date-68'],\n",
    "             row['ICD10 date-69'], row['ICD10 date-70'], row['ICD10 date-71'],\n",
    "             row['ICD10 date-72'], row['ICD10 date-73'], row['ICD10 date-74'],\n",
    "             row['ICD10 date-75'], row['ICD10 date-76'], row['ICD10 date-77'],\n",
    "             row['ICD10 date-78'], row['ICD10 date-79'], row['ICD10 date-80'],\n",
    "             row['ICD10 date-81'], row['ICD10 date-82'],row['ICD10 date-83'],\n",
    "             row['ICD10 date-84'], row['ICD10 date-85'],row['ICD10 date-86'],\n",
    "             row['ICD10 date-87'], row['ICD10 date-88'],row['ICD10 date-89'], row['ICD10 date-90'],\n",
    "             row['ICD10 date-91'], row['ICD10 date-92'],row['ICD10 date-93'],\n",
    "             row['ICD10 date-94'], row['ICD10 date-95'],row['ICD10 date-96'],\n",
    "             row['ICD10 date-97'], row['ICD10 date-98'],row['ICD10 date-99'], row['ICD10 date-100'],\n",
    "             row['ICD10 date-101'], row['ICD10 date-102'],\n",
    "             row['ICD10 date-103'], row['ICD10 date-104'], row['ICD10 date-105'],\n",
    "             row['ICD10 date-106'], row['ICD10 date-107'], row['ICD10 date-108'],\n",
    "             row['ICD10 date-109'], row['ICD10 date-110'], row['ICD10 date-111'],\n",
    "             row['ICD10 date-112'], row['ICD10 date-113'], row['ICD10 date-114'],\n",
    "             row['ICD10 date-115'], row['ICD10 date-116'], row['ICD10 date-117'],\n",
    "             row['ICD10 date-118'], row['ICD10 date-119'], row['ICD10 date-120'],\n",
    "             row['ICD10 date-121'], row['ICD10 date-122'], row['ICD10 date-123'],\n",
    "             row['ICD10 date-124'], row['ICD10 date-125'], row['ICD10 date-126'],\n",
    "             row['ICD10 date-127'], row['ICD10 date-128'], row['ICD10 date-129'],\n",
    "             row['ICD10 date-130'], row['ICD10 date-131'], row['ICD10 date-132'],\n",
    "             row['ICD10 date-133'], row['ICD10 date-134'], row['ICD10 date-135'],\n",
    "             row['ICD10 date-136'], row['ICD10 date-137'], row['ICD10 date-138'],\n",
    "             row['ICD10 date-139'], row['ICD10 date-140'], row['ICD10 date-141'],\n",
    "             row['ICD10 date-142'], row['ICD10 date-143'], row['ICD10 date-144'],\n",
    "             row['ICD10 date-145'], row['ICD10 date-146'], row['ICD10 date-147'],\n",
    "             row['ICD10 date-148'], row['ICD10 date-149'], row['ICD10 date-150'],\n",
    "             row['ICD10 date-151'], row['ICD10 date-152'], row['ICD10 date-153'],\n",
    "             row['ICD10 date-154'], row['ICD10 date-155'], row['ICD10 date-156'],\n",
    "             row['ICD10 date-157'], row['ICD10 date-158'], row['ICD10 date-159'],\n",
    "             row['ICD10 date-160'], row['ICD10 date-161'], row['ICD10 date-162'],\n",
    "             row['ICD10 date-163'], row['ICD10 date-164'], row['ICD10 date-165'],\n",
    "             row['ICD10 date-166'], row['ICD10 date-167'], row['ICD10 date-168'],\n",
    "             row['ICD10 date-169'], row['ICD10 date-170'], row['ICD10 date-171'],\n",
    "             row['ICD10 date-172'], row['ICD10 date-173'], row['ICD10 date-174'],\n",
    "             row['ICD10 date-175'], row['ICD10 date-176'], row['ICD10 date-177'],\n",
    "             row['ICD10 date-178'], row['ICD10 date-179'], row['ICD10 date-180'],\n",
    "             row['ICD10 date-181'], row['ICD10 date-182'],row['ICD10 date-183'],\n",
    "             row['ICD10 date-184'], row['ICD10 date-185'],row['ICD10 date-186'],\n",
    "             row['ICD10 date-187'], row['ICD10 date-188'],row['ICD10 date-189'], row['ICD10 date-190'],\n",
    "             row['ICD10 date-191'], row['ICD10 date-192'],row['ICD10 date-193'],\n",
    "             row['ICD10 date-194'], row['ICD10 date-195'],row['ICD10 date-196'],\n",
    "             row['ICD10 date-197'], row['ICD10 date-198'],row['ICD10 date-199'], row['ICD10 date-200'],\n",
    "             row['ICD10 date-201'], row['ICD10 date-202'],\n",
    "             row['ICD10 date-203'], row['ICD10 date-204'], row['ICD10 date-205'],\n",
    "             row['ICD10 date-206'], row['ICD10 date-207'], row['ICD10 date-208'],\n",
    "             row['ICD10 date-209'], row['ICD10 date-210'], row['ICD10 date-211'],\n",
    "             row['ICD10 date-212'], row['ICD10 date-213'], row['ICD10 date-214'],\n",
    "             row['ICD10 date-215'], row['ICD10 date-216'], row['ICD10 date-217'],\n",
    "             row['ICD10 date-218'], row['ICD10 date-219'], row['ICD10 date-220'],\n",
    "             row['ICD10 date-221'], row['ICD10 date-222'], row['ICD10 date-223'],\n",
    "             row['ICD10 date-224'], row['ICD10 date-225'], row['ICD10 date-226'],\n",
    "             row['ICD10 date-227'], row['ICD10 date-228'], row['ICD10 date-229'],\n",
    "             row['ICD10 date-230'], row['ICD10 date-231'], row['ICD10 date-232'],\n",
    "             row['ICD10 date-233'], row['ICD10 date-234'], row['ICD10 date-235'],\n",
    "             row['ICD10 date-236'], row['ICD10 date-237'], row['ICD10 date-238'],\n",
    "             row['ICD10 date-239'], row['ICD10 date-240'], row['ICD10 date-241'],\n",
    "             row['ICD10 date-242']]\n",
    "\n",
    "\n",
    "    for i in range(243):\n",
    "        new_row = [participant_id,year_of_birth,month_of_birth,ethnicity,sex, icd_names[i] if len(icd_names) > i else None, dates[i]]\n",
    "        Storing_ICD10_Codes_Dates_Table.append(new_row)\n",
    "\n",
    "#### Create a new DataFrame with the modified data\n",
    "ICD10_Codes_Dates_dataset = []\n",
    "ICD10_Codes_Dates_dataset = pd.DataFrame(Storing_ICD10_Codes_Dates_Table, columns=['Participant ID','Year of Birth','Month of Birth','Ethnicity','Sex','ICD10 codes with Diseases Names', 'ICD10 Diagnosis Date'])\n",
    "#### Remove rows with NaN values in both columns\n",
    "ICD10_Codes_Dates_dataset = ICD10_Codes_Dates_dataset.dropna(subset=['ICD10 codes with Diseases Names', 'ICD10 Diagnosis Date'])\n",
    "ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "#### Reset the index after dropping rows\n",
    "ICD10_Codes_Dates_dataset.reset_index(drop=True, inplace=True)\n",
    "Organised_ICD10_Codes_Dates_Table = []\n",
    "Organised_ICD10_Codes_Dates_Table = ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Organised_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Organised_ICD10_Codes_Dates_Table in Organised_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Organised_ICD10_Codes_Dates_Table = len(Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Organised_ICD10_Codes_Dates_Table = Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [unique_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [row_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [nan_count_Organised_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Organised_ICD10_Codes_Dates_Table[column_Organised_ICD10_Codes_Dates_Table] = [empty_count_Organised_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Organised_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Organised_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Organised_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Organised_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Organised_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Organised_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Organised_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Organised_ICD10_Codes_Dates_Table = []\n",
    "result_Organised_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Organised_ICD10_Codes_Dates_Table_databank, row_counts_Organised_ICD10_Codes_Dates_Table_databank, nan_counts_Organised_ICD10_Codes_Dates_Table_databank, empty_counts_Organised_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Organised ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_Organised_ICD10_Codes_Dates_Table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Seperate the ICD10 Codes and ICD10 Diseases columns\n",
    "Organised_ICD10_Codes_Dates_dataset = []\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_Table\n",
    "Organised_ICD10_Codes_Dates_dataset['ICD10 codes'] = Organised_ICD10_Codes_Dates_dataset['ICD10 codes with Diseases Names'].str.extract(r'([A-Z]\\d+\\.\\d+|[A-Z]\\d+)')  # Handles both cases\n",
    "Organised_ICD10_Codes_Dates_dataset['ICD10 Diseases'] = Organised_ICD10_Codes_Dates_dataset['ICD10 codes with Diseases Names'].str.replace(r'[A-Z]\\d+\\.\\d+|[A-Z]\\d+', '')\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_dataset.drop(columns=['ICD10 codes with Diseases Names'])\n",
    "Organised_ICD10_Codes_Dates_dataset = Organised_ICD10_Codes_Dates_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "ICD10_Codes_Dates_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIFlhf-tQcWJ"
   },
   "source": [
    "#### <center> 5(c). Extract the \"Main ICD10 Codes\" Variables from the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VZLxkG9QcWK",
    "outputId": "c7be2771-c591-4c7b-c2fd-ea787f043d83"
   },
   "outputs": [],
   "source": [
    "#### initialize df1 as ICD10 codes dataset\n",
    "Main_ICD10_Codes_Dates_Table = []\n",
    "Main_ICD10_Codes_Dates_Table = ICD10_dataset.iloc[:, :5].join(ICD10_dataset.iloc[:, 249:329])\n",
    "Main_ICD10_Codes_Dates_Table  = Main_ICD10_Codes_Dates_Table .dropna(subset=['Main ICD10 - Diagnosis'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Main_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Main_ICD10_Codes_Dates_Table in Main_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Main_ICD10_Codes_Dates_Table = len(Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [unique_count_Main_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [row_count_Main_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [nan_count_Main_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Main_ICD10_Codes_Dates_Table[column_Main_ICD10_Codes_Dates_Table] = [empty_count_Main_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Main_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Main_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Main_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Main_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Main_ICD10_Codes_Dates_Table = []\n",
    "result_Main_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Main_ICD10_Codes_Dates_Table_databank, row_counts_Main_ICD10_Codes_Dates_Table_databank, nan_counts_Main_ICD10_Codes_Dates_Table_databank, empty_counts_Main_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Main ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_Main_ICD10_Codes_Dates_Table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Main_ICD10_Codes_Dates_Table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCzm6QfFQcWN"
   },
   "source": [
    "#### <center> 5(d). Structure the \"Main ICD10 Codes\" with related Diseases and Diagnosis Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEHOhz4JQcWN",
    "outputId": "7fafa519-9ca6-41af-81fe-ab3965a12599"
   },
   "outputs": [],
   "source": [
    "#### Create a new DataFrame to store the modified data\n",
    "Storing_Main_ICD10_Codes_Dates_Table = []\n",
    "\n",
    "#### Iterate through each row\n",
    "for idx_Main, row in Main_ICD10_Codes_Dates_Table.iterrows():\n",
    "    participant_id_Main = row['Participant ID']\n",
    "    year_of_birth_Main = row['Year of Birth']\n",
    "    month_of_birth_Main = row['Month of Birth']\n",
    "    ethnicity_Main = row['Ethnicity']\n",
    "    sex_Main = row['Sex']\n",
    "    icd_names_Main = row['Main ICD10 - Diagnosis'].split('|')\n",
    "    dates_Main = [row['Main ICD10 date-0'], row['Main ICD10 date-1'], row['Main ICD10 date-2'],\n",
    "             row['Main ICD10 date-3'], row['Main ICD10 date-4'], row['Main ICD10 date-5'],\n",
    "             row['Main ICD10 date-6'], row['Main ICD10 date-7'], row['Main ICD10 date-8'],\n",
    "             row['Main ICD10 date-9'], row['Main ICD10 date-10'], row['Main ICD10 date-11'],\n",
    "             row['Main ICD10 date-12'], row['Main ICD10 date-13'], row['Main ICD10 date-14'],\n",
    "             row['Main ICD10 date-15'], row['Main ICD10 date-16'], row['Main ICD10 date-17'],\n",
    "             row['Main ICD10 date-18'], row['Main ICD10 date-19'], row['Main ICD10 date-20'],\n",
    "             row['Main ICD10 date-21'], row['Main ICD10 date-22'], row['Main ICD10 date-23'],\n",
    "             row['Main ICD10 date-24'], row['Main ICD10 date-25'], row['Main ICD10 date-26'],\n",
    "             row['Main ICD10 date-27'], row['Main ICD10 date-28'], row['Main ICD10 date-29'],\n",
    "             row['Main ICD10 date-30'], row['Main ICD10 date-31'], row['Main ICD10 date-32'],\n",
    "             row['Main ICD10 date-33'], row['Main ICD10 date-34'], row['Main ICD10 date-35'],\n",
    "             row['Main ICD10 date-36'], row['Main ICD10 date-37'], row['Main ICD10 date-38'],\n",
    "             row['Main ICD10 date-39'], row['Main ICD10 date-40'], row['Main ICD10 date-41'],\n",
    "             row['Main ICD10 date-42'], row['Main ICD10 date-43'], row['Main ICD10 date-44'],\n",
    "             row['Main ICD10 date-45'], row['Main ICD10 date-46'], row['Main ICD10 date-47'],\n",
    "             row['Main ICD10 date-48'], row['Main ICD10 date-49'], row['Main ICD10 date-50'],\n",
    "             row['Main ICD10 date-51'], row['Main ICD10 date-52'], row['Main ICD10 date-53'],\n",
    "             row['Main ICD10 date-54'], row['Main ICD10 date-55'], row['Main ICD10 date-56'],\n",
    "             row['Main ICD10 date-57'], row['Main ICD10 date-58'], row['Main ICD10 date-59'],\n",
    "             row['Main ICD10 date-60'], row['Main ICD10 date-61'], row['Main ICD10 date-62'],\n",
    "             row['Main ICD10 date-63'], row['Main ICD10 date-64'], row['Main ICD10 date-65'],\n",
    "             row['Main ICD10 date-66'], row['Main ICD10 date-67'], row['Main ICD10 date-68'],\n",
    "             row['Main ICD10 date-69'], row['Main ICD10 date-70'], row['Main ICD10 date-71'],\n",
    "             row['Main ICD10 date-72'], row['Main ICD10 date-73'], row['Main ICD10 date-74'],\n",
    "             row['Main ICD10 date-75'], row['Main ICD10 date-76'], row['Main ICD10 date-77'],\n",
    "             row['Main ICD10 date-78']]\n",
    "\n",
    "\n",
    "    for i in range(79):\n",
    "        new_row_Main = [participant_id_Main,year_of_birth_Main,month_of_birth_Main,ethnicity_Main,sex_Main, icd_names_Main[i] if len(icd_names_Main) > i else None, dates_Main[i]]\n",
    "        Storing_Main_ICD10_Codes_Dates_Table.append(new_row_Main)\n",
    "\n",
    "#### Create a new DataFrame with the modified data\n",
    "Main_ICD10_Codes_Dates_dataset = []\n",
    "Main_ICD10_Codes_Dates_dataset = pd.DataFrame(Storing_Main_ICD10_Codes_Dates_Table, columns=['Participant ID','Year of Birth','Month of Birth','Ethnicity','Sex','Main ICD10 codes with Diseases Names', 'Main ICD10 Diagnosis Date'])\n",
    "### Remove rows with NaN values in both columns\n",
    "Main_ICD10_Codes_Dates_dataset = Main_ICD10_Codes_Dates_dataset.dropna(subset=['Main ICD10 codes with Diseases Names', 'Main ICD10 Diagnosis Date'])\n",
    "Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "#### Reset the index after dropping rows\n",
    "Main_ICD10_Codes_Dates_dataset.reset_index(drop=True, inplace=True)\n",
    "Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Organised_Main_ICD10_Codes_Dates_Table = []\n",
    "Organised_Main_ICD10_Codes_Dates_Table = Main_ICD10_Codes_Dates_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "nan_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "empty_counts_Organised_Main_ICD10_Codes_Dates_Table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Organised_Main_ICD10_Codes_Dates_Table in Organised_Main_ICD10_Codes_Dates_Table.columns:\n",
    "    unique_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].nunique()\n",
    "    row_count_Organised_Main_ICD10_Codes_Dates_Table = len(Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table])\n",
    "    nan_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].isna().sum()  # Count NaN values\n",
    "    empty_count_Organised_Main_ICD10_Codes_Dates_Table = Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [unique_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    row_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [row_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    nan_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [nan_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "    empty_counts_Organised_Main_ICD10_Codes_Dates_Table[column_Organised_Main_ICD10_Codes_Dates_Table] = [empty_count_Organised_Main_ICD10_Codes_Dates_Table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = []\n",
    "nan_counts_Organised_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "empty_counts_Organised_Main_ICD10_ICD10_Codes_Dates_Table_databank = []\n",
    "\n",
    "unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(unique_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Unique Count'])\n",
    "row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(row_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Row Count'])\n",
    "nan_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(nan_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['NaN Count'])\n",
    "empty_counts_Organised_Main_ICD10_Codes_Dates_Table_databank = pd.DataFrame(empty_counts_Organised_Main_ICD10_Codes_Dates_Table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Organised_Main_ICD10_Codes_Dates_Table = []\n",
    "result_Organised_Main_ICD10_Codes_Dates_Table = pd.concat([unique_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, row_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, nan_counts_Organised_Main_ICD10_Codes_Dates_Table_databank, empty_counts_Organised_Main_ICD10_Codes_Dates_Table_databank])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Organised Main ICD10 Codes with Dates Table:\")\n",
    "print()\n",
    "display(result_Organised_Main_ICD10_Codes_Dates_Table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Seperate the Main ICD10 Codes and Main ICD10 Diseases columns\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = []\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_Table\n",
    "\n",
    "Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes'] = Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes with Diseases Names'].str.extract(r'([A-Z]\\d+\\.\\d+|[A-Z]\\d+)')  # Handles both cases\n",
    "Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 Diseases'] = Organised_Main_ICD10_Codes_Dates_dataset['Main ICD10 codes with Diseases Names'].str.replace(r'[A-Z]\\d+\\.\\d+|[A-Z]\\d+', '')\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_dataset.drop(columns=['Main ICD10 codes with Diseases Names'])\n",
    "Organised_Main_ICD10_Codes_Dates_dataset = Organised_Main_ICD10_Codes_Dates_dataset.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "Main_ICD10_Codes_Dates_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxcS834zQcWP"
   },
   "source": [
    "#### <center> 5(e). Concentenate \"ICD10_Code_dataset\"  and \"Main_ICD10_Code_dataset\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omeLLA8rQcWP",
    "outputId": "74e3c448-c563-4e7c-ecca-4674b908d69a"
   },
   "outputs": [],
   "source": [
    "### Concentenate (combine) ICD10 codes dataset  and Main ICD10 codes dataset\n",
    "data_table = []\n",
    "data_table = pd.concat([Organised_ICD10_Codes_Dates_dataset ,Organised_Main_ICD10_Codes_Dates_dataset])\n",
    "data_table = data_table.sort_values(by='Participant ID')\n",
    "data_table.fillna('', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_data_table = {}\n",
    "row_counts_data_table= {}\n",
    "nan_counts_data_table = {}\n",
    "empty_counts_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_data_table in data_table.columns:\n",
    "    unique_count_data_table = data_table[column_data_table].nunique()\n",
    "    row_count_data_table = len(data_table[column_data_table])\n",
    "    nan_count_data_table = data_table[column_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_data_table = data_table[column_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_data_table[column_data_table] = [unique_count_data_table]\n",
    "    row_counts_data_table[column_data_table] = [row_count_data_table]\n",
    "    nan_counts_data_table[column_data_table] = [nan_count_data_table]\n",
    "    empty_counts_data_table[column_data_table] = [empty_count_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_data_table_records = []\n",
    "row_counts_data_table_records = []\n",
    "nan_counts_data_table_records = []\n",
    "empty_counts_data_table_records = []\n",
    "\n",
    "unique_counts_data_table_records = pd.DataFrame(unique_counts_data_table, index=['Unique Count'])\n",
    "row_counts_data_table_records = pd.DataFrame(row_counts_data_table, index=['Row Count'])\n",
    "nan_counts_data_table_records = pd.DataFrame(nan_counts_data_table, index=['NaN Count'])\n",
    "empty_counts_data_table_records = pd.DataFrame(empty_counts_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_data_table = []\n",
    "result_data_table = pd.concat([unique_counts_data_table_records, row_counts_data_table_records, nan_counts_data_table_records, empty_counts_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Data Table Record:\")\n",
    "print()\n",
    "display(result_data_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YNKCKrKQcWR"
   },
   "source": [
    "#### <center> 5(f). Create Two new Variables: \"Combined ICD10 Codes\" and \"Combined ICD10 Diagnosis Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdHI2u4GQcWR",
    "outputId": "dfb35d7f-8422-47be-cac5-9108b78a61ae"
   },
   "outputs": [],
   "source": [
    "### Create a new column 'Combined ICD10 Codes' by concatenating 'ICD10 codes' and 'Main ICD10 codes'\n",
    "data_table['Combined ICD10 Codes'] = data_table['ICD10 codes'] + data_table['Main ICD10 codes']\n",
    "\n",
    "\n",
    "#### Create a new column 'Combined ICD10 Diagnosis Date' by selecting the non-empty value between 'ICD10 Diagnosis Date' and 'Main ICD10 Diagnosis Date'\n",
    "data_table['Combined ICD10 Diagnosis Date'] = np.where(data_table['ICD10 Diagnosis Date'] != '', data_table['ICD10 Diagnosis Date'], data_table['Main ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "#### Create a new column 'Combined ICD10 Diseases' by concatenating 'ICD10 Diseases' and 'Main ICD10 Diseases'\n",
    "data_table['Combined ICD10 Diseases'] = data_table['ICD10 Diseases']  + data_table['Main ICD10 Diseases']\n",
    "\n",
    "\n",
    "### Drop the original columns that are no longer needed\n",
    "data_table.drop(['ICD10 codes', 'ICD10 Diagnosis Date', 'Main ICD10 codes', 'Main ICD10 Diagnosis Date', 'ICD10 Diseases', 'Main ICD10 Diseases'], axis=1, inplace=True)\n",
    "\n",
    "data_table = data_table.sort_values(by=['Participant ID','Combined ICD10 Codes','Combined ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "\n",
    "Combined_ICD10_data_table = []\n",
    "Combined_ICD10_data_table = data_table\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Combined_ICD10_data_table = {}\n",
    "row_counts_Combined_ICD10_data_table= {}\n",
    "nan_counts_Combined_ICD10_data_table = {}\n",
    "empty_counts_Combined_ICD10_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Combined_ICD10_data_table in Combined_ICD10_data_table.columns:\n",
    "    unique_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].nunique()\n",
    "    row_count_Combined_ICD10_data_table = len(Combined_ICD10_data_table[column_Combined_ICD10_data_table])\n",
    "    nan_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_Combined_ICD10_data_table = Combined_ICD10_data_table[column_Combined_ICD10_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [unique_count_Combined_ICD10_data_table]\n",
    "    row_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [row_count_Combined_ICD10_data_table]\n",
    "    nan_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [nan_count_Combined_ICD10_data_table]\n",
    "    empty_counts_Combined_ICD10_data_table[column_Combined_ICD10_data_table] = [empty_count_Combined_ICD10_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Combined_ICD10_data_table_records = []\n",
    "row_counts_Combined_ICD10_data_table_records = []\n",
    "nan_counts_Combined_ICD10_data_table_records = []\n",
    "empty_counts_Combined_ICD10_data_table_records = []\n",
    "\n",
    "unique_counts_Combined_ICD10_data_table_records = pd.DataFrame(unique_counts_Combined_ICD10_data_table, index=['Unique Count'])\n",
    "row_counts_Combined_ICD10_data_table_records = pd.DataFrame(row_counts_Combined_ICD10_data_table, index=['Row Count'])\n",
    "nan_counts_Combined_ICD10_data_table_records = pd.DataFrame(nan_counts_Combined_ICD10_data_table, index=['NaN Count'])\n",
    "empty_counts_Combined_ICD10_data_table_records = pd.DataFrame(empty_counts_Combined_ICD10_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Combined_ICD10_data_table = []\n",
    "result_Combined_ICD10_data_table = pd.concat([unique_counts_Combined_ICD10_data_table_records, row_counts_Combined_ICD10_data_table_records, nan_counts_Combined_ICD10_data_table_records, empty_counts_Combined_ICD10_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Combined ICD10 Data Table Record:\")\n",
    "print()\n",
    "display(result_Combined_ICD10_data_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0gd9HheQcWT"
   },
   "source": [
    "#### <center> 5(g). Drop Duplicates and Rearrange Variables and Save the output as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDN-0NbXQcWV",
    "outputId": "941dad7a-6dfd-4104-c3d5-688c75edaf61"
   },
   "outputs": [],
   "source": [
    "data_table = data_table.drop_duplicates(['Participant ID','Combined ICD10 Codes','Combined ICD10 Diagnosis Date','Combined ICD10 Diseases'])\n",
    "data_table\n",
    "\n",
    "\n",
    "#### Rearrange the columns\n",
    "data_table = data_table[['Participant ID', 'Sex', 'Year of Birth', 'Month of Birth', 'Ethnicity', 'Combined ICD10 Diseases','Combined ICD10 Diagnosis Date','Combined ICD10 Codes']]\n",
    "data_table\n",
    "\n",
    "\n",
    "dropped_duplicates_data_table = []\n",
    "dropped_duplicates_data_table = data_table\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_dropped_duplicates_data_table = {}\n",
    "row_counts_dropped_duplicates_data_table= {}\n",
    "nan_counts_dropped_duplicates_data_table = {}\n",
    "empty_counts_dropped_duplicates_data_table = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_dropped_duplicates_data_table in dropped_duplicates_data_table.columns:\n",
    "    unique_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].nunique()\n",
    "    row_count_dropped_duplicates_data_table = len(dropped_duplicates_data_table[column_dropped_duplicates_data_table])\n",
    "    nan_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].isna().sum()  # Count NaN values\n",
    "    empty_count_dropped_duplicates_data_table = dropped_duplicates_data_table[column_dropped_duplicates_data_table].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [unique_count_dropped_duplicates_data_table]\n",
    "    row_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [row_count_dropped_duplicates_data_table]\n",
    "    nan_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [nan_count_dropped_duplicates_data_table]\n",
    "    empty_counts_dropped_duplicates_data_table[column_dropped_duplicates_data_table] = [empty_count_dropped_duplicates_data_table]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_dropped_duplicates_data_table_records = []\n",
    "row_counts_dropped_duplicates_data_table_records = []\n",
    "nan_counts_dropped_duplicates_data_table_records = []\n",
    "empty_counts_dropped_duplicates_data_table_records = []\n",
    "\n",
    "unique_counts_dropped_duplicates_data_table_records = pd.DataFrame(unique_counts_dropped_duplicates_data_table, index=['Unique Count'])\n",
    "row_counts_dropped_duplicates_data_table_records = pd.DataFrame(row_counts_dropped_duplicates_data_table, index=['Row Count'])\n",
    "nan_counts_dropped_duplicates_data_table_records = pd.DataFrame(nan_counts_dropped_duplicates_data_table, index=['NaN Count'])\n",
    "empty_counts_dropped_duplicates_data_table_records = pd.DataFrame(empty_counts_dropped_duplicates_data_table, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_dropped_duplicates_data_table = []\n",
    "result_dropped_duplicates_data_table = pd.concat([unique_counts_dropped_duplicates_data_table_records, row_counts_dropped_duplicates_data_table_records, nan_counts_dropped_duplicates_data_table_records, empty_counts_dropped_duplicates_data_table_records])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Dropped Duplicates from Data Table Record:\")\n",
    "print()\n",
    "display(result_dropped_duplicates_data_table)\n",
    "\n",
    "\n",
    "\n",
    "data_table\n",
    "\n",
    "\n",
    "\n",
    "#### Specify the file path where you want to save the CSV file\n",
    "#file_path = 'All ICD10 Codes with Diseases Names and Dates Data.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "#data_table.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlZCZhIUQcWW"
   },
   "source": [
    "### <center> 6. Import \"All_ICD10_with_Diseases_Dates_Data\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euNBXX23QcWW",
    "outputId": "63f261ef-e5bd-439d-8f54-9e70e89e7c10"
   },
   "outputs": [],
   "source": [
    "All_ICD10_with_Diseases_Dates_Data = []\n",
    "All_ICD10_with_Diseases_Dates_Data = pd.read_csv('All ICD10 Codes with Diseases Names and Dates Data.csv')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "#print(\"Dropped Duplicates from Data Table Record:\")\n",
    "print()\n",
    "#display(result_dropped_duplicates_data_table)\n",
    "\n",
    "All_ICD10_with_Diseases_Dates_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZv6FJShQcWX"
   },
   "source": [
    "### <center> 7. Merge the datasets \"Death_Date_Record\" and \"Death_Cause_Record\" based on the \"Participant ID\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_PHOUbFQcWY",
    "outputId": "d446ad71-2133-41b3-d474-935ff4ed068c"
   },
   "outputs": [],
   "source": [
    "### Merge the datasets based on the \"Participant ID\" column\n",
    "Death_Pair_datasets = []\n",
    "Death_Pair_datasets = pd.merge(Death_Date_Record, Death_Cause_Record,   on=\"Participant ID\", how=\"outer\")\n",
    "Death_Pair_datasets = Death_Pair_datasets.sort_values(by=['Participant ID'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Death_Pair_datasets = {}\n",
    "row_counts_Death_Pair_datasets = {}\n",
    "nan_counts_Death_Pair_datasets = {}\n",
    "empty_counts_Death_Pair_datasets = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Death_Pair_datasets in Death_Pair_datasets.columns:\n",
    "    unique_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].nunique()\n",
    "    row_count_Death_Pair_datasets = len(Death_Pair_datasets[column_Death_Pair_datasets])\n",
    "    nan_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].isna().sum()  # Count NaN values\n",
    "    empty_count_Death_Pair_datasets = Death_Pair_datasets[column_Death_Pair_datasets].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [unique_count_Death_Pair_datasets]\n",
    "    row_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [row_count_Death_Pair_datasets]\n",
    "    nan_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [nan_count_Death_Pair_datasets]\n",
    "    empty_counts_Death_Pair_datasets[column_Death_Pair_datasets] = [empty_count_Death_Pair_datasets]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Death_Pair = []\n",
    "row_counts_Death_Pair = []\n",
    "nan_counts_Death_Pair = []\n",
    "empty_counts_Death_Pair = []\n",
    "\n",
    "unique_counts_Death_Pair = pd.DataFrame(unique_counts_Death_Pair_datasets, index=['Unique Count'])\n",
    "row_counts_Death_Pair = pd.DataFrame(row_counts_Death_Pair_datasets, index=['Row Count'])\n",
    "nan_counts_Death_Pair = pd.DataFrame(nan_counts_Death_Pair_datasets, index=['NaN Count'])\n",
    "empty_counts_Death_Pair = pd.DataFrame(empty_counts_Death_Pair_datasets, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Death_Pair_datasets = []\n",
    "result_Death_Pair_datasets = pd.concat([unique_counts_Death_Pair, row_counts_Death_Pair, nan_counts_Death_Pair, empty_counts_Death_Pair])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Death Date & Cause Paired Datasets:\")\n",
    "print()\n",
    "display(result_Death_Pair_datasets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Death_Pair_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfy9cc1LQcWZ",
    "outputId": "9919e4ab-add6-47b4-8cec-2c4dc4c8fb61"
   },
   "outputs": [],
   "source": [
    "### Count NaN values in the DataFrame\n",
    "nan_counts_in_Death_Pair_datasets = []\n",
    "nan_counts_in_Death_Pair_datasets = Death_Pair_datasets.isna().sum()\n",
    "nan_counts_in_Death_Pair_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5h6uCIKBQcWa",
    "outputId": "18b8841b-0a9b-4ce2-b6b9-dcedc4b8f4dc"
   },
   "outputs": [],
   "source": [
    "### Create a Boolean mask to identify rows with NaN values\n",
    "nan_mask = []\n",
    "nan_mask = Death_Pair_datasets.isna().any(axis=1)\n",
    "\n",
    "#### Use the mask to extract and display rows with NaN values\n",
    "rows_with_nan_in_Death_Pair_datasets = Death_Pair_datasets[nan_mask]\n",
    "\n",
    "#### Display the rows with NaN values\n",
    "rows_with_nan_in_Death_Pair_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSbRuXwMQcWa"
   },
   "source": [
    "### <center> 8. Merge the datsets: \"Death_Pair_datasets\" and \"All_ICD10_with_Diseases_Dates_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhX6A5X8QcWb",
    "outputId": "7afbc861-daf0-40cb-89a4-a99f0bf2ce30"
   },
   "outputs": [],
   "source": [
    "Final_Dataset = []\n",
    "Final_Dataset = pd.merge(Death_Pair_datasets,All_ICD10_with_Diseases_Dates_Data,   on=\"Participant ID\", how=\"outer\")\n",
    "new_order = ['Participant ID', 'Year of Birth', 'Month of Birth' , 'Sex' , 'Ethnicity' ,  'Combined ICD10 Diagnosis Date', 'Combined ICD10 Diseases' , 'Date of Death' , 'Death Cause Diseases' , 'Combined ICD10 Codes','Death Cause Disease ICD10 Codes']  # Specify the order of columns\n",
    "Final_Dataset = Final_Dataset[new_order]\n",
    "Final_Dataset = Final_Dataset.sort_values(by=['Participant ID', 'Combined ICD10 Diagnosis Date'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Final_Dataset = {}\n",
    "row_counts_Final_Dataset = {}\n",
    "nan_counts_Final_Dataset = {}\n",
    "empty_counts_Final_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Final_Dataset in Final_Dataset.columns:\n",
    "    unique_count_Final_Dataset = Final_Dataset[column_Final_Dataset].nunique()\n",
    "    row_count_Final_Dataset = len(Final_Dataset[column_Final_Dataset])\n",
    "    nan_count_Final_Dataset = Final_Dataset[column_Final_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_Final_Dataset = Final_Dataset[column_Final_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Final_Dataset[column_Final_Dataset] = [unique_count_Final_Dataset]\n",
    "    row_counts_Final_Dataset[column_Final_Dataset] = [row_count_Final_Dataset]\n",
    "    nan_counts_Final_Dataset[column_Final_Dataset] = [nan_count_Final_Dataset]\n",
    "    empty_counts_Final_Dataset[column_Final_Dataset] = [empty_count_Final_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Final_Dataset_Record = []\n",
    "row_counts_Final_Dataset_Record = []\n",
    "nan_counts_Final_Dataset_Record = []\n",
    "empty_counts_Final_Dataset_Record = []\n",
    "\n",
    "unique_counts_Final_Dataset_Record = pd.DataFrame(unique_counts_Final_Dataset, index=['Unique Count'])\n",
    "row_counts_Final_Dataset_Record = pd.DataFrame(row_counts_Final_Dataset, index=['Row Count'])\n",
    "nan_counts_Final_Dataset_Record = pd.DataFrame(nan_counts_Final_Dataset, index=['NaN Count'])\n",
    "empty_counts_Final_Dataset_Record = pd.DataFrame(empty_counts_Final_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Final_Dataset = []\n",
    "result_Final_Dataset = pd.concat([unique_counts_Final_Dataset_Record, row_counts_Final_Dataset_Record, nan_counts_Final_Dataset_Record, empty_counts_Final_Dataset_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "print()\n",
    "display(result_Final_Dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMS0JBAQQcWc",
    "outputId": "0e4c26cc-f0a0-452d-fe2b-118387a4b52e"
   },
   "outputs": [],
   "source": [
    "Final_Dataset = Final_Dataset.drop_duplicates()\n",
    "Dropped_Duplicates_Final_Dataset = []\n",
    "Dropped_Duplicates_Final_Dataset = Final_Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "row_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Dropped_Duplicates_Final_Dataset in Dropped_Duplicates_Final_Dataset.columns:\n",
    "    unique_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].nunique()\n",
    "    row_count_Dropped_Duplicates_Final_Dataset = len(Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset])\n",
    "    nan_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_Dropped_Duplicates_Final_Dataset = Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [unique_count_Dropped_Duplicates_Final_Dataset]\n",
    "    row_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [row_count_Dropped_Duplicates_Final_Dataset]\n",
    "    nan_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [nan_count_Dropped_Duplicates_Final_Dataset]\n",
    "    empty_counts_Dropped_Duplicates_Final_Dataset[column_Dropped_Duplicates_Final_Dataset] = [empty_count_Dropped_Duplicates_Final_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "row_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset_Record = []\n",
    "\n",
    "unique_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(unique_counts_Dropped_Duplicates_Final_Dataset, index=['Unique Count'])\n",
    "row_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(row_counts_Dropped_Duplicates_Final_Dataset, index=['Row Count'])\n",
    "nan_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(nan_counts_Dropped_Duplicates_Final_Dataset, index=['NaN Count'])\n",
    "empty_counts_Dropped_Duplicates_Final_Dataset_Record = pd.DataFrame(empty_counts_Dropped_Duplicates_Final_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Dropped_Duplicates_Final_Dataset = []\n",
    "result_Dropped_Duplicates_Final_Dataset = pd.concat([unique_counts_Dropped_Duplicates_Final_Dataset_Record, row_counts_Dropped_Duplicates_Final_Dataset_Record, nan_counts_Dropped_Duplicates_Final_Dataset_Record, empty_counts_Dropped_Duplicates_Final_Dataset_Record])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Dropped Duplicates Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "print()\n",
    "display(result_Dropped_Duplicates_Final_Dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGgpKp51QcWd",
    "outputId": "fc94598c-139e-4444-fbe7-b3c80fbd7011"
   },
   "outputs": [],
   "source": [
    "#### Count NaN values in the DataFrame\n",
    "nan_counts_Final_Dataset = []\n",
    "nan_counts_Final_Dataset = Final_Dataset.isna().sum()\n",
    "nan_counts_Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSvFwr9nQcWe",
    "outputId": "553966f4-0f5c-42f0-8d45-9a0a95f64e76"
   },
   "outputs": [],
   "source": [
    "### Create a Boolean mask to identify rows with NaN values\n",
    "nan_mask_Final_Dataset = []\n",
    "nan_mask_Final_Dataset = Final_Dataset[['Year of Birth', 'Month of Birth' , 'Sex' , 'Combined ICD10 Diagnosis Date' , 'Combined ICD10 Codes' , 'Combined ICD10 Codes']].isna().any(axis=1)\n",
    "\n",
    "### Use the mask to extract and display rows with NaN values\n",
    "rows_with_nan_Final_Dataset = []\n",
    "rows_with_nan_Final_Dataset = Final_Dataset[nan_mask_Final_Dataset]\n",
    "\n",
    "#### Display the rows with NaN values\n",
    "rows_with_nan_Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWb-1_vXQcWf",
    "outputId": "ce0994dd-2040-40de-a621-e5b467166aaa"
   },
   "outputs": [],
   "source": [
    "#### List of columns with NaN values to consider\n",
    "columns_with_nan = []\n",
    "columns_with_nan = [\"Year of Birth\", \"Month of Birth\", \"Sex\", \"Combined ICD10 Diagnosis Date\", \"Combined ICD10 Diseases\", \"Combined ICD10 Codes\"]\n",
    "columns_with_nan\n",
    "\n",
    "\n",
    "\n",
    "#### Remove rows with NaN values in the specified columns\n",
    "Final_Dataset = Final_Dataset.dropna(subset=columns_with_nan)\n",
    "Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PA4JEqXiQcWg",
    "outputId": "4e48aa77-6287-413b-af80-e64439c100a9"
   },
   "outputs": [],
   "source": [
    "#### Count NaN values in the DataFrame\n",
    "nan_counts_Final_Dataset = []\n",
    "nan_counts_Final_Dataset = Final_Dataset.isna().sum()\n",
    "nan_counts_Final_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNFFvyXmQcWh"
   },
   "source": [
    "### <center> 9. Create a new variable in the dataset: \"Alive / Dead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJwWpxYpQcWi",
    "outputId": "afdb4625-dd8f-4206-cfe3-a30a61eb25a8"
   },
   "outputs": [],
   "source": [
    "#### Create a new column \"Alive / Dead\" based on the condition\n",
    "Final_Dataset['Alive / Dead'] = np.where(Final_Dataset['Date of Death'].isna(), 'Alive', 'Dead')\n",
    "\n",
    "Final_Dataset2 = []\n",
    "Final_Dataset2 = Final_Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Final_Dataset2 = {}\n",
    "row_counts_Final_Dataset2 = {}\n",
    "nan_counts_Final_Dataset2 = {}\n",
    "empty_counts_Final_Dataset2 = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Final_Dataset2 in Final_Dataset2.columns:\n",
    "    unique_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].nunique()\n",
    "    row_count_Final_Dataset2 = len(Final_Dataset2[column_Final_Dataset2])\n",
    "    nan_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].isna().sum()  # Count NaN values\n",
    "    empty_count_Final_Dataset2 = Final_Dataset2[column_Final_Dataset2].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Final_Dataset2[column_Final_Dataset2] = [unique_count_Final_Dataset2]\n",
    "    row_counts_Final_Dataset2[column_Final_Dataset2] = [row_count_Final_Dataset2]\n",
    "    nan_counts_Final_Dataset2[column_Final_Dataset2] = [nan_count_Final_Dataset2]\n",
    "    empty_counts_Final_Dataset2[column_Final_Dataset2] = [empty_count_Final_Dataset2]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Final_Dataset_Record2 = []\n",
    "row_counts_Final_Dataset_Record2 = []\n",
    "nan_counts_Final_Dataset_Record2 = []\n",
    "empty_counts_Final_Dataset_Record2 = []\n",
    "\n",
    "unique_counts_Final_Dataset_Record2 = pd.DataFrame(unique_counts_Final_Dataset2, index=['Unique Count'])\n",
    "row_counts_Final_Dataset_Record2 = pd.DataFrame(row_counts_Final_Dataset2, index=['Row Count'])\n",
    "nan_counts_Final_Dataset_Record2 = pd.DataFrame(nan_counts_Final_Dataset2, index=['NaN Count'])\n",
    "empty_counts_Final_Dataset_Record2 = pd.DataFrame(empty_counts_Final_Dataset2, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Final_Dataset2 = []\n",
    "result_Final_Dataset2 = pd.concat([unique_counts_Final_Dataset_Record2, row_counts_Final_Dataset_Record2, nan_counts_Final_Dataset_Record2, empty_counts_Final_Dataset_Record2])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Updated Final (ICD10 dataset & Death Datasets) Dataset Record:\")\n",
    "print()\n",
    "display(result_Final_Dataset2)\n",
    "\n",
    "\n",
    "\n",
    "Final_Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_path = []\n",
    "\n",
    "## Specify the file path where you want to save the CSV file\n",
    "#file_path = 'Dataset with ICD10 and Diseases Names and Death Records.csv'\n",
    "\n",
    "## Use the to_csv method to save the DataFrame as a CSV file\n",
    "#Final_Dataset.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Import Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGApk3kbQcWj",
    "outputId": "bd0cf4d6-4335-426b-d6d7-bdc9722e3141"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = []\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records = pd.read_csv('Dataset with ICD10 and Diseases Names and Death Records.csv')\n",
    "Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Final_Dataset2 = {}\n",
    "row_counts_Final_Dataset2 = {}\n",
    "nan_counts_Final_Dataset2 = {}\n",
    "empty_counts_Final_Dataset2 = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Final_Dataset2 in Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.columns:\n",
    "    unique_count_Final_Dataset2 = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[column_Final_Dataset2].nunique()\n",
    "    row_count_Final_Dataset2 = len(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[column_Final_Dataset2])\n",
    "    nan_count_Final_Dataset2 = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[column_Final_Dataset2].isna().sum()  # Count NaN values\n",
    "    empty_count_Final_Dataset2 = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[column_Final_Dataset2].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Final_Dataset2[column_Final_Dataset2] = [unique_count_Final_Dataset2]\n",
    "    row_counts_Final_Dataset2[column_Final_Dataset2] = [row_count_Final_Dataset2]\n",
    "    nan_counts_Final_Dataset2[column_Final_Dataset2] = [nan_count_Final_Dataset2]\n",
    "    empty_counts_Final_Dataset2[column_Final_Dataset2] = [empty_count_Final_Dataset2]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Final_Dataset_Record2 = []\n",
    "row_counts_Final_Dataset_Record2 = []\n",
    "nan_counts_Final_Dataset_Record2 = []\n",
    "empty_counts_Final_Dataset_Record2 = []\n",
    "\n",
    "unique_counts_Final_Dataset_Record2 = pd.DataFrame(unique_counts_Final_Dataset2, index=['Unique Count'])\n",
    "row_counts_Final_Dataset_Record2 = pd.DataFrame(row_counts_Final_Dataset2, index=['Row Count'])\n",
    "nan_counts_Final_Dataset_Record2 = pd.DataFrame(nan_counts_Final_Dataset2, index=['NaN Count'])\n",
    "empty_counts_Final_Dataset_Record2 = pd.DataFrame(empty_counts_Final_Dataset2, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Final_Dataset2 = []\n",
    "result_Final_Dataset2 = pd.concat([unique_counts_Final_Dataset_Record2, row_counts_Final_Dataset_Record2, nan_counts_Final_Dataset_Record2, empty_counts_Final_Dataset_Record2])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Dataset_with_ICD10_and_Diseases_Names_and_Death_Records:\")\n",
    "print()\n",
    "display(result_Final_Dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNj1AmgeQcWk"
   },
   "source": [
    "### <center> 10. Extract PH Patients Data with ICD10 Codes (I27.0, I27.2, I27.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iz4DZ3zQcWl",
    "outputId": "6d3898a3-a61b-47d2-fcd4-e380d135f18d"
   },
   "outputs": [],
   "source": [
    "# List of values to filter\n",
    "Identify_PH_Patients_using_ICD10_codes = []\n",
    "Identify_PH_Patients_using_ICD10_codes = ['I27.0','I27.2','I27.9']\n",
    "\n",
    "# Extract rows where the \"Item\" column has one of the specified values\n",
    "PH_Patients_using_ICD10_codes_Dataset = []\n",
    "\n",
    "PH_Patients_using_ICD10_codes_Dataset = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Codes'].isin(Identify_PH_Patients_using_ICD10_codes))]# | (Complete_dataset['Drug Name'].notna())]\n",
    "PH_Patients_using_ICD10_codes_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm4def-wQcWm",
    "outputId": "d74f26c6-5b3c-4b6c-a262-2acc4fd4e75e"
   },
   "outputs": [],
   "source": [
    "# Replace 'Combined ICD10 Codes' with the actual column name\n",
    "column_name = 'Combined ICD10 Codes'\n",
    "\n",
    "icd10_to_ph_group = {'I27.0': 'Primary PH','I27.2': 'Other Secondary PH','I27.9': 'Secondary PH'}\n",
    "# Create the 'PH Group' column based on the mapping\n",
    "PH_Patients_using_ICD10_codes_Dataset['PH Group'] = PH_Patients_using_ICD10_codes_Dataset[column_name].map(icd10_to_ph_group)\n",
    "PH_Patients_using_ICD10_codes_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOr-XsbaQcWn"
   },
   "source": [
    "### <center> 11. Final Dataset of PH Patients (I27.0 = Primary PH, I27.2 = Other Secondary PH, I27.9 = Secondary PH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvZEzz0rQcWo",
    "outputId": "17cd2064-670b-4a2b-d3ea-a8c2ad4f5573"
   },
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "PH_Patients_using_ICD10_codes_Dataset\n",
    "PH_Patients_using_ICD10_codes_Dataset['Primary PH'] = np.nan\n",
    "PH_Patients_using_ICD10_codes_Dataset['Other Secondary PH'] = np.nan\n",
    "PH_Patients_using_ICD10_codes_Dataset['Secondary PH'] = np.nan\n",
    "\n",
    "# Update the new columns based on the values in the 'Combined ICD10 Codes' column\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.0', 'Primary PH'] = 'I27.0'\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.2', 'Other Secondary PH'] = 'I27.2'\n",
    "PH_Patients_using_ICD10_codes_Dataset.loc[PH_Patients_using_ICD10_codes_Dataset['Combined ICD10 Codes'] == 'I27.9', 'Secondary PH'] = 'I27.9'\n",
    "PH_Patients_using_ICD10_codes_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMHZqUlQQcWp",
    "outputId": "a5d01c11-a3eb-46c1-a1a0-a2643a53e04a"
   },
   "outputs": [],
   "source": [
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "row_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Dataset = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_PH_Patients_using_ICD10_codes_Dataset in PH_Patients_using_ICD10_codes_Dataset.columns:\n",
    "    unique_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].nunique()\n",
    "    row_count_PH_Patients_using_ICD10_codes_Dataset = len(PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset])\n",
    "    nan_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].isna().sum()  # Count NaN values\n",
    "    empty_count_PH_Patients_using_ICD10_codes_Dataset = PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [unique_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    row_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [row_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    nan_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [nan_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "    empty_counts_PH_Patients_using_ICD10_codes_Dataset[column_PH_Patients_using_ICD10_codes_Dataset] = [empty_count_PH_Patients_using_ICD10_codes_Dataset]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "row_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Datasets = []\n",
    "\n",
    "unique_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(unique_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Unique Count'])\n",
    "row_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(row_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Row Count'])\n",
    "nan_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(nan_counts_PH_Patients_using_ICD10_codes_Dataset, index=['NaN Count'])\n",
    "empty_counts_PH_Patients_using_ICD10_codes_Datasets = pd.DataFrame(empty_counts_PH_Patients_using_ICD10_codes_Dataset, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_PH_Patients_using_ICD10_codes_Dataset = []\n",
    "result_PH_Patients_using_ICD10_codes_Dataset = pd.concat([unique_counts_PH_Patients_using_ICD10_codes_Datasets, row_counts_PH_Patients_using_ICD10_codes_Datasets, nan_counts_PH_Patients_using_ICD10_codes_Datasets, empty_counts_PH_Patients_using_ICD10_codes_Datasets])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"PH Patients using ICD10 codes & Drugs Names:\")\n",
    "print()\n",
    "display(result_PH_Patients_using_ICD10_codes_Dataset)\n",
    "\n",
    "\n",
    "\n",
    "#PH_Patients_using_ICD10_codes_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm39Ja_2QcWr"
   },
   "source": [
    "### <center>  12. I27.0 = Primary PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAOATooYQcWr",
    "outputId": "b617636d-b52b-405b-dca6-f773575a186f"
   },
   "outputs": [],
   "source": [
    "# Extract rows where 'Primary PH' is 'I27.0'\n",
    "primary_ph_rows = []\n",
    "primary_ph_rows = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Primary PH'] == 'I27.0']\n",
    "primary_ph_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_primary_ph_rows = {}\n",
    "row_counts_primary_ph_rows = {}\n",
    "nan_counts_primary_ph_rows = {}\n",
    "empty_counts_primary_ph_rows = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_primary_ph_rows in primary_ph_rows.columns:\n",
    "    unique_count_primary_ph_rows  = primary_ph_rows [column_primary_ph_rows ].nunique()\n",
    "    row_count_primary_ph_rows  = len(primary_ph_rows [column_primary_ph_rows ])\n",
    "    nan_count_primary_ph_rows  = primary_ph_rows [column_primary_ph_rows ].isna().sum()  # Count NaN values\n",
    "    empty_count_primary_ph_rows  = primary_ph_rows [column_primary_ph_rows ].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_primary_ph_rows [column_primary_ph_rows ] = [unique_count_primary_ph_rows ]\n",
    "    row_counts_primary_ph_rows [column_primary_ph_rows ] = [row_count_primary_ph_rows ]\n",
    "    nan_counts_primary_ph_rows [column_primary_ph_rows ] = [nan_count_primary_ph_rows ]\n",
    "    empty_counts_primary_ph_rows [column_primary_ph_rows ] = [empty_count_primary_ph_rows ]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_primary_ph_rows1  = []\n",
    "row_counts_primary_ph_rows1  = []\n",
    "nan_counts_primary_ph_rows1  = []\n",
    "empty_counts_primary_ph_rows1  = []\n",
    "\n",
    "unique_counts_primary_ph_rows1  = pd.DataFrame(unique_counts_primary_ph_rows , index=['Unique Count'])\n",
    "row_counts_primary_ph_rows1  = pd.DataFrame(row_counts_primary_ph_rows , index=['Row Count'])\n",
    "nan_counts_primary_ph_rows1  = pd.DataFrame(nan_counts_primary_ph_rows , index=['NaN Count'])\n",
    "empty_counts_primary_ph_rows1  = pd.DataFrame(empty_counts_primary_ph_rows , index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_primary_ph_rows  = []\n",
    "result_primary_ph_rows  = pd.concat([unique_counts_primary_ph_rows1 , row_counts_primary_ph_rows1 , nan_counts_primary_ph_rows1 , empty_counts_primary_ph_rows1 ])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"PH Patients (Primary PH))\")\n",
    "print()\n",
    "display(result_primary_ph_rows )\n",
    "\n",
    "#primary_ph_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gYI9tmiQcWs"
   },
   "source": [
    "### <center>  13. I27.2 = Other Secondary PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49qFv8ohQcWt",
    "outputId": "b640c5c8-3503-4467-f36f-5386a6c63356"
   },
   "outputs": [],
   "source": [
    "# Extract rows where 'Other Secondary PH' is 'I27.2'\n",
    "other_secondary_ph_rows = []\n",
    "other_secondary_ph_rows = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Other Secondary PH'] == 'I27.2']\n",
    "other_secondary_ph_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_other_secondary_ph_rows = {}\n",
    "row_counts_other_secondary_ph_rows = {}\n",
    "nan_counts_other_secondary_ph_rows = {}\n",
    "empty_counts_other_secondary_ph_rows = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_other_secondary_ph_rows in other_secondary_ph_rows.columns:\n",
    "    unique_count_other_secondary_ph_rows  = other_secondary_ph_rows [column_other_secondary_ph_rows ].nunique()\n",
    "    row_count_other_secondary_ph_rows  = len(other_secondary_ph_rows [column_other_secondary_ph_rows ])\n",
    "    nan_count_other_secondary_ph_rows  = other_secondary_ph_rows [column_other_secondary_ph_rows ].isna().sum()  # Count NaN values\n",
    "    empty_count_other_secondary_ph_rows  = other_secondary_ph_rows [column_other_secondary_ph_rows ].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_other_secondary_ph_rows [column_other_secondary_ph_rows ] = [unique_count_other_secondary_ph_rows ]\n",
    "    row_counts_other_secondary_ph_rows [column_other_secondary_ph_rows ] = [row_count_other_secondary_ph_rows ]\n",
    "    nan_counts_other_secondary_ph_rows [column_other_secondary_ph_rows ] = [nan_count_other_secondary_ph_rows ]\n",
    "    empty_counts_other_secondary_ph_rows [column_other_secondary_ph_rows ] = [empty_count_other_secondary_ph_rows ]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_other_secondary_ph_rows1  = []\n",
    "row_counts_other_secondary_ph_rows1  = []\n",
    "nan_counts_other_secondary_ph_rows1  = []\n",
    "empty_counts_other_secondary_ph_rows1  = []\n",
    "\n",
    "unique_counts_other_secondary_ph_rows1  = pd.DataFrame(unique_counts_other_secondary_ph_rows , index=['Unique Count'])\n",
    "row_counts_other_secondary_ph_rows1  = pd.DataFrame(row_counts_other_secondary_ph_rows , index=['Row Count'])\n",
    "nan_counts_other_secondary_ph_rows1  = pd.DataFrame(nan_counts_other_secondary_ph_rows , index=['NaN Count'])\n",
    "empty_counts_other_secondary_ph_rows1  = pd.DataFrame(empty_counts_other_secondary_ph_rows , index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_other_secondary_ph_rows  = []\n",
    "result_other_secondary_ph_rows  = pd.concat([unique_counts_other_secondary_ph_rows1 , row_counts_other_secondary_ph_rows1 , nan_counts_other_secondary_ph_rows1 , empty_counts_other_secondary_ph_rows1 ])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"PH Patients (Other Secondary PH))\")\n",
    "print()\n",
    "display(result_other_secondary_ph_rows )\n",
    "\n",
    "#other_secondary_ph_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNhdzTR1QcWu"
   },
   "source": [
    "### <center>  14. I27.9 = Secondary PH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9G20KfV-QcWv",
    "outputId": "64941002-65eb-4ca7-e9dd-08e9ce851e4e"
   },
   "outputs": [],
   "source": [
    "# Extract rows where 'Secondary PH' is 'I27.9'\n",
    "secondary_ph_rows = []\n",
    "secondary_ph_rows = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Secondary PH'] == 'I27.9']\n",
    "secondary_ph_rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_secondary_ph_rows = {}\n",
    "row_counts_secondary_ph_rows = {}\n",
    "nan_counts_secondary_ph_rows = {}\n",
    "empty_counts_secondary_ph_rows = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_secondary_ph_rows in secondary_ph_rows.columns:\n",
    "    unique_count_secondary_ph_rows  = secondary_ph_rows [column_secondary_ph_rows ].nunique()\n",
    "    row_count_secondary_ph_rows  = len(secondary_ph_rows [column_secondary_ph_rows ])\n",
    "    nan_count_secondary_ph_rows  = secondary_ph_rows [column_secondary_ph_rows ].isna().sum()  # Count NaN values\n",
    "    empty_count_secondary_ph_rows  = secondary_ph_rows [column_secondary_ph_rows ].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_secondary_ph_rows [column_secondary_ph_rows ] = [unique_count_secondary_ph_rows ]\n",
    "    row_counts_secondary_ph_rows [column_secondary_ph_rows ] = [row_count_secondary_ph_rows ]\n",
    "    nan_counts_secondary_ph_rows [column_secondary_ph_rows ] = [nan_count_secondary_ph_rows ]\n",
    "    empty_counts_secondary_ph_rows [column_secondary_ph_rows ] = [empty_count_secondary_ph_rows ]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_secondary_ph_rows1  = []\n",
    "row_counts_secondary_ph_rows1  = []\n",
    "nan_counts_secondary_ph_rows1  = []\n",
    "empty_counts_secondary_ph_rows1  = []\n",
    "\n",
    "unique_counts_secondary_ph_rows1  = pd.DataFrame(unique_counts_secondary_ph_rows , index=['Unique Count'])\n",
    "row_counts_secondary_ph_rows1  = pd.DataFrame(row_counts_secondary_ph_rows , index=['Row Count'])\n",
    "nan_counts_secondary_ph_rows1  = pd.DataFrame(nan_counts_secondary_ph_rows , index=['NaN Count'])\n",
    "empty_counts_secondary_ph_rows1  = pd.DataFrame(empty_counts_secondary_ph_rows , index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_secondary_ph_rows  = []\n",
    "result_secondary_ph_rows  = pd.concat([unique_counts_secondary_ph_rows1 , row_counts_secondary_ph_rows1 , nan_counts_secondary_ph_rows1 , empty_counts_secondary_ph_rows1 ])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"PH Patients (Secondary PH))\")\n",
    "print()\n",
    "display(result_secondary_ph_rows )\n",
    "\n",
    "#secondary_ph_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZshPgpY-QcWw"
   },
   "source": [
    "###  <center>  15. Identify the rows (Participants) having more than one PH type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn-RbpaaQcWx"
   },
   "source": [
    "#### <center>  15 (a). Identify the Common PH Participants (I27.0 and I27.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDz4Tl6EQcWy",
    "outputId": "381231e8-a2e6-4d2d-910c-f6cd0d35fb01"
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames on \"Participant ID\"\n",
    "Commmon_ph_patients_I270_I272 = []\n",
    "Commmon_ph_patients_I270_I272 = pd.merge(primary_ph_rows, other_secondary_ph_rows, on='Participant ID', how='inner')\n",
    "\n",
    "# Display the common items under \"Participant ID\"\n",
    "common_participant_ids_I270_I272 = []\n",
    "common_participant_ids_I270_I272 = Commmon_ph_patients_I270_I272['Participant ID'].unique()\n",
    "#common_participant_ids_I270_I272\n",
    "\n",
    "print('Number of PH Pateints with I27.0 & I27.2:\\n',len(common_participant_ids_I270_I272))\n",
    "print()\n",
    "\n",
    "specific_item_id_I270_I272 = []\n",
    "specific_item_id_I270_I272 = [1014690, 1021691, 1068225, 1111508, 1117897, 1122689, 1134416,\n",
    "       1188939, 1190389, 1229997, 1269547, 1297277, 1319129, 1334819,\n",
    "       1347470, 1363234, 1364599, 1384168, 1395617, 1400425, 1413240,\n",
    "       1415423, 1426583, 1430769, 1435804, 1460749, 1460954, 1468540,\n",
    "       1477114, 1497164, 1528477, 1545374, 1546705, 1548802, 1556509,\n",
    "       1580380, 1596801, 1624084, 1631060, 1632050, 1646854, 1658055,\n",
    "       1668106, 1671186, 1686948, 1693625, 1720139, 1726911, 1727414,\n",
    "       1727652, 1732400, 1763014, 1772073, 1778782, 1797371, 1821039,\n",
    "       1829612, 1843080, 1865538, 1867949, 1875217, 1888912, 1894951,\n",
    "       1949094, 1957199, 1977549, 1994987, 1999140, 2130401, 2173751,\n",
    "       2181079, 2211209, 2217710, 2220892, 2266805, 2290747, 2343234,\n",
    "       2356346, 2357195, 2359087, 2373665, 2396386, 2415090, 2429453,\n",
    "       2429462, 2436344, 2441692, 2449782, 2466415, 2580621, 2591319,\n",
    "       2650820, 2651344, 2654801, 2693340, 2706860, 2727143, 2732629,\n",
    "       2784062, 2899870, 2915962, 2920699, 2946238, 2964441, 2969310,\n",
    "       2988833, 3017806, 3017919, 3020863, 3021089, 3066916, 3081287,\n",
    "       3088174, 3134778, 3148474, 3164077, 3193975, 3200678, 3201009,\n",
    "       3207048, 3221932, 3241711, 3262677, 3270146, 3270706, 3317803,\n",
    "       3327653, 3345749, 3354167, 3383747, 3425393, 3440294, 3443124,\n",
    "       3459015, 3491453, 3502771, 3510366, 3535644, 3572985, 3575728,\n",
    "       3576880, 3591991, 3595188, 3604427, 3607223, 3607572, 3670291,\n",
    "       3691201, 3692043, 3743092, 3749759, 3750424, 3818086, 3855064,\n",
    "       3855739, 3974853, 3977431, 3977577, 4060251, 4064568, 4072100,\n",
    "       4104092, 4115615, 4122157, 4133641, 4134549, 4141318, 4143404,\n",
    "       4153439, 4166604, 4175377, 4212172, 4218174, 4241979, 4243940,\n",
    "       4265576, 4274115, 4291372, 4295543, 4313951, 4359523, 4384646,\n",
    "       4388186, 4427392, 4427495, 4470224, 4475296, 4493893, 4496892,\n",
    "       4500760, 4518879, 4531021, 4543104, 4555483, 4569339, 4574867,\n",
    "       4585222, 4629878, 4642022, 4648201, 4664515, 4667134, 4672974,\n",
    "       4686475, 4690415, 4694936, 4702353, 4707110, 4764927, 4766385,\n",
    "       4776332, 4813638, 4864522, 4897138, 4914366, 4918186, 4939157,\n",
    "       4950212, 4960981, 4978389, 5012125, 5036416, 5039020, 5044666,\n",
    "       5049732, 5060165, 5067106, 5076927, 5102315, 5110413, 5119367,\n",
    "       5137036, 5153134, 5217264, 5221192, 5258566, 5265304, 5308676,\n",
    "       5371949, 5383222, 5387121, 5388383, 5390063, 5403139, 5411114,\n",
    "       5426316, 5428585, 5430868, 5440203, 5468958, 5489309, 5517792,\n",
    "       5521351, 5548595, 5571767, 5581579, 5586285, 5614037, 5635449,\n",
    "       5667351, 5709846, 5721223, 5734061, 5772800, 5791043, 5822113,\n",
    "       5866009, 5909011, 5916117, 5919164, 5962163, 5964481, 5970250,\n",
    "       5981193, 5989873, 6015027, 6015629]\n",
    "Display_rows_I270_I272 =[]\n",
    "Display_rows_I270_I272 = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Participant ID'].isin(specific_item_id_I270_I272)]\n",
    "Display_rows_I270_I272"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEPwmiyjQcWz"
   },
   "source": [
    "#### <center>  15 (b). Identify the Common PH Participants (I27.0 and I27.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWICFbNnQcWz",
    "outputId": "b11c88a9-8ba1-4193-f2ad-1526ee6de655"
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames on \"Participant ID\"\n",
    "Commmon_ph_patients_I270_I279 = []\n",
    "Commmon_ph_patients_I270_I279 = pd.merge(primary_ph_rows, secondary_ph_rows, on='Participant ID', how='inner')\n",
    "\n",
    "# Display the common items under \"Participant ID\"\n",
    "common_participant_ids_I270_I279 = []\n",
    "common_participant_ids_I270_I279 = Commmon_ph_patients_I270_I279['Participant ID'].unique()\n",
    "#common_participant_ids_I270_I279\n",
    "\n",
    "print('Number of PH Pateints with I27.0 & I27.9:\\n',len(common_participant_ids_I270_I279))\n",
    "print()\n",
    "\n",
    "specific_item_id_I270_I279 = []\n",
    "specific_item_id_I270_I279 = [1297277, 1413240, 1556509, 1720139, 1726911, 1763014, 1770726,\n",
    "       1775259, 1861228, 1977549, 1994987, 2090642, 2356346, 2429462,\n",
    "       2899870, 2939408, 3213112, 3310893, 3345749, 3425393, 3530057,\n",
    "       3572985, 3636338, 4291372, 4407008, 5003674, 5012428, 5110413,\n",
    "       5119367, 5343240, 5369292, 5403139, 5822113]\n",
    "Display_rows_I270_I279 =[]\n",
    "Display_rows_I270_I279 = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Participant ID'].isin(specific_item_id_I270_I279)]\n",
    "Display_rows_I270_I279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soSAiBa4QcW0"
   },
   "source": [
    "#### <center>  15 (c). Identify the Common PH Participants (I27.2 and I27.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5UxEy1wQcW1",
    "outputId": "317e30c4-99d1-4873-c079-9675278d7076"
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames on \"Participant ID\"\n",
    "Commmon_ph_patients_I272_I279 = []\n",
    "Commmon_ph_patients_I272_I279 = pd.merge(other_secondary_ph_rows, secondary_ph_rows, on='Participant ID', how='inner')\n",
    "\n",
    "# Display the common items under \"Participant ID\"\n",
    "common_participant_ids_I272_I279 = []\n",
    "common_participant_ids_I272_I279 = Commmon_ph_patients_I272_I279['Participant ID'].unique()\n",
    "#common_participant_ids_I272_I279\n",
    "\n",
    "\n",
    "print('Number of PH Pateints with I27.2 & I27.9:\\n',len(common_participant_ids_I272_I279))\n",
    "print()\n",
    "\n",
    "specific_item_id_I272_I279 = []\n",
    "specific_item_id_I272_I279 = [1033772, 1106773, 1212039, 1297277, 1322506, 1413240, 1556509,\n",
    "       1604052, 1610270, 1648819, 1682936, 1720139, 1726911, 1748521,\n",
    "       1763014, 1807331, 1827593, 1902484, 1924641, 1977549, 1994987,\n",
    "       2070676, 2333334, 2356346, 2404529, 2429462, 2440237, 2441821,\n",
    "       2476580, 2526790, 2576638, 2583641, 2591525, 2634101, 2750489,\n",
    "       2798682, 2828315, 2889021, 2899870, 2961458, 3024548, 3125222,\n",
    "       3207726, 3290526, 3301228, 3330935, 3345749, 3425393, 3434633,\n",
    "       3540321, 3568363, 3572985, 3702255, 3719989, 3722084, 3750378,\n",
    "       3781735, 3815670, 3872986, 3945584, 3974580, 4004658, 4071449,\n",
    "       4135553, 4135586, 4161083, 4177187, 4257010, 4288209, 4291372,\n",
    "       4333478, 4335869, 4351000, 4371718, 4434110, 4573481, 4608159,\n",
    "       4616669, 4635452, 4646585, 4707640, 4721951, 4774829, 4887043,\n",
    "       4892734, 4914213, 4958971, 5030160, 5110413, 5119367, 5159444,\n",
    "       5276833, 5398862, 5403139, 5416972, 5471436, 5573732, 5626274,\n",
    "       5671919, 5822113, 5864333, 5958830]\n",
    "Display_rows_I272_I279 =[]\n",
    "Display_rows_I272_I279 = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Participant ID'].isin(specific_item_id_I272_I279)]\n",
    "Display_rows_I272_I279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwG7w_KBQcW2"
   },
   "source": [
    "#### <center>  15 (d). Identify the Common PH Participants (I27.0 , I27.2 , I27.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhEW76gQQcW3",
    "outputId": "183ea4ee-14fc-4b84-b684-3bb192174fe5"
   },
   "outputs": [],
   "source": [
    "# Merge the DataFrames on \"Participant ID\"\n",
    "Commmon_ph_patients_rows = []\n",
    "Commmon_ph_patients_I270_I272_I279 = []\n",
    "Commmon_ph_patients_rows = pd.merge(primary_ph_rows, other_secondary_ph_rows, on='Participant ID', how='inner')\n",
    "Commmon_ph_patients_I270_I272_I279 = pd.merge(Commmon_ph_patients_rows, secondary_ph_rows, on='Participant ID', how='inner')\n",
    "\n",
    "# Display the common items under \"Participant ID\"\n",
    "common_participant_ids_I270_I272_I279 = []\n",
    "common_participant_ids_I270_I272_I279 = Commmon_ph_patients_I270_I272_I279['Participant ID'].unique()\n",
    "#common_participant_ids_I270_I272_I279\n",
    "\n",
    "print('Number of PH Pateints with I27.0 , I27.2 , I27.9:\\n',len(common_participant_ids_I270_I272_I279))\n",
    "print()\n",
    "\n",
    "specific_item_id_I270_I272_I279 = []\n",
    "specific_item_id_I270_I272_I279 = [1297277, 1413240, 1556509, 1720139, 1726911, 1763014, 1977549,\n",
    "       1994987, 2356346, 2429462, 2899870, 3345749, 3425393, 3572985,\n",
    "       4291372, 5110413, 5119367, 5403139, 5822113]\n",
    "Display_rows_I270_I272_I279 =[]\n",
    "Display_rows_I270_I272_I279 = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Participant ID'].isin(specific_item_id_I270_I272_I279)]\n",
    "Display_rows_I270_I272_I279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LWjXThYQcW4"
   },
   "source": [
    "### <center> 16. Steps for the concentenation of GP Prescription/ Drugs dataset with Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L6RjnKaQcW4"
   },
   "source": [
    "#### <center> 16 (a). Drop NaN from GP Drugs dataset (UKbio bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi-P252eQcW5",
    "outputId": "a60c2296-c5ba-444f-a231-1fd9a36afdf0"
   },
   "outputs": [],
   "source": [
    "###\n",
    "Dropped_Nan_GP_Prescription_Record = []\n",
    "Dropped_Nan_GP_Prescription_Record = GP_Prescription_Record\n",
    "Dropped_Nan_GP_Prescription_Record = Dropped_Nan_GP_Prescription_Record.dropna()\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Dropped_Nan_GP_Prescription_Record = {}\n",
    "row_counts_Dropped_Nan_GP_Prescription_Record = {}\n",
    "nan_counts_Dropped_Nan_GP_Prescription_Record = {}\n",
    "empty_counts_Dropped_Nan_GP_Prescription_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Dropped_Nan_GP_Prescription_Record in Dropped_Nan_GP_Prescription_Record.columns:\n",
    "    unique_count_Dropped_Nan_GP_Prescription_Record = Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record].nunique()\n",
    "    row_count_Dropped_Nan_GP_Prescription_Record = len(Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record])\n",
    "    nan_count_Dropped_Nan_GP_Prescription_Record = Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Dropped_Nan_GP_Prescription_Record = Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record] = [unique_count_Dropped_Nan_GP_Prescription_Record]\n",
    "    row_counts_Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record] = [row_count_Dropped_Nan_GP_Prescription_Record]\n",
    "    nan_counts_Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record] = [nan_count_Dropped_Nan_GP_Prescription_Record]\n",
    "    empty_counts_Dropped_Nan_GP_Prescription_Record[column_Dropped_Nan_GP_Prescription_Record] = [empty_count_Dropped_Nan_GP_Prescription_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Dropped_Nan_GP_Prescription = []\n",
    "row_counts_Dropped_Nan_GP_Prescription = []\n",
    "nan_counts_Dropped_Nan_GP_Prescription = []\n",
    "empty_counts_Dropped_Nan_GP_Prescription = []\n",
    "\n",
    "unique_counts_Dropped_Nan_GP_Prescription = pd.DataFrame(unique_counts_Dropped_Nan_GP_Prescription_Record, index=['Unique Count'])\n",
    "row_counts_Dropped_Nan_GP_Prescription = pd.DataFrame(row_counts_Dropped_Nan_GP_Prescription_Record, index=['Row Count'])\n",
    "nan_counts_Dropped_Nan_GP_Prescription = pd.DataFrame(nan_counts_Dropped_Nan_GP_Prescription_Record, index=['NaN Count'])\n",
    "empty_counts_Dropped_Nan_GP_Prescription = pd.DataFrame(empty_counts_Dropped_Nan_GP_Prescription_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Dropped_Nan_GP_Prescription_Record = []\n",
    "result_Dropped_Nan_GP_Prescription_Record = pd.concat([unique_counts_Dropped_Nan_GP_Prescription, row_counts_Dropped_Nan_GP_Prescription, nan_counts_Dropped_Nan_GP_Prescription, empty_counts_Dropped_Nan_GP_Prescription])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"GP Prescription Dataset Record (Without NaN):\")\n",
    "print()\n",
    "display(result_Dropped_Nan_GP_Prescription_Record)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dropped_Nan_GP_Prescription_Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-BYuPpBQcW6"
   },
   "source": [
    "#### <center> 16 (b). Extract Particular Medicines used for PH Patients and extract the rows from GP Drugs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA05_x-gQcW6",
    "outputId": "ae3e016e-ebd9-410b-8681-0bd15e8c6086"
   },
   "outputs": [],
   "source": [
    "### Keyword to search for\n",
    "keywords = []\n",
    "keywords = [\"Sildenafil\", \"Tadalafil\", \"Ambrisentan\",\"Bosentan\",\"Epoprostenol\",\"Iloprost - Unkown\",\"Iloprost - Inhaled\",\"Iloprost - Infusion (intravenous)\",\"Treprostinil - Unknown\",\n",
    "            \"Treprostinil - Inhaled\",\"Treprostinil - Infusion (intravenous)\",\"Treprostinil - Infusion (subcutaneous)\",\"CCB for vaso-reactive PAH\",\"Sitaxsentan\",\"Riociguat\",\"Macitentan\",\n",
    "            \"Selexipag\",\"Long-term oxygen therapy\"]\n",
    "\n",
    "### Create a boolean mask to filter rows containing any of the keywords\n",
    "mask= []\n",
    "mask = Dropped_Nan_GP_Prescription_Record['Drug Name'].str.contains('|'.join(keywords), case=False)\n",
    "\n",
    "### Apply the mask to the DataFrame to filter rows\n",
    "Drugs_Name_Filtered_GP_Prescription_Record = []\n",
    "Drugs_Name_Filtered_GP_Prescription_Record = Dropped_Nan_GP_Prescription_Record[mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Drugs_Name_Filtered_GP_Prescription_Record in Drugs_Name_Filtered_GP_Prescription_Record.columns:\n",
    "    unique_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].nunique()\n",
    "    row_count_Drugs_Name_Filtered_GP_Prescription_Record = len(Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record])\n",
    "    nan_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [unique_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    row_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [row_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    nan_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [nan_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    empty_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [empty_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(unique_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Unique Count'])\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(row_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Row Count'])\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(nan_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['NaN Count'])\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(empty_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Drugs_Name_Filtered_GP_Prescription_Record = []\n",
    "result_Drugs_Name_Filtered_GP_Prescription_Record = pd.concat([unique_counts_Drugs_Name_Filtered_GP_Prescription, row_counts_Drugs_Name_Filtered_GP_Prescription, nan_counts_Drugs_Name_Filtered_GP_Prescription, empty_counts_Drugs_Name_Filtered_GP_Prescription])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Drugs Name from GP Prescription Record:\")\n",
    "print()\n",
    "display(result_Drugs_Name_Filtered_GP_Prescription_Record)\n",
    "\n",
    "\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "Drugs_Name_Filtered_GP_Prescription_Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64uRdxr6QcW7"
   },
   "source": [
    "#### <center> 16 (c). Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvnwAakVQcW7",
    "outputId": "7f76f70c-83d9-4088-ffdc-bbbf59badf0f"
   },
   "outputs": [],
   "source": [
    "Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record.drop_duplicates(subset=['Participant ID','Drug Name'])\n",
    "Drugs_Name_Filtered_GP_Prescription_Record.sort_values(by='Participant ID')\n",
    "\n",
    "\n",
    "# Create dictionaries to store unique counts, row counts, NaN counts, and empty counts\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription_Record = {}\n",
    "\n",
    "# Loop through columns and count unique items, row counts, NaN counts, and empty counts\n",
    "for column_Drugs_Name_Filtered_GP_Prescription_Record in Drugs_Name_Filtered_GP_Prescription_Record.columns:\n",
    "    unique_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].nunique()\n",
    "    row_count_Drugs_Name_Filtered_GP_Prescription_Record = len(Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record])\n",
    "    nan_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].isna().sum()  # Count NaN values\n",
    "    empty_count_Drugs_Name_Filtered_GP_Prescription_Record = Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record].eq('').sum()  # Count empty string values\n",
    "\n",
    "    unique_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [unique_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    row_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [row_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    nan_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [nan_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "    empty_counts_Drugs_Name_Filtered_GP_Prescription_Record[column_Drugs_Name_Filtered_GP_Prescription_Record] = [empty_count_Drugs_Name_Filtered_GP_Prescription_Record]\n",
    "\n",
    "# Create DataFrames from the dictionaries\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription = []\n",
    "\n",
    "unique_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(unique_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Unique Count'])\n",
    "row_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(row_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Row Count'])\n",
    "nan_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(nan_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['NaN Count'])\n",
    "empty_counts_Drugs_Name_Filtered_GP_Prescription = pd.DataFrame(empty_counts_Drugs_Name_Filtered_GP_Prescription_Record, index=['Empty Count'])\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "result_Drugs_Name_Filtered_GP_Prescription_Record = []\n",
    "result_Drugs_Name_Filtered_GP_Prescription_Record = pd.concat([unique_counts_Drugs_Name_Filtered_GP_Prescription, row_counts_Drugs_Name_Filtered_GP_Prescription, nan_counts_Drugs_Name_Filtered_GP_Prescription, empty_counts_Drugs_Name_Filtered_GP_Prescription])\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(\"Drugs Name from GP Prescription Record:\")\n",
    "print()\n",
    "display(result_Drugs_Name_Filtered_GP_Prescription_Record)\n",
    "\n",
    "\n",
    "Drugs_Name_Filtered_GP_Prescription_Record\n",
    "\n",
    "Drugs_Data_for_PH_Patients = []\n",
    "Drugs_Data_for_PH_Patients = Drugs_Name_Filtered_GP_Prescription_Record\n",
    "Drugs_Data_for_PH_Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kskwtvQcW8"
   },
   "source": [
    "### <center> Identify and display rows in \"Drugs_Data_for_PH_Patients\" with Participant IDs are present in \"PH_Patients_using_ICD10_codes_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVSNAC1ZQcW9",
    "outputId": "d6d0d709-b89d-44ff-e8cc-1107e4616803"
   },
   "outputs": [],
   "source": [
    "identified_PH_Patients_with_Drugs_Names = []\n",
    "identified_PH_Patients_with_Drugs_Names = Drugs_Data_for_PH_Patients[Drugs_Data_for_PH_Patients['Participant ID'].isin(PH_Patients_using_ICD10_codes_Dataset['Participant ID'])]\n",
    "\n",
    "participant_ids = []\n",
    "participant_ids = identified_PH_Patients_with_Drugs_Names['Participant ID']\n",
    "# Get unique values\n",
    "unique_participant_ids = []\n",
    "unique_participant_ids = participant_ids.unique()\n",
    "\n",
    "# Get the length of unique values\n",
    "len_unique_participant_ids =[]\n",
    "len_unique_participant_ids = len(unique_participant_ids)\n",
    "print(len_unique_participant_ids)\n",
    "\n",
    "identified_PH_Patients_with_Drugs_Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ngdUG0oQcW-"
   },
   "source": [
    "### <center> Identify and display rows in \"PH_Patients_using_ICD10_codes_Dataset\" with Participant IDs are present in \"Drugs_Data_for_PH_Patients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb0RP-WEQcW-",
    "outputId": "77ae5c2a-3c4a-4434-ea1b-c5d180cf17fd"
   },
   "outputs": [],
   "source": [
    "identified_PH_Patients_with_Drugs_Names2 = []\n",
    "identified_PH_Patients_with_Drugs_Names2 = PH_Patients_using_ICD10_codes_Dataset[PH_Patients_using_ICD10_codes_Dataset['Participant ID'].isin(Drugs_Data_for_PH_Patients['Participant ID'])]\n",
    "\n",
    "participant_ids2 = []\n",
    "participant_ids2 = identified_PH_Patients_with_Drugs_Names2['Participant ID']\n",
    "# Get unique values\n",
    "unique_participant_ids2 = []\n",
    "unique_participant_ids2 = participant_ids2.unique()\n",
    "\n",
    "# Get the length of unique values\n",
    "len_unique_participant_ids2 =[]\n",
    "len_unique_participant_ids2 = len(unique_participant_ids2)\n",
    "print(len_unique_participant_ids2)\n",
    "\n",
    "identified_PH_Patients_with_Drugs_Names2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVFhh1vBQcW_"
   },
   "source": [
    "#### <center> 16 (d). Merge PH Patients Extracted data with Drugs Data (inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MsFHq_PQcW_",
    "outputId": "d8c471ad-cfca-416b-a2c4-fed68416f1b0"
   },
   "outputs": [],
   "source": [
    "Merged_PH_with_PH_Drugs_inner = []\n",
    "Merged_PH_with_PH_Drugs_inner = pd.merge(PH_Patients_using_ICD10_codes_Dataset,identified_PH_Patients_with_Drugs_Names,   on=\"Participant ID\", how=\"inner\")\n",
    "\n",
    "participant_ids3 = []\n",
    "participant_ids3 = Merged_PH_with_PH_Drugs_inner['Participant ID']\n",
    "# Get unique values\n",
    "unique_participant_ids3 = []\n",
    "unique_participant_ids3 = participant_ids3.unique()\n",
    "\n",
    "# Get the length of unique values\n",
    "len_unique_participant_ids3 =[]\n",
    "len_unique_participant_ids3 = len(unique_participant_ids3)\n",
    "print(len_unique_participant_ids3)\n",
    "\n",
    "\n",
    "Merged_PH_with_PH_Drugs_inner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dX07aQLQcXA"
   },
   "source": [
    "#### <center> 16 (e). Merge PH Patients Extracted data with Drugs Data (outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELp_nz4-QcXB",
    "outputId": "f764301a-69c2-43a5-e3c6-8104dcdfb005"
   },
   "outputs": [],
   "source": [
    "Merged_PH_with_PH_Drugs_outer = []\n",
    "Merged_PH_with_PH_Drugs_outer = pd.merge(PH_Patients_using_ICD10_codes_Dataset,identified_PH_Patients_with_Drugs_Names,   on=\"Participant ID\", how=\"outer\")\n",
    "\n",
    "\n",
    "participant_ids4 = []\n",
    "participant_ids4 = Merged_PH_with_PH_Drugs_outer['Participant ID']\n",
    "# Get unique values\n",
    "unique_participant_ids4 = []\n",
    "unique_participant_ids4 = participant_ids4.unique()\n",
    "\n",
    "# Get the length of unique values\n",
    "len_unique_participant_ids4 =[]\n",
    "len_unique_participant_ids4 = len(unique_participant_ids4)\n",
    "print(len_unique_participant_ids4)\n",
    "\n",
    "Merged_PH_with_PH_Drugs_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JYMjkIvQcXC"
   },
   "source": [
    "### <center> 17. PH Patients with Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R3o7vGBQcXC",
    "outputId": "831ab483-db2b-46e7-fa81-de718c9c2ed1"
   },
   "outputs": [],
   "source": [
    "PH_with_Comorbidities = []\n",
    "# Extracting rows with Pulmonary Hypertension and other diseases\n",
    "PH_with_Comorbidities = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[\"Combined ICD10 Diseases\"].str.contains(\"pulmonary hypertension\", case=False) | Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[\"Participant ID\"].isin(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.loc[Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[\"Combined ICD10 Diseases\"].str.contains(\"pulmonary hypertension\", case=False), \"Participant ID\"])]\n",
    "PH_with_Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the unique items in the \"Participant ID\" column\n",
    "unique_participant_ids = []\n",
    "unique_participant_ids = pd.DataFrame(PH_with_Comorbidities[\"Participant ID\"].unique())\n",
    "unique_participant_ids = unique_participant_ids.rename(columns={0: \"Participant ID\"})\n",
    "unique_participant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Participant IDs based on condition in the \"Combined ICD10 Codes\" column\n",
    "#condition = []\n",
    "#condition = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Codes'].str.contains('I27', case=False, na=False)\n",
    "#filtered_participant_ids = pd.DataFrame(Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.loc[condition, 'Participant ID'])\n",
    "#filtered_participant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rows based on condition in the \"Combined ICD10 Codes\" column\n",
    "condition = []\n",
    "PH_Patients_ids = []\n",
    "condition = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Combined ICD10 Codes'].str.contains('I27.0|I27.2|I27.9', case=False, na=False)\n",
    "PH_Patients_ids = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records.loc[condition]\n",
    "PH_Patients_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique Participant IDs\n",
    "unique_participant_ids = []\n",
    "unique_participant_ids = PH_Patients_ids['Participant ID'].unique()\n",
    "unique_participant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rows from the second dataset based on unique Participant IDs\n",
    "comorbidities = []\n",
    "comorbidities = Dataset_with_ICD10_and_Diseases_Names_and_Death_Records[Dataset_with_ICD10_and_Diseases_Names_and_Death_Records['Participant ID'].isin(unique_participant_ids)]\n",
    "\n",
    "# Display the resulting rows\n",
    "comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes based on \"Participant ID\"\n",
    "PH_pateints_with_comorbidities = []\n",
    "PH_pateints_with_comorbidities = pd.merge(PH_Patients_ids, comorbidities, on='Participant ID', how='inner')\n",
    "# Drop the specified columns\n",
    "PH_pateints_with_comorbidities = PH_pateints_with_comorbidities.drop(columns=['Year of Birth_x','Month of Birth_x','Sex_x','Ethnicity_x','Combined ICD10 Diagnosis Date_x','Combined ICD10 Diseases_x','Date of Death_x','Death Cause Diseases_x','Combined ICD10 Codes_x','Death Cause Disease ICD10 Codes_x','Alive / Dead_x'])\n",
    "# Rename the columns\n",
    "PH_pateints_with_comorbidities.rename(columns={'Year of Birth_y': 'Year of Birth', 'Month of Birth_y': 'Month of Birth',\n",
    "    'Sex_y': 'Sex',\n",
    "    'Ethnicity_y': 'Ethnicity', 'Combined ICD10 Diagnosis Date_y': 'Combined ICD10 Diagnosis Date',\n",
    "    'Combined ICD10 Diseases_y': 'Combined ICD10 Diseases',\n",
    "    'Date of Death_y': 'Date of Death',\n",
    "    'Death Cause Diseases_y': 'Death Cause Diseases',\n",
    "    'Death Cause Diseases': 'Death Cause Diseases',  # Added missing column name\n",
    "    'Combined ICD10 Codes_y': 'Combined ICD10 Codes',\n",
    "    'Death Cause Disease ICD10 Codes_y': 'Death Cause Disease ICD10 Codes',\n",
    "    'Alive / Dead_y': 'Alive / Dead'}, inplace=True)\n",
    "\n",
    "PH_pateints_with_comorbidities\n",
    "\n",
    "PH_pateints_with_comorbidities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Extract unique Participant IDs with \"Pulmonary Hypertension\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz8anqL3QcXH"
   },
   "outputs": [],
   "source": [
    "unique_ids_with_ph = []\n",
    "unique_ids_with_ph = PH_pateints_with_comorbidities.loc[PH_pateints_with_comorbidities[\"Combined ICD10 Diseases\"].str.lower().str.contains(\"pulmonary hypertension\"), \"Participant ID\"].unique()\n",
    "unique_ids_with_ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Extract unique Participant IDs with PH with \"I27.0 = Primary PH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_with_ph_primary = []\n",
    "filtered_rows = []\n",
    "unique_ids_with_ph_primary = PH_pateints_with_comorbidities.loc[PH_pateints_with_comorbidities[\"Combined ICD10 Codes\"] == \"I27.0\", \"Participant ID\"].unique()\n",
    "filtered_rows = PH_pateints_with_comorbidities[PH_pateints_with_comorbidities[\"Participant ID\"].isin(unique_ids_with_ph_primary)]\n",
    "print(unique_ids_with_ph_primary)\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Extract unique Participant IDs with PH with \"I27.2 = Other secondary PH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_with_ph_other_secondary = []\n",
    "filtered_rows = []\n",
    "unique_ids_with_ph_other_secondary = PH_pateints_with_comorbidities.loc[PH_pateints_with_comorbidities[\"Combined ICD10 Codes\"] == \"I27.2\", \"Participant ID\"].unique()\n",
    "filtered_rows = PH_pateints_with_comorbidities[PH_pateints_with_comorbidities[\"Participant ID\"].isin(unique_ids_with_ph_other_secondary)]\n",
    "print(unique_ids_with_ph_other_secondary)\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Extract unique Participant IDs with PH with \"I27.9 = Secondary PH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rows = []\n",
    "unique_ids_with_ph_secondary = []\n",
    "unique_ids_with_ph_secondary = PH_pateints_with_comorbidities.loc[PH_pateints_with_comorbidities[\"Combined ICD10 Codes\"] == \"I27.9\", \"Participant ID\"].unique()\n",
    "filtered_rows = PH_pateints_with_comorbidities[PH_pateints_with_comorbidities[\"Participant ID\"].isin(unique_ids_with_ph_secondary)]\n",
    "print(unique_ids_with_ph_secondary)\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4H6iqU0SQcXK",
    "outputId": "e1b518ba-b4aa-4a94-ade0-ffaaeb1fc773"
   },
   "outputs": [],
   "source": [
    "# Define a function to determine the PH Type for each group\n",
    "def map_ph_types(participant_group):\n",
    "    if any(participant_group['Combined ICD10 Codes'].str.contains('I27.0', na=False)):\n",
    "        participant_group['PH Types'] = 'I27.0 - Primary Pulmonary Hypertension'\n",
    "    elif any(participant_group['Combined ICD10 Codes'].str.contains('I27.2', na=False)):\n",
    "        participant_group['PH Types'] = 'I27.2 - Other secondary pulmonary hypertension'\n",
    "    elif any(participant_group['Combined ICD10 Codes'].str.contains('I27.9', na=False)):\n",
    "        participant_group['PH Types'] = 'I27.9 - Secondary Pulmonary Hypertension'\n",
    "    else:\n",
    "        participant_group['PH Types'] = 'Other'\n",
    "    return participant_group\n",
    "\n",
    "# Apply the function to each group of 'Participant ID'\n",
    "PH_pateints_with_comorbidities = PH_pateints_with_comorbidities.groupby('Participant ID', group_keys=False).apply(map_ph_types)\n",
    "PH_pateints_with_comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the file path where you want to save the CSV file\n",
    "file_path = []\n",
    "file_path = 'PH Patients with Commorbidities Dataset.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "PH_pateints_with_comorbidities.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn4YvPZqQcXU",
    "outputId": "b9db157b-b189-4677-c444-04046ccf1a02"
   },
   "outputs": [],
   "source": [
    "#df_sorted = []\n",
    "#df_sorted = df\n",
    "# Sort the DataFrame for each instance in the 'Participant ID' column\n",
    "#df_sorted = df.groupby('Participant ID', group_keys=False).apply(lambda x: x.sort_values(by=['Participant ID', 'Combined ICD10 Diagnosis Date']))\n",
    "\n",
    "# Resetting the index\n",
    "#df_sorted.reset_index(drop=True, inplace=True)\n",
    "#df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"background-color: pink\">17. Pre-PH Patients with Comorbidities</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = []\n",
    "result_df = PH_pateints_with_comorbidities\n",
    "result_df\n",
    "\n",
    "\n",
    "# Convert \"Year of Birth\" to numeric\n",
    "result_df['Year of Birth'] = pd.to_numeric(result_df['Year of Birth'], errors='coerce')\n",
    "\n",
    "# Map month names to corresponding numerical values\n",
    "#month_mapping = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "#                 'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "\n",
    "# Convert \"Month of Birth\" to numeric\n",
    "result_df['Month of Birth'] = result_df['Month of Birth'].map(month_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming result_df is the DataFrame with the \"PH\" column\n",
    "result_df[\"Combined ICD10 Diagnosis Date\"] = pd.to_datetime(result_df[\"Combined ICD10 Diagnosis Date\"])\n",
    "\n",
    "\n",
    "pre_ph_rows = []\n",
    "\n",
    "# Function to filter rows diagnosed before pulmonary hypertension\n",
    "def filter_pre_ph_rows(group):\n",
    "    ph_rows = group[group[\"Combined ICD10 Diseases\"].str.lower().str.contains(\"pulmonary hypertension\")]\n",
    "\n",
    "    if not ph_rows.empty:\n",
    "        ph_index = ph_rows.index[0]\n",
    "        pre_ph_rows = group[group[\"Combined ICD10 Diagnosis Date\"] < group.loc[ph_index, \"Combined ICD10 Diagnosis Date\"]]\n",
    "        if not pre_ph_rows.empty:\n",
    "            pre_ph_rows = pd.concat([pre_ph_rows, group.loc[ph_index:ph_index]])\n",
    "            return pre_ph_rows\n",
    "    return pd.DataFrame()\n",
    "\n",
    "#Apply the function to each group of \"Participant ID\"\n",
    "pre_ph_rows = result_df.groupby(\"Participant ID\").apply(filter_pre_ph_rows)\n",
    "\n",
    "# Resetting the index\n",
    "pre_ph_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Displaying the result\n",
    "# Convert to integers\n",
    "pre_ph_rows['Participant ID'] = pre_ph_rows['Participant ID'].astype(int)\n",
    "pre_ph_rows['Year of Birth'] = pre_ph_rows['Year of Birth'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert \"Year of Birth\" to datetime\n",
    "pre_ph_rows['Year of Birth'] = pd.to_datetime(pre_ph_rows['Year of Birth'], format='%Y')\n",
    "\n",
    "# Convert \"Combined ICD10 Diagnosis Date\" to datetime\n",
    "pre_ph_rows['Combined ICD10 Diagnosis Date'] = pd.to_datetime(pre_ph_rows['Combined ICD10 Diagnosis Date'], errors='coerce')\n",
    "\n",
    "# Calculate age\n",
    "pre_ph_rows['Age'] = (pre_ph_rows['Combined ICD10 Diagnosis Date'] - pre_ph_rows['Year of Birth']).astype('<m8[Y]').astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "pre_ph_rows = pre_ph_rows[['Participant ID', 'Year of Birth','Month of Birth', 'Sex','Ethnicity','Combined ICD10 Codes','Combined ICD10 Diseases','Combined ICD10 Diagnosis Date', 'Age','PH Types','Date of Death','Death Cause Diseases','Death Cause Disease ICD10 Codes','Alive / Dead']]\n",
    "\n",
    "# Rename columns\n",
    "pre_ph_rows = pre_ph_rows.rename(columns={'Combined ICD10 Codes': 'ICD10 Codes', 'Combined ICD10 Diseases': 'Diseases', 'Combined ICD10 Diagnosis Date': 'Diagnosis Date'})\n",
    "pre_ph_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the file path where you want to save the CSV file\n",
    "#file_path = []\n",
    "#file_path = 'Pre PH with Comorbidities Dataset.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "\n",
    "#pre_ph_rows.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = []\n",
    "df = pre_ph_rows\n",
    "# Extract rows with \"Male\" and count unique Participant IDs\n",
    "male_participant_ids = df[df['Sex'] == 'Male']['Participant ID'].unique()\n",
    "num_male_participants = len(male_participant_ids)\n",
    "\n",
    "# Extract rows with \"Female\" and count unique Participant IDs\n",
    "female_participant_ids = df[df['Sex'] == 'Female']['Participant ID'].unique()\n",
    "num_female_participants = len(female_participant_ids)\n",
    "\n",
    "# Display the counts\n",
    "print(f'Number of Males: {num_male_participants}')\n",
    "print(f'Number of Females: {num_female_participants}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "pie_labels = ['Male', 'Female']\n",
    "plt.pie([num_male_participants, num_female_participants], labels=pie_labels, autopct='%1.1f%%', startangle=90, colors=['blue', 'pink'], textprops={'fontsize': 14})\n",
    "\n",
    "# Add count labels beneath Male and Female\n",
    "plt.text(-1.3, -0.3, f'Count: {num_male_participants}', color='black', fontsize=12, ha='center', va='center')\n",
    "plt.text(1.3, -0.05, f'Count: {num_female_participants}', color='black', fontsize=12, ha='center', va='center')\n",
    "\n",
    "plt.title('Distribution of Males and Females', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URark-cGQcXc"
   },
   "source": [
    "### <center> 17.2: Post-PH Patients with Comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KpzrwwhQcXc"
   },
   "outputs": [],
   "source": [
    "post_ph_rows = []\n",
    "\n",
    "# Function to filter rows diagnosed before pulmonary hypertension\n",
    "def filter_post_ph_rows(group):\n",
    "    pph_rows = group[group[\"Combined ICD10 Diseases\"].str.lower().str.contains(\"pulmonary hypertension\")]\n",
    "\n",
    "    if not pph_rows.empty:\n",
    "        pph_index = pph_rows.index[0]\n",
    "        post_ph_rows = group[group[\"Combined ICD10 Diagnosis Date\"] > group.loc[pph_index, \"Combined ICD10 Diagnosis Date\"]]\n",
    "        if not post_ph_rows.empty:\n",
    "            post_ph_rows = pd.concat([post_ph_rows, group.loc[pph_index:pph_index]])\n",
    "            return post_ph_rows\n",
    "    return pd.DataFrame()\n",
    "\n",
    "#Apply the function to each group of \"Participant ID\"\n",
    "post_ph_rows = result_df.groupby(\"Participant ID\").apply(filter_post_ph_rows)\n",
    "\n",
    "# Resetting the index\n",
    "post_ph_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the result\n",
    "# Convert to integers\n",
    "post_ph_rows['Participant ID'] = post_ph_rows['Participant ID'].astype(int)\n",
    "post_ph_rows['Year of Birth'] = post_ph_rows['Year of Birth'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert \"Year of Birth\" to datetime\n",
    "post_ph_rows['Year of Birth'] = pd.to_datetime(post_ph_rows['Year of Birth'], format='%Y')\n",
    "\n",
    "# Convert \"Combined ICD10 Diagnosis Date\" to datetime\n",
    "post_ph_rows['Combined ICD10 Diagnosis Date'] = pd.to_datetime(post_ph_rows['Combined ICD10 Diagnosis Date'], errors='coerce')\n",
    "\n",
    "# Calculate age\n",
    "post_ph_rows['Age'] = (post_ph_rows['Combined ICD10 Diagnosis Date'] - post_ph_rows['Year of Birth']).astype('<m8[Y]').astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "post_ph_rows = post_ph_rows[['Participant ID', 'Year of Birth','Month of Birth', 'Sex','Ethnicity','Combined ICD10 Codes','Combined ICD10 Diseases','Combined ICD10 Diagnosis Date', 'Age','PH Types','Date of Death','Death Cause Diseases','Death Cause Disease ICD10 Codes','Alive / Dead']]\n",
    "\n",
    "# Rename columns\n",
    "post_ph_rows = post_ph_rows.rename(columns={'Combined ICD10 Codes': 'ICD10 Codes', 'Combined ICD10 Diseases': 'Diseases', 'Combined ICD10 Diagnosis Date': 'Diagnosis Date'})\n",
    "post_ph_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify the file path where you want to save the CSV file\n",
    "#file_path = []\n",
    "#file_path = 'Post PH with Comorbidities Dataset.csv'\n",
    "\n",
    "#### Use the to_csv method to save the DataFrame as a CSV file\n",
    "\n",
    "#post_ph_rows.to_csv(file_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwMGG7USQcXc"
   },
   "source": [
    "### <center> 17.3: PH Patients with Comorbidities Same Diagnosis Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRGFxEsZQcXd"
   },
   "outputs": [],
   "source": [
    "e_ph_rows = []\n",
    "# Function to filter rows diagnosed before pulmonary hypertension\n",
    "def filter_e_ph_rows(group):\n",
    "    eph_rows = group[group[\"Combined ICD10 Diseases\"].str.lower().str.contains(\"pulmonary hypertension\")]\n",
    "\n",
    "    if not eph_rows.empty:\n",
    "        eph_index = eph_rows.index[0]\n",
    "        e_ph_rows = group[group[\"Combined ICD10 Diagnosis Date\"] == group.loc[eph_index, \"Combined ICD10 Diagnosis Date\"]]\n",
    "        if not e_ph_rows.empty:\n",
    "            e_ph_rows = pd.concat([e_ph_rows, group.loc[eph_index:eph_index]])\n",
    "            return e_ph_rows\n",
    "    return pd.DataFrame()\n",
    "\n",
    "#Apply the function to each group of \"Participant ID\"\n",
    "e_ph_rows = result_df.groupby(\"Participant ID\").apply(filter_e_ph_rows)\n",
    "\n",
    "# Resetting the index\n",
    "e_ph_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Convert to integers\n",
    "e_ph_rows['Participant ID'] = e_ph_rows['Participant ID'].astype(int)\n",
    "e_ph_rows['Year of Birth'] = e_ph_rows['Year of Birth'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert \"Year of Birth\" to datetime\n",
    "e_ph_rows['Year of Birth'] = pd.to_datetime(e_ph_rows['Year of Birth'], format='%Y')\n",
    "\n",
    "# Convert \"Combined ICD10 Diagnosis Date\" to datetime\n",
    "e_ph_rows['Combined ICD10 Diagnosis Date'] = pd.to_datetime(e_ph_rows['Combined ICD10 Diagnosis Date'], errors='coerce')\n",
    "\n",
    "# Calculate age\n",
    "e_ph_rows['Age'] = (e_ph_rows['Combined ICD10 Diagnosis Date'] - e_ph_rows['Year of Birth']).astype('<m8[Y]').astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "e_ph_rows = e_ph_rows[['Participant ID', 'Year of Birth','Month of Birth', 'Sex','Ethnicity','Combined ICD10 Codes','Combined ICD10 Diseases','Combined ICD10 Diagnosis Date', 'Age','PH Types','Date of Death','Death Cause Diseases','Death Cause Disease ICD10 Codes','Alive / Dead']]\n",
    "\n",
    "# Rename columns\n",
    "e_ph_rows = e_ph_rows.rename(columns={'Combined ICD10 Codes': 'ICD10 Codes', 'Combined ICD10 Diseases': 'Diseases', 'Combined ICD10 Diagnosis Date': 'Diagnosis Date'})\n",
    "e_ph_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
